{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim import models,corpora\n",
    "from tqdm import tqdm,tqdm_gui\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "import logging\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/glove.6B.200d.txt'),\n",
       " PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/glove.6B.zip'),\n",
       " PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/train.tsv'),\n",
       " PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/test.tsv'),\n",
       " PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/glove.6B.100d.txt'),\n",
       " PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/glove.6B.300d.txt'),\n",
       " PosixPath('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge/glove.6B.50d.txt')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path('/data/ssubrahmannian/.kaggle/competitions/mercari-price-suggestion-challenge')\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(PATH/'train.tsv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_tokens_valid.pkl','rb') as f:\n",
    "    tokenized_text_test = pickle.load(f)\n",
    "\n",
    "with open('text_tokens_train.pkl','rb') as f:\n",
    "    tokenized_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text += tokenized_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_len_test = [len(x) for x in tokenized_text_test]\n",
    "review_len = [len(x) for x in tokenized_text]\n",
    "np.percentile(review_len,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 212777 words and 117576 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 427771 words and 203227 word types\n",
      "INFO : PROGRESS: at sentence #30000, processed 643205 words and 278290 word types\n",
      "INFO : PROGRESS: at sentence #40000, processed 857555 words and 347115 word types\n",
      "INFO : PROGRESS: at sentence #50000, processed 1072517 words and 410024 word types\n",
      "INFO : PROGRESS: at sentence #60000, processed 1284386 words and 468275 word types\n",
      "INFO : PROGRESS: at sentence #70000, processed 1498844 words and 524867 word types\n",
      "INFO : PROGRESS: at sentence #80000, processed 1711750 words and 578535 word types\n",
      "INFO : PROGRESS: at sentence #90000, processed 1925925 words and 629694 word types\n",
      "INFO : PROGRESS: at sentence #100000, processed 2139448 words and 679079 word types\n",
      "INFO : PROGRESS: at sentence #110000, processed 2353179 words and 726718 word types\n",
      "INFO : PROGRESS: at sentence #120000, processed 2569298 words and 772731 word types\n",
      "INFO : PROGRESS: at sentence #130000, processed 2782073 words and 817634 word types\n",
      "INFO : PROGRESS: at sentence #140000, processed 2992169 words and 861396 word types\n",
      "INFO : PROGRESS: at sentence #150000, processed 3204831 words and 904634 word types\n",
      "INFO : PROGRESS: at sentence #160000, processed 3415391 words and 946102 word types\n",
      "INFO : PROGRESS: at sentence #170000, processed 3627196 words and 986082 word types\n",
      "INFO : PROGRESS: at sentence #180000, processed 3842498 words and 1026182 word types\n",
      "INFO : PROGRESS: at sentence #190000, processed 4055979 words and 1066125 word types\n",
      "INFO : PROGRESS: at sentence #200000, processed 4270558 words and 1104635 word types\n",
      "INFO : PROGRESS: at sentence #210000, processed 4483349 words and 1143145 word types\n",
      "INFO : PROGRESS: at sentence #220000, processed 4698626 words and 1180883 word types\n",
      "INFO : PROGRESS: at sentence #230000, processed 4910817 words and 1218326 word types\n",
      "INFO : PROGRESS: at sentence #240000, processed 5123752 words and 1254844 word types\n",
      "INFO : PROGRESS: at sentence #250000, processed 5334820 words and 1289935 word types\n",
      "INFO : PROGRESS: at sentence #260000, processed 5547989 words and 1325604 word types\n",
      "INFO : PROGRESS: at sentence #270000, processed 5760737 words and 1359702 word types\n",
      "INFO : PROGRESS: at sentence #280000, processed 5973819 words and 1393776 word types\n",
      "INFO : PROGRESS: at sentence #290000, processed 6189942 words and 1428032 word types\n",
      "INFO : PROGRESS: at sentence #300000, processed 6403589 words and 1460863 word types\n",
      "INFO : PROGRESS: at sentence #310000, processed 6618297 words and 1494146 word types\n",
      "INFO : PROGRESS: at sentence #320000, processed 6832848 words and 1527571 word types\n",
      "INFO : PROGRESS: at sentence #330000, processed 7047889 words and 1560139 word types\n",
      "INFO : PROGRESS: at sentence #340000, processed 7262282 words and 1591994 word types\n",
      "INFO : PROGRESS: at sentence #350000, processed 7476163 words and 1622919 word types\n",
      "INFO : PROGRESS: at sentence #360000, processed 7686655 words and 1654119 word types\n",
      "INFO : PROGRESS: at sentence #370000, processed 7898981 words and 1684378 word types\n",
      "INFO : PROGRESS: at sentence #380000, processed 8110784 words and 1715034 word types\n",
      "INFO : PROGRESS: at sentence #390000, processed 8323451 words and 1744951 word types\n",
      "INFO : PROGRESS: at sentence #400000, processed 8532462 words and 1774280 word types\n",
      "INFO : PROGRESS: at sentence #410000, processed 8745863 words and 1804704 word types\n",
      "INFO : PROGRESS: at sentence #420000, processed 8955958 words and 1834119 word types\n",
      "INFO : PROGRESS: at sentence #430000, processed 9166583 words and 1863308 word types\n",
      "INFO : PROGRESS: at sentence #440000, processed 9378121 words and 1891863 word types\n",
      "INFO : PROGRESS: at sentence #450000, processed 9590382 words and 1920385 word types\n",
      "INFO : PROGRESS: at sentence #460000, processed 9802140 words and 1948775 word types\n",
      "INFO : PROGRESS: at sentence #470000, processed 10013054 words and 1976984 word types\n",
      "INFO : PROGRESS: at sentence #480000, processed 10226387 words and 2004368 word types\n",
      "INFO : PROGRESS: at sentence #490000, processed 10439317 words and 2032935 word types\n",
      "INFO : PROGRESS: at sentence #500000, processed 10653822 words and 2060954 word types\n",
      "INFO : PROGRESS: at sentence #510000, processed 10867755 words and 2088409 word types\n",
      "INFO : PROGRESS: at sentence #520000, processed 11079055 words and 2115436 word types\n",
      "INFO : PROGRESS: at sentence #530000, processed 11292761 words and 2142822 word types\n",
      "INFO : PROGRESS: at sentence #540000, processed 11505580 words and 2170215 word types\n",
      "INFO : PROGRESS: at sentence #550000, processed 11718216 words and 2197306 word types\n",
      "INFO : PROGRESS: at sentence #560000, processed 11930925 words and 2223896 word types\n",
      "INFO : PROGRESS: at sentence #570000, processed 12148105 words and 2251521 word types\n",
      "INFO : PROGRESS: at sentence #580000, processed 12361727 words and 2277334 word types\n",
      "INFO : PROGRESS: at sentence #590000, processed 12576522 words and 2302963 word types\n",
      "INFO : PROGRESS: at sentence #600000, processed 12789370 words and 2328279 word types\n",
      "INFO : PROGRESS: at sentence #610000, processed 13001789 words and 2353711 word types\n",
      "INFO : PROGRESS: at sentence #620000, processed 13211937 words and 2379040 word types\n",
      "INFO : PROGRESS: at sentence #630000, processed 13425009 words and 2404409 word types\n",
      "INFO : PROGRESS: at sentence #640000, processed 13640792 words and 2430052 word types\n",
      "INFO : PROGRESS: at sentence #650000, processed 13856064 words and 2455294 word types\n",
      "INFO : PROGRESS: at sentence #660000, processed 14066682 words and 2479703 word types\n",
      "INFO : PROGRESS: at sentence #670000, processed 14278529 words and 2504388 word types\n",
      "INFO : PROGRESS: at sentence #680000, processed 14492425 words and 2529747 word types\n",
      "INFO : PROGRESS: at sentence #690000, processed 14704885 words and 2554565 word types\n",
      "INFO : PROGRESS: at sentence #700000, processed 14920149 words and 2578846 word types\n",
      "INFO : PROGRESS: at sentence #710000, processed 15132243 words and 2602929 word types\n",
      "INFO : PROGRESS: at sentence #720000, processed 15341225 words and 2626929 word types\n",
      "INFO : PROGRESS: at sentence #730000, processed 15556097 words and 2651563 word types\n",
      "INFO : PROGRESS: at sentence #740000, processed 15770396 words and 2675668 word types\n",
      "INFO : PROGRESS: at sentence #750000, processed 15981495 words and 2699084 word types\n",
      "INFO : PROGRESS: at sentence #760000, processed 16195577 words and 2723315 word types\n",
      "INFO : PROGRESS: at sentence #770000, processed 16411107 words and 2746895 word types\n",
      "INFO : PROGRESS: at sentence #780000, processed 16621199 words and 2770028 word types\n",
      "INFO : PROGRESS: at sentence #790000, processed 16833781 words and 2792948 word types\n",
      "INFO : PROGRESS: at sentence #800000, processed 17046056 words and 2816252 word types\n",
      "INFO : PROGRESS: at sentence #810000, processed 17257197 words and 2839230 word types\n",
      "INFO : PROGRESS: at sentence #820000, processed 17471214 words and 2862550 word types\n",
      "INFO : PROGRESS: at sentence #830000, processed 17686367 words and 2886018 word types\n",
      "INFO : PROGRESS: at sentence #840000, processed 17899828 words and 2909331 word types\n",
      "INFO : PROGRESS: at sentence #850000, processed 18113761 words and 2932206 word types\n",
      "INFO : PROGRESS: at sentence #860000, processed 18327852 words and 2954663 word types\n",
      "INFO : PROGRESS: at sentence #870000, processed 18540403 words and 2977123 word types\n",
      "INFO : PROGRESS: at sentence #880000, processed 18755630 words and 2999944 word types\n",
      "INFO : PROGRESS: at sentence #890000, processed 18969443 words and 3022492 word types\n",
      "INFO : PROGRESS: at sentence #900000, processed 19181201 words and 3044371 word types\n",
      "INFO : PROGRESS: at sentence #910000, processed 19393669 words and 3066643 word types\n",
      "INFO : PROGRESS: at sentence #920000, processed 19606989 words and 3088398 word types\n",
      "INFO : PROGRESS: at sentence #930000, processed 19822363 words and 3110801 word types\n",
      "INFO : PROGRESS: at sentence #940000, processed 20033812 words and 3132885 word types\n",
      "INFO : PROGRESS: at sentence #950000, processed 20246974 words and 3154527 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: at sentence #960000, processed 20458046 words and 3175479 word types\n",
      "INFO : PROGRESS: at sentence #970000, processed 20669430 words and 3197080 word types\n",
      "INFO : PROGRESS: at sentence #980000, processed 20882843 words and 3218738 word types\n",
      "INFO : PROGRESS: at sentence #990000, processed 21094917 words and 3239744 word types\n",
      "INFO : PROGRESS: at sentence #1000000, processed 21309038 words and 3260776 word types\n",
      "INFO : PROGRESS: at sentence #1010000, processed 21521576 words and 3281811 word types\n",
      "INFO : PROGRESS: at sentence #1020000, processed 21735337 words and 3303272 word types\n",
      "INFO : PROGRESS: at sentence #1030000, processed 21948192 words and 3324630 word types\n",
      "INFO : PROGRESS: at sentence #1040000, processed 22160342 words and 3345987 word types\n",
      "INFO : PROGRESS: at sentence #1050000, processed 22372788 words and 3366620 word types\n",
      "INFO : PROGRESS: at sentence #1060000, processed 22584391 words and 3387750 word types\n",
      "INFO : PROGRESS: at sentence #1070000, processed 22799826 words and 3409082 word types\n",
      "INFO : PROGRESS: at sentence #1080000, processed 23014022 words and 3430154 word types\n",
      "INFO : PROGRESS: at sentence #1090000, processed 23230768 words and 3451035 word types\n",
      "INFO : PROGRESS: at sentence #1100000, processed 23443310 words and 3471651 word types\n",
      "INFO : PROGRESS: at sentence #1110000, processed 23655561 words and 3492231 word types\n",
      "INFO : PROGRESS: at sentence #1120000, processed 23870773 words and 3513036 word types\n",
      "INFO : PROGRESS: at sentence #1130000, processed 24083962 words and 3534086 word types\n",
      "INFO : PROGRESS: at sentence #1140000, processed 24296444 words and 3554176 word types\n",
      "INFO : PROGRESS: at sentence #1150000, processed 24511221 words and 3574433 word types\n",
      "INFO : PROGRESS: at sentence #1160000, processed 24725421 words and 3594774 word types\n",
      "INFO : PROGRESS: at sentence #1170000, processed 24939038 words and 3614966 word types\n",
      "INFO : PROGRESS: at sentence #1180000, processed 25153481 words and 3635312 word types\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-7dbf6d0cc2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigram_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'npmi'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, min_count, threshold, max_vocab_size, delimiter, progress_per, scoring, common_terms)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36madd_vocab\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;31m# counts collected in previous learn_vocab runs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         min_reduce, vocab, total_words = self.learn_vocab(\n\u001b[0;32m--> 424\u001b[0;31m             sentences, self.max_vocab_size, self.delimiter, self.progress_per, self.common_terms)\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36mlearn_vocab\u001b[0;34m(sentences, max_vocab_size, delimiter, progress_per, common_terms)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 )\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many2utf8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0mlast_uncommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0min_between\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 )\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many2utf8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0mlast_uncommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0min_between\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2utf8\u001b[0;34m(text, errors, encoding)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;31m# do bytestring -> unicode -> utf8 full circle, to ensure valid utf8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def bigramModel(review):\n",
    "    return bigram_model[review]\n",
    "bigram_model = Phrases(tokenized_text,scoring='npmi',threshold=0.5,min_count=100)\n",
    "p = Pool(24)\n",
    "processed_bigram_reviews = p.map(bigramModel,tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-395:\n",
      "Process ForkPoolWorker-399:\n",
      "Process ForkPoolWorker-392:\n",
      "Process ForkPoolWorker-382:\n",
      "Process ForkPoolWorker-389:\n",
      "Process ForkPoolWorker-397:\n",
      "Process ForkPoolWorker-403:\n",
      "Process ForkPoolWorker-391:\n",
      "Process ForkPoolWorker-383:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-387:\n",
      "Process ForkPoolWorker-386:\n",
      "Process ForkPoolWorker-398:\n",
      "Process ForkPoolWorker-396:\n",
      "Process ForkPoolWorker-402:\n",
      "Process ForkPoolWorker-394:\n",
      "Process ForkPoolWorker-400:\n",
      "Process ForkPoolWorker-393:\n",
      "Process ForkPoolWorker-390:\n",
      "Process ForkPoolWorker-388:\n",
      "Process ForkPoolWorker-405:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-385:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-404:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-384:\n",
      "Process ForkPoolWorker-401:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/data/ssubrahmannian/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(16355 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #20000 to Dictionary(24783 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #30000 to Dictionary(31624 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #40000 to Dictionary(37732 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #50000 to Dictionary(43042 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #60000 to Dictionary(47886 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #70000 to Dictionary(52611 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #80000 to Dictionary(57072 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #90000 to Dictionary(61263 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #100000 to Dictionary(65323 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #110000 to Dictionary(69193 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #120000 to Dictionary(72938 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #130000 to Dictionary(76701 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #140000 to Dictionary(80341 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #150000 to Dictionary(84086 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #160000 to Dictionary(87499 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #170000 to Dictionary(90768 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #180000 to Dictionary(94042 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #190000 to Dictionary(97443 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #200000 to Dictionary(100605 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #210000 to Dictionary(103902 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #220000 to Dictionary(107019 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #230000 to Dictionary(110206 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #240000 to Dictionary(113335 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #250000 to Dictionary(116194 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #260000 to Dictionary(119253 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #270000 to Dictionary(122156 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #280000 to Dictionary(125102 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #290000 to Dictionary(128088 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #300000 to Dictionary(130921 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #310000 to Dictionary(133791 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #320000 to Dictionary(136640 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #330000 to Dictionary(139471 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #340000 to Dictionary(142179 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #350000 to Dictionary(144815 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #360000 to Dictionary(147574 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #370000 to Dictionary(150055 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #380000 to Dictionary(152695 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #390000 to Dictionary(155269 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #400000 to Dictionary(157746 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #410000 to Dictionary(160337 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #420000 to Dictionary(162811 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #430000 to Dictionary(165370 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #440000 to Dictionary(167908 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #450000 to Dictionary(170421 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #460000 to Dictionary(172963 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #470000 to Dictionary(175331 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #480000 to Dictionary(177669 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #490000 to Dictionary(180178 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #500000 to Dictionary(182575 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #510000 to Dictionary(185007 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #520000 to Dictionary(187294 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #530000 to Dictionary(189688 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #540000 to Dictionary(192202 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #550000 to Dictionary(194644 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #560000 to Dictionary(197055 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #570000 to Dictionary(199538 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #580000 to Dictionary(201842 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #590000 to Dictionary(204146 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #600000 to Dictionary(206412 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #610000 to Dictionary(208761 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #620000 to Dictionary(211033 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #630000 to Dictionary(213388 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #640000 to Dictionary(215749 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #650000 to Dictionary(218019 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #660000 to Dictionary(220246 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #670000 to Dictionary(222485 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #680000 to Dictionary(224799 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #690000 to Dictionary(227145 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #700000 to Dictionary(229279 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #710000 to Dictionary(231513 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #720000 to Dictionary(233767 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #730000 to Dictionary(235980 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #740000 to Dictionary(238124 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #750000 to Dictionary(240281 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #760000 to Dictionary(242503 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #770000 to Dictionary(244685 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #780000 to Dictionary(246844 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #790000 to Dictionary(248995 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #800000 to Dictionary(251129 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #810000 to Dictionary(253289 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #820000 to Dictionary(255411 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #830000 to Dictionary(257598 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #840000 to Dictionary(259814 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #850000 to Dictionary(261932 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #860000 to Dictionary(263992 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #870000 to Dictionary(266163 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #880000 to Dictionary(268274 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #890000 to Dictionary(270417 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #900000 to Dictionary(272406 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #910000 to Dictionary(274486 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #920000 to Dictionary(276440 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #930000 to Dictionary(278534 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #940000 to Dictionary(280660 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #950000 to Dictionary(282699 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #960000 to Dictionary(284578 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #970000 to Dictionary(286738 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #980000 to Dictionary(288777 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #990000 to Dictionary(290696 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1000000 to Dictionary(292730 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1010000 to Dictionary(294738 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1020000 to Dictionary(296749 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1030000 to Dictionary(298835 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1040000 to Dictionary(300899 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1050000 to Dictionary(302824 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1060000 to Dictionary(304891 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1070000 to Dictionary(306929 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1080000 to Dictionary(308939 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1090000 to Dictionary(310901 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1100000 to Dictionary(312851 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1110000 to Dictionary(314762 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1120000 to Dictionary(316746 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1130000 to Dictionary(318869 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1140000 to Dictionary(320832 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1150000 to Dictionary(322718 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1160000 to Dictionary(324707 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1170000 to Dictionary(326629 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n",
      "INFO : adding document #1180000 to Dictionary(328662 unique tokens: ['4+)/bottom', 'great_condition', 'kid', 'kid_boy', 'l']...)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-7e9ac606391a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_bigram_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# doc_term_matrix = [dictionary.doc2bow(doc) for doc in tqdm(processed_bigram_reviews)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_bigram_reviews)\n",
    "# doc_term_matrix = [dictionary.doc2bow(doc) for doc in tqdm(processed_bigram_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_term_matrix = p.map(dictionary.doc2bow,processed_bigram_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[AINFO : using symmetric alpha at 0.2\n",
      "INFO : using symmetric eta at 0.2\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 5 topics, 3 passes over the supplied corpus of 1186028 documents, updating every 88000 documents, evaluating every ~880000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 11 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30DEBUG : processing chunk #5 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4964/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4971/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 33\n",
      "DEBUG : 4781/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4921/8000 documents converged within 50 iterations\n",
      "DEBUG : 4974/8000 documents converged within 50 iterations\n",
      "DEBUG : 4959/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4962/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 31\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 32DEBUG : 4904/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4814/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4951/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4946/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.026*\"size\" + 0.009*\"1\" + 0.008*\"man\" + 0.008*\"color\" + 0.008*\"black\" + 0.007*\"home\" + 0.006*\"woman\" + 0.006*\"case\" + 0.006*\"3\" + 0.006*\"4\"\n",
      "INFO : topic #1 (0.200): 0.036*\"woman\" + 0.028*\"size\" + 0.011*\"new\" + 0.010*\"brand_new\" + 0.009*\"black\" + 0.009*\"rm\" + 0.009*\"dress\" + 0.008*\"2\" + 0.008*\"small\" + 0.008*\"legging\"\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.015*\"black\" + 0.012*\"size\" + 0.008*\"color\" + 0.008*\"bundle\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.006*\"3\" + 0.006*\"rm\" + 0.006*\"pink\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.010*\"woman\" + 0.009*\"brand_new\" + 0.008*\"shirt\" + 0.007*\"beauty_makeup\" + 0.006*\"size\" + 0.006*\"item\" + 0.006*\"woman_athletic\" + 0.006*\"1\" + 0.006*\"color\"\n",
      "INFO : topic #4 (0.200): 0.020*\"size\" + 0.015*\"woman\" + 0.011*\"pink\" + 0.010*\"2\" + 0.010*\"brand_new\" + 0.010*\"rm\" + 0.009*\"shoe\" + 0.009*\"new\" + 0.009*\"shirt\" + 0.007*\"small\"\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=6.603693, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 33\n",
      "DEBUG : 4864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4920/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 33\n",
      "DEBUG : 4872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 4962/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4923/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4915/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 4941/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 32\n",
      "DEBUG : 5811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 4979/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 4935/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5853/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.026*\"size\" + 0.009*\"1\" + 0.008*\"man\" + 0.008*\"color\" + 0.008*\"black\" + 0.007*\"home\" + 0.006*\"case\" + 0.006*\"4\" + 0.006*\"3\" + 0.006*\"woman\"\n",
      "INFO : topic #1 (0.200): 0.036*\"woman\" + 0.028*\"size\" + 0.011*\"new\" + 0.010*\"brand_new\" + 0.010*\"black\" + 0.009*\"rm\" + 0.009*\"dress\" + 0.008*\"2\" + 0.008*\"small\" + 0.008*\"legging\"\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.015*\"black\" + 0.011*\"size\" + 0.008*\"color\" + 0.008*\"bundle\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.006*\"rm\" + 0.006*\"pink\" + 0.006*\"3\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.010*\"woman\" + 0.010*\"brand_new\" + 0.008*\"shirt\" + 0.007*\"beauty_makeup\" + 0.006*\"size\" + 0.006*\"item\" + 0.006*\"woman_athletic\" + 0.006*\"1\" + 0.006*\"color\"\n",
      "INFO : topic #4 (0.200): 0.020*\"size\" + 0.015*\"woman\" + 0.012*\"pink\" + 0.010*\"brand_new\" + 0.010*\"2\" + 0.010*\"rm\" + 0.009*\"shoe\" + 0.009*\"new\" + 0.009*\"shirt\" + 0.007*\"small\"\n",
      "INFO : topic diff=0.309061, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 33\n",
      "DEBUG : 5887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 33\n",
      "DEBUG : 5847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5910/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 5928/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 31\n",
      "DEBUG : 5952/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5938/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 33\n",
      "DEBUG : 5887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 5874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.025*\"size\" + 0.010*\"1\" + 0.009*\"man\" + 0.008*\"color\" + 0.008*\"home\" + 0.008*\"black\" + 0.007*\"case\" + 0.006*\"2\" + 0.006*\"4\" + 0.006*\"3\"\n",
      "INFO : topic #1 (0.200): 0.038*\"woman\" + 0.030*\"size\" + 0.011*\"new\" + 0.010*\"black\" + 0.010*\"brand_new\" + 0.009*\"dress\" + 0.009*\"small\" + 0.009*\"legging\" + 0.008*\"rm\" + 0.008*\"pink\"\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.014*\"black\" + 0.011*\"size\" + 0.009*\"color\" + 0.008*\"brand_new\" + 0.008*\"bundle\" + 0.008*\"new\" + 0.007*\"bag\" + 0.006*\"beauty_makeup\" + 0.006*\"rm\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.010*\"brand_new\" + 0.009*\"woman\" + 0.008*\"beauty_makeup\" + 0.007*\"shirt\" + 0.007*\"item\" + 0.006*\"game\" + 0.006*\"lip\" + 0.006*\"1\" + 0.006*\"color\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.200): 0.021*\"size\" + 0.014*\"woman\" + 0.012*\"pink\" + 0.010*\"shirt\" + 0.010*\"brand_new\" + 0.010*\"shoe\" + 0.010*\"2\" + 0.010*\"rm\" + 0.009*\"new\" + 0.008*\"man\"\n",
      "INFO : topic diff=0.237370, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 33\n",
      "DEBUG : 5922/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5939/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5922/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5933/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5976/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5926/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 33\n",
      "DEBUG : 5953/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5955/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5937/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 33\n",
      "DEBUG : 5985/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5944/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.024*\"size\" + 0.010*\"1\" + 0.009*\"man\" + 0.008*\"home\" + 0.008*\"color\" + 0.007*\"black\" + 0.007*\"case\" + 0.007*\"2\" + 0.007*\"4\" + 0.006*\"home_dcor\"\n",
      "INFO : topic #1 (0.200): 0.039*\"woman\" + 0.030*\"size\" + 0.010*\"new\" + 0.010*\"black\" + 0.010*\"dress\" + 0.010*\"brand_new\" + 0.009*\"legging\" + 0.009*\"small\" + 0.008*\"pink\" + 0.008*\"rm\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.014*\"black\" + 0.010*\"size\" + 0.009*\"color\" + 0.008*\"brand_new\" + 0.008*\"bag\" + 0.008*\"new\" + 0.008*\"bundle\" + 0.006*\"beauty_makeup\" + 0.006*\"woman_top\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.010*\"brand_new\" + 0.009*\"beauty_makeup\" + 0.008*\"woman\" + 0.007*\"item\" + 0.007*\"game\" + 0.007*\"shirt\" + 0.006*\"lip\" + 0.006*\"box\" + 0.006*\"1\"\n",
      "INFO : topic #4 (0.200): 0.021*\"size\" + 0.013*\"woman\" + 0.012*\"pink\" + 0.012*\"shirt\" + 0.010*\"shoe\" + 0.010*\"brand_new\" + 0.010*\"2\" + 0.010*\"rm\" + 0.010*\"new\" + 0.008*\"man\"\n",
      "INFO : topic diff=0.198907, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 33\n",
      "DEBUG : 6270/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6323/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6304/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6286/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6295/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 33\n",
      "DEBUG : 6263/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 33\n",
      "DEBUG : 6211/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 6230/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6250/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6330/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 6293/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6428/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.024*\"size\" + 0.010*\"1\" + 0.009*\"man\" + 0.009*\"home\" + 0.008*\"case\" + 0.007*\"color\" + 0.007*\"black\" + 0.007*\"2\" + 0.007*\"home_dcor\" + 0.007*\"4\"\n",
      "INFO : topic #1 (0.200): 0.040*\"woman\" + 0.031*\"size\" + 0.011*\"black\" + 0.010*\"new\" + 0.010*\"dress\" + 0.009*\"brand_new\" + 0.009*\"legging\" + 0.009*\"small\" + 0.009*\"pink\" + 0.009*\"woman_athletic\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.014*\"black\" + 0.009*\"size\" + 0.009*\"bag\" + 0.009*\"color\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"bundle\" + 0.006*\"beauty_makeup\" + 0.006*\"rm\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.011*\"brand_new\" + 0.009*\"beauty_makeup\" + 0.007*\"item\" + 0.007*\"game\" + 0.007*\"woman\" + 0.007*\"lip\" + 0.006*\"box\" + 0.006*\"color\" + 0.006*\"1\"\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.200): 0.021*\"size\" + 0.013*\"shirt\" + 0.012*\"woman\" + 0.012*\"pink\" + 0.011*\"shoe\" + 0.010*\"rm\" + 0.010*\"brand_new\" + 0.010*\"2\" + 0.010*\"new\" + 0.009*\"man\"\n",
      "INFO : topic diff=0.173128, rho=0.149071\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 33\n",
      "DEBUG : 6477/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6421/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6525/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 33\n",
      "DEBUG : 6493/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : 6451/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6418/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : 6447/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6441/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6467/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.023*\"size\" + 0.010*\"1\" + 0.009*\"home\" + 0.009*\"man\" + 0.008*\"case\" + 0.007*\"2\" + 0.007*\"home_dcor\" + 0.007*\"color\" + 0.007*\"black\" + 0.007*\"4\"\n",
      "DEBUG : 6463/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #1 (0.200): 0.041*\"woman\" + 0.032*\"size\" + 0.011*\"black\" + 0.011*\"dress\" + 0.010*\"new\" + 0.010*\"legging\" + 0.009*\"small\" + 0.009*\"brand_new\" + 0.009*\"woman_athletic\" + 0.009*\"pink\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.013*\"black\" + 0.010*\"bag\" + 0.009*\"size\" + 0.009*\"color\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"bundle\" + 0.006*\"woman_handbag\" + 0.006*\"bra\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.011*\"brand_new\" + 0.010*\"beauty_makeup\" + 0.008*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.006*\"woman\" + 0.006*\"color\" + 0.006*\"1\"\n",
      "INFO : topic #4 (0.200): 0.021*\"size\" + 0.014*\"shirt\" + 0.012*\"pink\" + 0.012*\"woman\" + 0.011*\"shoe\" + 0.010*\"rm\" + 0.010*\"brand_new\" + 0.010*\"2\" + 0.010*\"new\" + 0.009*\"man\"\n",
      "DEBUG : 6670/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic diff=0.162657, rho=0.133631\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 32\n",
      "DEBUG : 6603/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6607/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6602/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6690/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6697/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 33\n",
      "DEBUG : 6648/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 33\n",
      "DEBUG : 6780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6662/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6815/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6808/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.022*\"size\" + 0.010*\"1\" + 0.010*\"home\" + 0.009*\"man\" + 0.009*\"case\" + 0.008*\"2\" + 0.008*\"home_dcor\" + 0.007*\"color\" + 0.007*\"black\" + 0.007*\"4\"\n",
      "INFO : topic #1 (0.200): 0.042*\"woman\" + 0.032*\"size\" + 0.011*\"black\" + 0.011*\"dress\" + 0.010*\"new\" + 0.010*\"legging\" + 0.010*\"small\" + 0.009*\"woman_athletic\" + 0.009*\"pink\" + 0.009*\"brand_new\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.013*\"black\" + 0.011*\"bag\" + 0.009*\"color\" + 0.009*\"size\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"bundle\" + 0.007*\"woman_handbag\" + 0.006*\"bra\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.011*\"brand_new\" + 0.010*\"beauty_makeup\" + 0.008*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.006*\"color\" + 0.006*\"2\" + 0.006*\"1\"\n",
      "INFO : topic #4 (0.200): 0.022*\"size\" + 0.015*\"shirt\" + 0.012*\"pink\" + 0.012*\"shoe\" + 0.011*\"woman\" + 0.010*\"rm\" + 0.010*\"new\" + 0.010*\"brand_new\" + 0.010*\"2\" + 0.010*\"man\"\n",
      "DEBUG : 6737/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.155129, rho=0.122169\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 33DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 33\n",
      "DEBUG : 6792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 33\n",
      "DEBUG : 6805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6699/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6808/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 33\n",
      "DEBUG : 6751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6745/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #0 (0.200): 0.021*\"size\" + 0.010*\"1\" + 0.010*\"home\" + 0.009*\"case\" + 0.009*\"man\" + 0.008*\"home_dcor\" + 0.008*\"2\" + 0.007*\"3\" + 0.007*\"4\" + 0.007*\"color\"\n",
      "INFO : topic #1 (0.200): 0.042*\"woman\" + 0.033*\"size\" + 0.012*\"black\" + 0.011*\"dress\" + 0.010*\"new\" + 0.010*\"woman_athletic\" + 0.010*\"legging\" + 0.010*\"small\" + 0.009*\"pink\" + 0.009*\"brand_new\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.013*\"black\" + 0.012*\"bag\" + 0.009*\"color\" + 0.008*\"size\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"bundle\" + 0.007*\"woman_handbag\" + 0.007*\"bra\"\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.011*\"brand_new\" + 0.011*\"beauty_makeup\" + 0.008*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.006*\"color\" + 0.006*\"1\"\n",
      "INFO : topic #4 (0.200): 0.022*\"size\" + 0.017*\"shirt\" + 0.012*\"shoe\" + 0.012*\"pink\" + 0.011*\"woman\" + 0.010*\"man\" + 0.010*\"rm\" + 0.010*\"new\" + 0.010*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic diff=0.148384, rho=0.113228\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 32\n",
      "DEBUG : 6834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6941/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 33\n",
      "DEBUG : 6888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6929/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 33\n",
      "DEBUG : 6913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.021*\"size\" + 0.011*\"home\" + 0.010*\"1\" + 0.010*\"case\" + 0.009*\"man\" + 0.008*\"home_dcor\" + 0.008*\"2\" + 0.007*\"3\" + 0.007*\"4\" + 0.007*\"color\"\n",
      "INFO : topic #1 (0.200): 0.043*\"woman\" + 0.033*\"size\" + 0.012*\"black\" + 0.011*\"dress\" + 0.010*\"woman_athletic\" + 0.010*\"new\" + 0.010*\"legging\" + 0.010*\"small\" + 0.010*\"pink\" + 0.009*\"brand_new\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.013*\"bag\" + 0.012*\"black\" + 0.009*\"color\" + 0.008*\"size\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"woman_handbag\" + 0.007*\"bra\" + 0.007*\"bundle\"\n",
      "INFO : topic #3 (0.200): 0.017*\"new\" + 0.011*\"brand_new\" + 0.011*\"beauty_makeup\" + 0.008*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.007*\"color\" + 0.007*\"rm\"\n",
      "INFO : topic #4 (0.200): 0.022*\"size\" + 0.018*\"shirt\" + 0.013*\"shoe\" + 0.012*\"pink\" + 0.011*\"woman\" + 0.011*\"man\" + 0.010*\"rm\" + 0.010*\"new\" + 0.010*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic diff=0.142204, rho=0.106000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 33\n",
      "DEBUG : 7015/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6916/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6979/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 32\n",
      "DEBUG : 7011/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 32\n",
      "DEBUG : 6924/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7011/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 33\n",
      "DEBUG : 7028/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 33\n",
      "DEBUG : 7007/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7061/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7023/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 33\n",
      "DEBUG : 7013/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7014/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.020*\"size\" + 0.011*\"home\" + 0.011*\"1\" + 0.010*\"case\" + 0.009*\"man\" + 0.008*\"home_dcor\" + 0.008*\"2\" + 0.007*\"3\" + 0.007*\"4\" + 0.007*\"color\"\n",
      "INFO : topic #1 (0.200): 0.043*\"woman\" + 0.034*\"size\" + 0.012*\"black\" + 0.011*\"dress\" + 0.010*\"woman_athletic\" + 0.010*\"new\" + 0.010*\"legging\" + 0.010*\"pink\" + 0.010*\"small\" + 0.009*\"brand_new\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.013*\"bag\" + 0.012*\"black\" + 0.009*\"color\" + 0.008*\"brand_new\" + 0.008*\"size\" + 0.008*\"woman_handbag\" + 0.008*\"new\" + 0.008*\"bra\" + 0.007*\"kid_toy\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.012*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "DEBUG : 7078/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7080/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.200): 0.023*\"size\" + 0.019*\"shirt\" + 0.013*\"shoe\" + 0.012*\"pink\" + 0.011*\"man\" + 0.010*\"woman\" + 0.010*\"rm\" + 0.010*\"new\" + 0.010*\"brand_new\" + 0.009*\"2\"\n",
      "INFO : topic diff=0.139629, rho=0.100000\n",
      "DEBUG : bound: at document #0\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7076/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7097/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7079/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : 7049/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7109/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7092/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7133/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7100/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7165/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7055/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7146/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7165/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7148/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7164/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7175/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7160/8000 documents converged within 50 iterations\n",
      "DEBUG : 7155/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7134/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7193/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7143/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7179/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7146/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7183/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7160/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7138/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7156/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7145/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7179/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7168/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : -7.746 per-word bound, 214.7 perplexity estimate based on a held-out corpus of 8000 documents with 143156 words\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.019*\"size\" + 0.011*\"home\" + 0.011*\"case\" + 0.011*\"1\" + 0.009*\"man\" + 0.008*\"home_dcor\" + 0.008*\"2\" + 0.007*\"3\" + 0.007*\"4\" + 0.006*\"color\"\n",
      "INFO : topic #1 (0.200): 0.043*\"woman\" + 0.034*\"size\" + 0.012*\"black\" + 0.011*\"dress\" + 0.011*\"woman_athletic\" + 0.010*\"pink\" + 0.010*\"legging\" + 0.010*\"new\" + 0.010*\"small\" + 0.009*\"brand_new\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.014*\"bag\" + 0.012*\"black\" + 0.009*\"color\" + 0.008*\"woman_handbag\" + 0.008*\"brand_new\" + 0.008*\"bra\" + 0.008*\"new\" + 0.008*\"size\" + 0.007*\"kid_toy\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.012*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.023*\"size\" + 0.020*\"shirt\" + 0.014*\"shoe\" + 0.012*\"pink\" + 0.012*\"man\" + 0.010*\"new\" + 0.010*\"rm\" + 0.010*\"woman\" + 0.009*\"brand_new\" + 0.009*\"2\"\n",
      "INFO : topic diff=0.142872, rho=0.094916\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "INFO : topic #0 (0.200): 0.019*\"size\" + 0.012*\"case\" + 0.011*\"home\" + 0.011*\"1\" + 0.009*\"man\" + 0.009*\"2\" + 0.008*\"home_dcor\" + 0.007*\"3\" + 0.007*\"4\" + 0.006*\"color\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.034*\"size\" + 0.012*\"black\" + 0.011*\"dress\" + 0.011*\"woman_athletic\" + 0.010*\"pink\" + 0.010*\"legging\" + 0.010*\"small\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.014*\"bag\" + 0.012*\"black\" + 0.009*\"color\" + 0.008*\"woman_handbag\" + 0.008*\"bra\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"kid_toy\" + 0.007*\"size\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.013*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.023*\"size\" + 0.021*\"shirt\" + 0.014*\"shoe\" + 0.012*\"man\" + 0.012*\"pink\" + 0.010*\"new\" + 0.010*\"rm\" + 0.010*\"woman\" + 0.009*\"woman_top\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.132218, rho=0.089803\n",
      "DEBUG : 7116/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7138/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1822/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7165/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : 7159/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.018*\"size\" + 0.012*\"case\" + 0.012*\"home\" + 0.011*\"1\" + 0.009*\"man\" + 0.009*\"2\" + 0.009*\"home_dcor\" + 0.007*\"3\" + 0.007*\"4\" + 0.006*\"brand_new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.034*\"size\" + 0.012*\"black\" + 0.011*\"dress\" + 0.011*\"woman_athletic\" + 0.010*\"pink\" + 0.010*\"legging\" + 0.010*\"small\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.015*\"bag\" + 0.012*\"black\" + 0.009*\"color\" + 0.009*\"woman_handbag\" + 0.009*\"bra\" + 0.008*\"brand_new\" + 0.008*\"kid_toy\" + 0.008*\"new\" + 0.007*\"size\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.013*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.008*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.023*\"size\" + 0.021*\"shirt\" + 0.014*\"shoe\" + 0.012*\"man\" + 0.012*\"pink\" + 0.010*\"new\" + 0.010*\"rm\" + 0.010*\"woman_top\" + 0.009*\"woman\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.130597, rho=0.086066\n",
      "DEBUG : 7199/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7225/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 24000 documents into a model of 1186028 documents\n",
      "INFO : topic #0 (0.200): 0.018*\"size\" + 0.012*\"case\" + 0.012*\"home\" + 0.011*\"1\" + 0.009*\"2\" + 0.009*\"man\" + 0.009*\"home_dcor\" + 0.007*\"3\" + 0.007*\"4\" + 0.006*\"brand_new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.035*\"size\" + 0.013*\"black\" + 0.011*\"woman_athletic\" + 0.011*\"dress\" + 0.010*\"pink\" + 0.010*\"legging\" + 0.010*\"small\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.015*\"bag\" + 0.011*\"black\" + 0.009*\"woman_handbag\" + 0.009*\"color\" + 0.009*\"bra\" + 0.008*\"kid_toy\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"size\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.013*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.007*\"lip\" + 0.007*\"box\" + 0.007*\"2\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.024*\"size\" + 0.022*\"shirt\" + 0.014*\"shoe\" + 0.013*\"man\" + 0.012*\"pink\" + 0.010*\"new\" + 0.010*\"woman_top\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.083091, rho=0.082689\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.645 per-word bound, 200.2 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7392/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7315/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 33\n",
      "DEBUG : 7326/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7352/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7309/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7363/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7360/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7333/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7294/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7375/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 32\n",
      "DEBUG : 7377/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7335/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.017*\"size\" + 0.013*\"case\" + 0.012*\"home\" + 0.011*\"1\" + 0.009*\"2\" + 0.009*\"home_dcor\" + 0.009*\"man\" + 0.007*\"3\" + 0.007*\"4\" + 0.007*\"electronic_cell\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.035*\"size\" + 0.013*\"black\" + 0.012*\"woman_athletic\" + 0.011*\"dress\" + 0.011*\"pink\" + 0.010*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.016*\"bag\" + 0.011*\"black\" + 0.009*\"woman_handbag\" + 0.009*\"bra\" + 0.009*\"color\" + 0.009*\"kid_toy\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"size\"DEBUG : processing chunk #21 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.013*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"lip\" + 0.007*\"2\" + 0.007*\"box\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.024*\"size\" + 0.023*\"shirt\" + 0.015*\"shoe\" + 0.013*\"man\" + 0.011*\"pink\" + 0.010*\"new\" + 0.010*\"woman_top\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.055499, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 33\n",
      "DEBUG : 7296/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7357/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7338/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 31\n",
      "DEBUG : 7390/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 32DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : 7354/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 32\n",
      "DEBUG : 7390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7348/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7336/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7365/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 32\n",
      "DEBUG : 7376/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7376/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7388/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7401/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.016*\"size\" + 0.013*\"case\" + 0.012*\"home\" + 0.011*\"1\" + 0.009*\"home_dcor\" + 0.009*\"2\" + 0.008*\"man\" + 0.007*\"3\" + 0.007*\"4\" + 0.007*\"electronic_cell\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.035*\"size\" + 0.013*\"black\" + 0.012*\"woman_athletic\" + 0.011*\"dress\" + 0.011*\"pink\" + 0.010*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.018*\"woman\" + 0.016*\"bag\" + 0.011*\"black\" + 0.009*\"bra\" + 0.009*\"woman_handbag\" + 0.009*\"color\" + 0.009*\"kid_toy\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"size\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.014*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"lip\" + 0.008*\"2\" + 0.008*\"box\" + 0.007*\"rm\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.024*\"size\" + 0.023*\"shirt\" + 0.015*\"shoe\" + 0.013*\"man\" + 0.011*\"pink\" + 0.010*\"woman_top\" + 0.010*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.068178, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "DEBUG : 7403/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 32\n",
      "DEBUG : 7430/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7414/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7375/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 33DEBUG : 7407/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7430/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : 7406/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7360/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.016*\"size\" + 0.014*\"case\" + 0.012*\"home\" + 0.011*\"1\" + 0.009*\"home_dcor\" + 0.009*\"2\" + 0.008*\"man\" + 0.007*\"3\" + 0.007*\"4\" + 0.007*\"electronic_cell\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.035*\"size\" + 0.013*\"black\" + 0.012*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.010*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.016*\"bag\" + 0.011*\"black\" + 0.010*\"bra\" + 0.009*\"woman_handbag\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"size\"\n",
      "DEBUG : 7380/8000 documents converged within 50 iterations\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.014*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"rm\" + 0.007*\"color\"DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.200): 0.024*\"size\" + 0.024*\"shirt\" + 0.015*\"shoe\" + 0.014*\"man\" + 0.011*\"pink\" + 0.011*\"woman_top\" + 0.010*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.067479, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 34\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7412/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7459/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7447/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : 7394/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 32\n",
      "DEBUG : 7388/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 32\n",
      "DEBUG : 7429/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7429/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7407/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7461/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7395/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.015*\"size\" + 0.014*\"case\" + 0.012*\"home\" + 0.011*\"1\" + 0.009*\"2\" + 0.009*\"home_dcor\" + 0.008*\"man\" + 0.007*\"3\" + 0.007*\"electronic_cell\" + 0.007*\"phone_accessory\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.035*\"size\" + 0.013*\"black\" + 0.012*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.010*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.017*\"bag\" + 0.011*\"black\" + 0.010*\"bra\" + 0.010*\"woman_handbag\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"size\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.014*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"rm\" + 0.008*\"box\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.025*\"size\" + 0.024*\"shirt\" + 0.015*\"shoe\" + 0.014*\"man\" + 0.011*\"pink\" + 0.011*\"woman_top\" + 0.010*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.068794, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 32\n",
      "DEBUG : 7473/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7463/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 32\n",
      "DEBUG : 7464/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7412/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7437/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7480/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7433/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7446/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7420/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7457/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7452/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7491/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.015*\"size\" + 0.015*\"case\" + 0.013*\"home\" + 0.011*\"1\" + 0.009*\"home_dcor\" + 0.009*\"2\" + 0.008*\"man\" + 0.008*\"electronic_cell\" + 0.008*\"phone_accessory\" + 0.007*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.036*\"size\" + 0.013*\"black\" + 0.012*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.010*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "DEBUG : 7476/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.017*\"bag\" + 0.011*\"black\" + 0.010*\"bra\" + 0.010*\"woman_handbag\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.015*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"2\" + 0.008*\"rm\" + 0.008*\"lip\" + 0.008*\"box\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.025*\"shirt\" + 0.025*\"size\" + 0.016*\"shoe\" + 0.015*\"man\" + 0.011*\"woman_top\" + 0.011*\"pink\" + 0.010*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.067534, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 33\n",
      "DEBUG : 7442/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 31\n",
      "DEBUG : 7441/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7507/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7465/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 31DEBUG : processing chunk #70 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7468/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 32DEBUG : 7470/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7464/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 32\n",
      "DEBUG : 7469/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7475/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7444/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.015*\"case\" + 0.014*\"size\" + 0.013*\"home\" + 0.011*\"1\" + 0.009*\"home_dcor\" + 0.009*\"2\" + 0.008*\"electronic_cell\" + 0.008*\"phone_accessory\" + 0.008*\"3\" + 0.007*\"man\"\n",
      "DEBUG : 7524/8000 documents converged within 50 iterations\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.036*\"size\" + 0.013*\"black\" + 0.013*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.010*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.017*\"bag\" + 0.010*\"black\" + 0.010*\"bra\" + 0.010*\"woman_handbag\" + 0.010*\"kid_toy\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.015*\"beauty_makeup\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"2\" + 0.008*\"rm\" + 0.008*\"lip\" + 0.008*\"box\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.200): 0.025*\"shirt\" + 0.025*\"size\" + 0.016*\"shoe\" + 0.015*\"man\" + 0.011*\"woman_top\" + 0.011*\"pink\" + 0.010*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.068724, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 33\n",
      "DEBUG : 7507/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 33DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7473/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7474/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7503/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : 7510/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7465/8000 documents converged within 50 iterations\n",
      "DEBUG : 7507/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7436/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7513/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7530/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.015*\"case\" + 0.014*\"size\" + 0.013*\"home\" + 0.011*\"1\" + 0.009*\"2\" + 0.009*\"home_dcor\" + 0.008*\"electronic_cell\" + 0.008*\"phone_accessory\" + 0.008*\"3\" + 0.007*\"man\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.036*\"size\" + 0.013*\"black\" + 0.013*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.009*\"lularoe\"\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.017*\"bag\" + 0.011*\"bra\" + 0.010*\"black\" + 0.010*\"woman_handbag\" + 0.010*\"kid_toy\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "DEBUG : 7523/8000 documents converged within 50 iterations\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.015*\"beauty_makeup\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"2\" + 0.008*\"rm\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"color\"\n",
      "INFO : topic #4 (0.200): 0.026*\"shirt\" + 0.025*\"size\" + 0.016*\"shoe\" + 0.015*\"man\" + 0.011*\"woman_top\" + 0.011*\"pink\" + 0.010*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.068916, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 31\n",
      "DEBUG : 7490/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 31\n",
      "DEBUG : 7513/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7516/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 32\n",
      "DEBUG : 7495/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7516/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 32\n",
      "DEBUG : 7514/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7475/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 32\n",
      "DEBUG : 7522/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 33\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7532/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.016*\"case\" + 0.013*\"size\" + 0.013*\"home\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"electronic_cell\" + 0.008*\"phone_accessory\" + 0.008*\"3\" + 0.007*\"4\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.036*\"size\" + 0.013*\"black\" + 0.013*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"new\" + 0.010*\"lularoe\"\n",
      "DEBUG : 7493/8000 documents converged within 50 iterations\n",
      "INFO : topic #2 (0.200): 0.019*\"woman\" + 0.018*\"bag\" + 0.011*\"bra\" + 0.010*\"black\" + 0.010*\"kid_toy\" + 0.010*\"woman_handbag\" + 0.008*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #3 (0.200): 0.016*\"new\" + 0.015*\"beauty_makeup\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"2\" + 0.008*\"rm\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"color\"DEBUG : getting a new job\n",
      "\n",
      "DEBUG : 7524/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.200): 0.026*\"shirt\" + 0.026*\"size\" + 0.016*\"shoe\" + 0.016*\"man\" + 0.011*\"woman_top\" + 0.010*\"new\" + 0.010*\"pink\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.069119, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7540/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7514/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 32\n",
      "DEBUG : 7548/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7521/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 32\n",
      "DEBUG : 7548/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 32\n",
      "DEBUG : 7519/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7514/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 32\n",
      "DEBUG : 7501/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7509/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7538/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7549/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.200): 0.016*\"case\" + 0.013*\"home\" + 0.013*\"size\" + 0.011*\"1\" + 0.010*\"home_dcor\" + 0.010*\"2\" + 0.009*\"electronic_cell\" + 0.009*\"phone_accessory\" + 0.008*\"3\" + 0.007*\"4\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.036*\"size\" + 0.013*\"black\" + 0.013*\"woman_athletic\" + 0.012*\"dress\" + 0.011*\"pink\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.010*\"new\"\n",
      "INFO : topic #2 (0.200): 0.020*\"woman\" + 0.018*\"bag\" + 0.011*\"bra\" + 0.010*\"kid_toy\" + 0.010*\"woman_handbag\" + 0.010*\"black\" + 0.008*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.200): 0.015*\"new\" + 0.015*\"beauty_makeup\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"color\"\n",
      "INFO : topic #4 (0.200): 0.027*\"shirt\" + 0.026*\"size\" + 0.016*\"shoe\" + 0.016*\"man\" + 0.011*\"woman_top\" + 0.010*\"new\" + 0.010*\"pink\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.069094, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 33DEBUG : 7528/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7556/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7532/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 31\n",
      "DEBUG : 7533/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7524/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 32\n",
      "DEBUG : 7549/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7558/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7575/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7544/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7537/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.016*\"case\" + 0.013*\"home\" + 0.012*\"size\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"phone_accessory\" + 0.009*\"electronic_cell\" + 0.008*\"3\" + 0.007*\"brand_new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.036*\"size\" + 0.013*\"black\" + 0.013*\"woman_athletic\" + 0.012*\"dress\" + 0.012*\"pink\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.010*\"new\"\n",
      "DEBUG : 7537/8000 documents converged within 50 iterations\n",
      "INFO : topic #2 (0.200): 0.020*\"woman\" + 0.018*\"bag\" + 0.011*\"bra\" + 0.010*\"kid_toy\" + 0.010*\"woman_handbag\" + 0.010*\"black\" + 0.008*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "DEBUG : 7549/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7576/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.200): 0.016*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"color\"\n",
      "INFO : topic #4 (0.200): 0.027*\"shirt\" + 0.026*\"size\" + 0.017*\"shoe\" + 0.017*\"man\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"pink\" + 0.009*\"rm\" + 0.009*\"blouse_t\" + 0.009*\"2\"\n",
      "INFO : topic diff=0.070966, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7557/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 32\n",
      "DEBUG : 7548/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7536/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7571/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7558/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7525/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7518/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7601/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.017*\"case\" + 0.013*\"home\" + 0.012*\"size\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"phone_accessory\" + 0.009*\"electronic_cell\" + 0.008*\"3\" + 0.007*\"brand_new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"black\" + 0.013*\"woman_athletic\" + 0.012*\"dress\" + 0.012*\"pink\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.020*\"woman\" + 0.018*\"bag\" + 0.011*\"bra\" + 0.011*\"kid_toy\" + 0.010*\"woman_handbag\" + 0.010*\"black\" + 0.008*\"color\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.016*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"color\"\n",
      "INFO : topic #4 (0.200): 0.028*\"shirt\" + 0.026*\"size\" + 0.017*\"man\" + 0.017*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"pink\" + 0.009*\"rm\" + 0.009*\"blouse_t\" + 0.009*\"2\"\n",
      "DEBUG : 7610/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.071676, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7565/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7543/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7563/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7591/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7547/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7538/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7576/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7583/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7600/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7566/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7575/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7569/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.017*\"case\" + 0.013*\"home\" + 0.011*\"size\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"phone_accessory\" + 0.009*\"electronic_cell\" + 0.008*\"3\" + 0.007*\"brand_new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"black\" + 0.014*\"woman_athletic\" + 0.012*\"dress\" + 0.012*\"pink\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.020*\"woman\" + 0.019*\"bag\" + 0.011*\"bra\" + 0.011*\"kid_toy\" + 0.010*\"woman_handbag\" + 0.010*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.016*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.009*\"game\" + 0.009*\"item\" + 0.008*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"box\" + 0.008*\"color\"\n",
      "INFO : topic #4 (0.200): 0.028*\"shirt\" + 0.026*\"size\" + 0.017*\"man\" + 0.017*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"pink\" + 0.009*\"rm\" + 0.009*\"blouse_t\" + 0.009*\"2\"\n",
      "INFO : topic diff=0.071478, rho=0.081581\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7594/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7573/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7573/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7555/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7582/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7584/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7590/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7587/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 1923/2028 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7549/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.200): 0.017*\"case\" + 0.013*\"home\" + 0.011*\"size\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"phone_accessory\" + 0.010*\"home_dcor\" + 0.010*\"electronic_cell\" + 0.008*\"3\" + 0.007*\"new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"black\" + 0.014*\"woman_athletic\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.021*\"woman\" + 0.019*\"bag\" + 0.012*\"bra\" + 0.011*\"kid_toy\" + 0.010*\"woman_handbag\" + 0.010*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.016*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.009*\"item\" + 0.008*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"color\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.029*\"shirt\" + 0.027*\"size\" + 0.018*\"man\" + 0.017*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"pink\" + 0.009*\"rm\" + 0.009*\"blouse_t\" + 0.009*\"2\"\n",
      "INFO : topic diff=0.071549, rho=0.081581\n",
      "DEBUG : 7623/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7598/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7626/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 32000 documents into a model of 1186028 documents\n",
      "INFO : topic #0 (0.200): 0.017*\"case\" + 0.013*\"home\" + 0.011*\"size\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"phone_accessory\" + 0.010*\"electronic_cell\" + 0.010*\"home_dcor\" + 0.008*\"3\" + 0.007*\"new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"black\" + 0.014*\"woman_athletic\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.021*\"woman\" + 0.019*\"bag\" + 0.012*\"bra\" + 0.011*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.016*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.009*\"item\" + 0.009*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"color\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.029*\"shirt\" + 0.027*\"size\" + 0.018*\"man\" + 0.017*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"pink\" + 0.009*\"blouse_t\" + 0.009*\"rm\" + 0.008*\"2\"\n",
      "INFO : topic diff=0.055330, rho=0.081581\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.556 per-word bound, 188.1 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29DEBUG : 7651/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7619/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7592/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7628/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7624/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7599/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 31\n",
      "DEBUG : 7613/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 33\n",
      "DEBUG : 7591/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 33\n",
      "DEBUG : 7574/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7607/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : 7624/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.200): 0.018*\"case\" + 0.014*\"home\" + 0.011*\"1\" + 0.010*\"size\" + 0.010*\"phone_accessory\" + 0.010*\"electronic_cell\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"3\" + 0.007*\"new\"\n",
      "DEBUG : result put\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.021*\"woman\" + 0.019*\"bag\" + 0.012*\"bra\" + 0.011*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.017*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.009*\"item\" + 0.009*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"color\" + 0.008*\"box\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #4 (0.200): 0.029*\"shirt\" + 0.027*\"size\" + 0.018*\"man\" + 0.017*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.009*\"pink\" + 0.009*\"blouse_t\" + 0.009*\"rm\" + 0.009*\"woman_jewelry\"\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "INFO : topic diff=0.058725, rho=0.081311\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 33\n",
      "DEBUG : 7591/8000 documents converged within 50 iterations\n",
      "DEBUG : 7631/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : 7607/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7587/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7644/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7588/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7614/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7616/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 32\n",
      "DEBUG : 7606/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7589/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7610/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.018*\"case\" + 0.014*\"home\" + 0.011*\"1\" + 0.010*\"phone_accessory\" + 0.010*\"electronic_cell\" + 0.010*\"2\" + 0.010*\"size\" + 0.010*\"home_dcor\" + 0.008*\"3\" + 0.008*\"new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.021*\"woman\" + 0.019*\"bag\" + 0.012*\"bra\" + 0.011*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.017*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.009*\"item\" + 0.009*\"rm\" + 0.008*\"2\" + 0.008*\"lip\" + 0.008*\"color\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.029*\"shirt\" + 0.027*\"size\" + 0.019*\"man\" + 0.017*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.009*\"pink\" + 0.009*\"blouse_t\" + 0.009*\"rm\" + 0.009*\"woman_jewelry\"\n",
      "DEBUG : 7620/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.065506, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 32\n",
      "DEBUG : 7573/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 33\n",
      "DEBUG : 7629/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7611/8000 documents converged within 50 iterations\n",
      "DEBUG : 7616/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 32\n",
      "DEBUG : 7627/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7634/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 31\n",
      "DEBUG : 7635/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 32\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7640/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.018*\"case\" + 0.014*\"home\" + 0.011*\"1\" + 0.011*\"phone_accessory\" + 0.011*\"electronic_cell\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.010*\"size\" + 0.008*\"3\" + 0.008*\"new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.022*\"woman\" + 0.019*\"bag\" + 0.012*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.017*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.009*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"lip\" + 0.008*\"color\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.030*\"shirt\" + 0.027*\"size\" + 0.019*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.009*\"blouse_t\" + 0.009*\"pink\" + 0.009*\"woman_jewelry\" + 0.009*\"rm\"\n",
      "DEBUG : 7661/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.063463, rho=0.081311\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 32\n",
      "DEBUG : 7648/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 32\n",
      "DEBUG : 7651/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 33\n",
      "DEBUG : 7640/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 33\n",
      "DEBUG : 7619/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7635/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7654/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7662/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 33DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7603/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7647/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : 7639/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.018*\"case\" + 0.014*\"home\" + 0.011*\"phone_accessory\" + 0.011*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"size\" + 0.008*\"3\" + 0.008*\"new\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.022*\"woman\" + 0.019*\"bag\" + 0.012*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"brand_new\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.017*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.009*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"lip\" + 0.008*\"color\" + 0.008*\"box\"\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "INFO : topic #4 (0.200): 0.030*\"shirt\" + 0.027*\"size\" + 0.019*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.009*\"blouse_t\" + 0.009*\"woman_jewelry\" + 0.009*\"pink\" + 0.009*\"rm\"DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.063808, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 33\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7664/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7651/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 32\n",
      "DEBUG : 7644/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : 7624/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 31DEBUG : 7643/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7626/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7636/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.018*\"case\" + 0.014*\"home\" + 0.011*\"phone_accessory\" + 0.011*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"size\" + 0.008*\"new\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.037*\"size\" + 0.014*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.200): 0.022*\"woman\" + 0.020*\"bag\" + 0.012*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"color\" + 0.007*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.017*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.030*\"shirt\" + 0.027*\"size\" + 0.020*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.009*\"blouse_t\" + 0.009*\"woman_jewelry\" + 0.009*\"pink\" + 0.009*\"rm\"\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "INFO : topic diff=0.061934, rho=0.081311\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7639/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 31\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7645/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7662/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 32DEBUG : 7673/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 33\n",
      "DEBUG : 7652/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7658/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7629/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7662/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7655/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7675/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7670/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.011*\"phone_accessory\" + 0.011*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"size\" + 0.008*\"new\" + 0.008*\"rm\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.014*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.010*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.023*\"woman\" + 0.020*\"bag\" + 0.012*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"color\" + 0.008*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.017*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "DEBUG : result put\n",
      "INFO : topic #4 (0.200): 0.030*\"shirt\" + 0.027*\"size\" + 0.020*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.009*\"woman_jewelry\" + 0.009*\"blouse_t\" + 0.009*\"pink\" + 0.009*\"kid_boy\"\n",
      "INFO : topic diff=0.062466, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 32DEBUG : processing chunk #77 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 30\n",
      "DEBUG : 7625/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 31\n",
      "DEBUG : 7675/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7664/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7661/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 31\n",
      "DEBUG : 7683/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7676/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.011*\"phone_accessory\" + 0.011*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"size\" + 0.008*\"new\" + 0.008*\"rm\"\n",
      "DEBUG : 7684/8000 documents converged within 50 iterations\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.023*\"woman\" + 0.020*\"bag\" + 0.012*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"color\" + 0.008*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.030*\"shirt\" + 0.027*\"size\" + 0.021*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"blouse_t\" + 0.009*\"kid_boy\" + 0.009*\"pink\"\n",
      "INFO : topic diff=0.062050, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 31\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7694/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7688/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 30\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 31\n",
      "DEBUG : 7680/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 31\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7670/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7691/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7698/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7657/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.011*\"phone_accessory\" + 0.011*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"size\" + 0.008*\"new\" + 0.008*\"rm\"\n",
      "DEBUG : 7662/8000 documents converged within 50 iterations\n",
      "DEBUG : 7714/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.012*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.023*\"woman\" + 0.020*\"bag\" + 0.013*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"color\" + 0.008*\"purse\"\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "DEBUG : 7681/8000 documents converged within 50 iterations\n",
      "INFO : topic #4 (0.200): 0.030*\"shirt\" + 0.028*\"size\" + 0.021*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"blouse_t\" + 0.009*\"kid_boy\" + 0.009*\"pink\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.062008, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 30\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 31\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 31\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7673/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7674/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 31\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7691/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7670/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7696/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"new\" + 0.008*\"rm\" + 0.008*\"size\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.013*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.023*\"woman\" + 0.020*\"bag\" + 0.013*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"brand_new\" + 0.008*\"purse\" + 0.008*\"color\"\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.031*\"shirt\" + 0.028*\"size\" + 0.021*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"blouse_t\" + 0.009*\"kid_boy\" + 0.008*\"pink\"\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.061822, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 31\n",
      "DEBUG : 7699/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7677/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7695/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 7671/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 31\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 31\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7701/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 32\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7687/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 32DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7664/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 32DEBUG : 7703/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"rm\" + 0.008*\"new\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.013*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.024*\"woman\" + 0.020*\"bag\" + 0.013*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"brand_new\" + 0.008*\"purse\" + 0.007*\"color\"\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"color\" + 0.008*\"lip\" + 0.008*\"box\"DEBUG : processing chunk #120 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7703/8000 documents converged within 50 iterations\n",
      "INFO : topic #4 (0.200): 0.031*\"shirt\" + 0.028*\"size\" + 0.021*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"blouse_t\" + 0.009*\"kid_boy\" + 0.008*\"pink\"\n",
      "INFO : topic diff=0.062527, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7702/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 30\n",
      "DEBUG : 7724/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7686/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7673/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7701/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7690/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7680/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"rm\" + 0.008*\"new\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.013*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.200): 0.024*\"woman\" + 0.020*\"bag\" + 0.013*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"brand_new\" + 0.008*\"purse\" + 0.007*\"color\"\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.031*\"shirt\" + 0.028*\"size\" + 0.022*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"blouse_t\" + 0.009*\"kid_boy\" + 0.008*\"2\"\n",
      "INFO : topic diff=0.061695, rho=0.081311\n",
      "DEBUG : 7672/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7717/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7729/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7727/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7711/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7693/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7700/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7696/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7699/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7708/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7723/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7708/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.200): 0.019*\"case\" + 0.014*\"home\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"rm\" + 0.008*\"new\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.013*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.024*\"woman\" + 0.020*\"bag\" + 0.013*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"new\" + 0.009*\"black\" + 0.008*\"brand_new\" + 0.008*\"purse\" + 0.007*\"accessory\"\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"color\" + 0.008*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.031*\"shirt\" + 0.028*\"size\" + 0.022*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.010*\"blouse_t\" + 0.009*\"kid_boy\" + 0.008*\"2\"\n",
      "INFO : topic diff=0.061800, rho=0.081311\n",
      "DEBUG : 7718/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7674/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7695/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7723/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7700/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7689/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1970/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7672/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7701/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.200): 0.020*\"case\" + 0.014*\"home\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.011*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.008*\"rm\" + 0.008*\"new\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.013*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.025*\"woman\" + 0.020*\"bag\" + 0.013*\"bra\" + 0.012*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"new\" + 0.008*\"black\" + 0.008*\"brand_new\" + 0.008*\"purse\" + 0.007*\"accessory\"\n",
      "INFO : topic #3 (0.200): 0.018*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"color\" + 0.009*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.031*\"shirt\" + 0.028*\"size\" + 0.022*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.010*\"blouse_t\" + 0.009*\"kid_boy\" + 0.008*\"2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.061712, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7669/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7747/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 40000 documents into a model of 1186028 documents\n",
      "INFO : topic #0 (0.200): 0.020*\"case\" + 0.014*\"home\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.010*\"1\" + 0.010*\"2\" + 0.010*\"home_dcor\" + 0.009*\"rm\" + 0.008*\"new\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.200): 0.044*\"woman\" + 0.038*\"size\" + 0.015*\"woman_athletic\" + 0.014*\"black\" + 0.013*\"pink\" + 0.012*\"dress\" + 0.011*\"small\" + 0.011*\"legging\" + 0.010*\"lularoe\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.200): 0.025*\"woman\" + 0.021*\"bag\" + 0.013*\"bra\" + 0.013*\"kid_toy\" + 0.011*\"woman_handbag\" + 0.009*\"new\" + 0.008*\"black\" + 0.008*\"brand_new\" + 0.008*\"purse\" + 0.007*\"accessory\"\n",
      "INFO : topic #3 (0.200): 0.019*\"beauty_makeup\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\" + 0.010*\"item\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"color\" + 0.009*\"lip\" + 0.008*\"box\"\n",
      "INFO : topic #4 (0.200): 0.031*\"shirt\" + 0.028*\"size\" + 0.023*\"man\" + 0.018*\"shoe\" + 0.012*\"woman_top\" + 0.011*\"new\" + 0.010*\"woman_jewelry\" + 0.010*\"blouse_t\" + 0.009*\"kid_boy\" + 0.008*\"2\"\n",
      "INFO : topic diff=0.050532, rho=0.081311\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.510 per-word bound, 182.3 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=329852, num_topics=5, decay=0.5, chunksize=8000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 585000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 586000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 587000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 588000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 589000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 590000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 591000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 592000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 593000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 594000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 595000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 596000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 597000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 598000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 599000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 600000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 601000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 602000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 603000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 604000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 605000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 606000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 607000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 608000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 609000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 610000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 611000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 612000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 613000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 614000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 615000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 616000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 617000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 618000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 619000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 620000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 621000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 622000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 623000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 624000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 625000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 626000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 627000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 628000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 629000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 630000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 631000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 632000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 633000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 634000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 635000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 636000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 637000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 638000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 639000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 640000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 641000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 642000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 643000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 644000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 645000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 646000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 647000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 648000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 649000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 650000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 651000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 652000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 653000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 654000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 655000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 656000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 657000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 658000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 659000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 660000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 661000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 662000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 663000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 664000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 665000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 666000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 667000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 668000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 669000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 670000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 671000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 672000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 673000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 674000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 675000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 676000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 677000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 678000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 679000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 680000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 681000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 682000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 683000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 684000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 685000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 686000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 687000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 688000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 689000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 690000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 691000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 692000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 693000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 694000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 695000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 696000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 697000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 698000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 699000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 700000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 701000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 702000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 703000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 704000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 705000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 706000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 707000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 708000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 709000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 710000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 711000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 712000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 713000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 714000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 715000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 716000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 717000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 718000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 719000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 720000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 721000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 722000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 723000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 724000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 725000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 726000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 727000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 728000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 729000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 730000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 731000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 732000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 733000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 734000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 735000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 736000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 737000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 738000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 739000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 740000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 741000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 742000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 743000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 744000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 745000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 746000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 747000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 748000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 749000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 750000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 751000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 752000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 753000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 754000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 755000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 756000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 757000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 758000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 759000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 760000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 761000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 762000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 763000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 764000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 765000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 766000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 767000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 768000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 769000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 770000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 771000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 772000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 773000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 774000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 775000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 776000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 777000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 778000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 779000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 780000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 781000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 782000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 783000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 784000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 785000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 786000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 787000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 788000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 789000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 790000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 791000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 792000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 793000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 794000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 795000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 796000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 797000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 798000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 799000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 800000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 801000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 802000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 803000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 804000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 805000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 806000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 807000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 808000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 809000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 810000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 811000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 812000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 813000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 814000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 815000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 816000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 817000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 818000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 819000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 820000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 821000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 822000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 823000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 824000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 825000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 826000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 827000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 828000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 829000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 830000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 831000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 832000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 833000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 834000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 835000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 836000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 837000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 838000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 839000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 840000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 841000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 842000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 843000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 844000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 845000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 846000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 847000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 848000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 849000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 850000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 851000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 852000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 853000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 854000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 855000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 856000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 857000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 858000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 859000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 860000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 861000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 862000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 863000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 864000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 865000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 866000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 867000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 868000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 869000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 870000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 871000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 872000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 873000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 874000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 875000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 876000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 877000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 878000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 879000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 880000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 881000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 882000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 883000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 884000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 885000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 886000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 887000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 888000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 889000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 890000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 891000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 892000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 893000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 894000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 895000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 896000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 897000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 898000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 899000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 900000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 901000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 902000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 903000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 904000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 905000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 906000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 907000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 908000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 909000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 910000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 911000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 912000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 913000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 914000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 915000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 916000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 917000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 918000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 919000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 920000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 921000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 922000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 923000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 924000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 925000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 926000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 927000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 928000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 929000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 930000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 931000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 932000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 933000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 934000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 935000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 936000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 937000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 938000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 939000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 940000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 941000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 942000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 943000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 944000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 945000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 946000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 947000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 948000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 949000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 950000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 951000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 952000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 953000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 954000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 955000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 956000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 957000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 958000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 959000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 960000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 961000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 962000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 963000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 964000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 965000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 966000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 967000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 968000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 969000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 970000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 971000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 972000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 973000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 974000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 975000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 976000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 977000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 978000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 979000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 980000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 981000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 982000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 983000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 984000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 985000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 986000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 987000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 988000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 989000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 990000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 991000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 992000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 993000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 994000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 995000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 996000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 997000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 998000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 999000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1000000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1001000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1002000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1003000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1004000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1005000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1006000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1007000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1008000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1009000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1010000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1011000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1012000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1013000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1014000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1015000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1016000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1017000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1018000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1019000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1020000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1021000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1022000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1023000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1024000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1025000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1026000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1027000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1028000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1029000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1030000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1031000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1032000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1033000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1034000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1035000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1036000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1037000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1038000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1039000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1040000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1041000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1042000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1043000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1044000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1045000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1046000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1047000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1048000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1049000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1050000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1051000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1052000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1053000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1054000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1055000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1056000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1057000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1058000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1059000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1060000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1061000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1062000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1063000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1064000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1065000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1066000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1067000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1068000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1069000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1070000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1071000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1072000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1073000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1074000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1075000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1076000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1077000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1078000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1079000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1080000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1081000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1082000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1083000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1084000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1085000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1086000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1087000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1088000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1089000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1090000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1091000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1092000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1093000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1094000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1095000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1096000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1097000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1098000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1099000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1111000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1186000 documents\n",
      "DEBUG : performing inference on a chunk of 1186028 documents\n",
      "DEBUG : 1144837/1186028 documents converged within 50 iterations\n",
      "\n",
      "\n",
      " 20%|        | 1/5 [08:42<34:51, 522.76s/it]\u001b[A\u001b[AINFO : using symmetric alpha at 0.1\n",
      "INFO : using symmetric eta at 0.1\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 10 topics, 3 passes over the supplied corpus of 1186028 documents, updating every 88000 documents, evaluating every ~880000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 11 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7DEBUG : processing chunk #0 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 33\n",
      "DEBUG : 5865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5866/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 5753/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5773/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5928/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 5912/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 33\n",
      "DEBUG : 5928/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 33\n",
      "DEBUG : 5859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #8 (0.100): 0.015*\"pink\" + 0.014*\"new\" + 0.013*\"woman\" + 0.013*\"size\" + 0.011*\"brand_new\" + 0.010*\"legging\" + 0.010*\"black\" + 0.009*\"rm\" + 0.009*\"game\" + 0.008*\"woman_athletic\"\n",
      "INFO : topic #0 (0.100): 0.024*\"size\" + 0.013*\"1\" + 0.010*\"man\" + 0.010*\"color\" + 0.008*\"home\" + 0.007*\"black\" + 0.007*\"4\" + 0.007*\"3\" + 0.007*\"brand_new\" + 0.006*\"2\"\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.011*\"brand_new\" + 0.010*\"woman\" + 0.009*\"beauty_makeup\" + 0.008*\"lip\" + 0.008*\"1\" + 0.007*\"color\" + 0.007*\"box\" + 0.007*\"man\" + 0.006*\"item\"\n",
      "INFO : topic #6 (0.100): 0.023*\"woman\" + 0.017*\"size\" + 0.009*\"brand_new\" + 0.008*\"pink\" + 0.007*\"game\" + 0.007*\"item\" + 0.007*\"2\" + 0.006*\"rm\" + 0.006*\"shirt\" + 0.006*\"bundle\"\n",
      "INFO : topic #2 (0.100): 0.018*\"woman\" + 0.015*\"black\" + 0.010*\"color\" + 0.010*\"size\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"beauty_makeup\" + 0.007*\"bundle\" + 0.007*\"3\" + 0.006*\"rm\"\n",
      "INFO : topic diff=10.999966, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 33\n",
      "DEBUG : 5842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 33\n",
      "DEBUG : 5865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 33\n",
      "DEBUG : 5889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 33\n",
      "DEBUG : 5841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5940/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5958/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 5921/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 33\n",
      "DEBUG : 5842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6298/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6274/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 5818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.011*\"brand_new\" + 0.010*\"woman\" + 0.009*\"beauty_makeup\" + 0.008*\"lip\" + 0.008*\"1\" + 0.007*\"color\" + 0.007*\"box\" + 0.007*\"man\" + 0.007*\"item\"\n",
      "INFO : topic #0 (0.100): 0.025*\"size\" + 0.013*\"1\" + 0.010*\"man\" + 0.009*\"color\" + 0.008*\"home\" + 0.007*\"4\" + 0.007*\"black\" + 0.007*\"3\" + 0.007*\"brand_new\" + 0.006*\"2\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #1 (0.100): 0.035*\"woman\" + 0.028*\"size\" + 0.012*\"brand_new\" + 0.011*\"dress\" + 0.011*\"rm\" + 0.011*\"new\" + 0.009*\"2\" + 0.009*\"small\" + 0.009*\"black\" + 0.008*\"pant\"\n",
      "INFO : topic #7 (0.100): 0.032*\"size\" + 0.031*\"woman\" + 0.016*\"new\" + 0.013*\"black\" + 0.013*\"shirt\" + 0.009*\"pink\" + 0.009*\"shoe\" + 0.007*\"2\" + 0.007*\"medium\" + 0.007*\"white\"\n",
      "INFO : topic #5 (0.100): 0.027*\"size\" + 0.013*\"woman\" + 0.012*\"shirt\" + 0.011*\"shoe\" + 0.010*\"small\" + 0.009*\"man\" + 0.007*\"bra\" + 0.007*\"color\" + 0.006*\"dress\" + 0.006*\"black\"\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "INFO : topic diff=0.348442, rho=0.288675\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 33\n",
      "DEBUG : 6238/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 34\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6366/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 33\n",
      "DEBUG : 6270/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6374/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6260/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6258/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6271/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6320/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.100): 0.026*\"size\" + 0.012*\"shirt\" + 0.012*\"woman\" + 0.011*\"shoe\" + 0.011*\"man\" + 0.010*\"bra\" + 0.010*\"small\" + 0.007*\"color\" + 0.006*\"bundle\" + 0.006*\"dress\"\n",
      "INFO : topic #2 (0.100): 0.018*\"woman\" + 0.014*\"black\" + 0.011*\"color\" + 0.009*\"brand_new\" + 0.009*\"size\" + 0.009*\"beauty_makeup\" + 0.008*\"new\" + 0.008*\"bag\" + 0.007*\"bundle\" + 0.006*\"3\"\n",
      "INFO : topic #1 (0.100): 0.038*\"woman\" + 0.029*\"size\" + 0.013*\"dress\" + 0.011*\"brand_new\" + 0.011*\"rm\" + 0.010*\"new\" + 0.009*\"small\" + 0.009*\"black\" + 0.009*\"2\" + 0.008*\"pant\"\n",
      "INFO : topic #6 (0.100): 0.023*\"woman\" + 0.017*\"size\" + 0.008*\"brand_new\" + 0.008*\"game\" + 0.008*\"pink\" + 0.007*\"2\" + 0.007*\"item\" + 0.007*\"short\" + 0.006*\"bundle\" + 0.006*\"rm\"\n",
      "INFO : topic #9 (0.100): 0.018*\"woman_top\" + 0.018*\"shirt\" + 0.013*\"size\" + 0.012*\"blouse\" + 0.009*\"brand_new\" + 0.009*\"blouse_t\" + 0.008*\"pink\" + 0.008*\"kid_toy\" + 0.008*\"woman\" + 0.008*\"color\"\n",
      "INFO : topic diff=0.265699, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 33\n",
      "DEBUG : 6325/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6230/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6330/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 32\n",
      "DEBUG : 6303/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : 6317/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6201/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6226/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 33\n",
      "DEBUG : 6272/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6279/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6224/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6306/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.100): 0.036*\"size\" + 0.034*\"woman\" + 0.015*\"new\" + 0.014*\"shirt\" + 0.014*\"black\" + 0.011*\"shoe\" + 0.011*\"pink\" + 0.009*\"medium\" + 0.008*\"large\" + 0.008*\"small\"\n",
      "INFO : topic #4 (0.100): 0.016*\"size\" + 0.013*\"rm\" + 0.012*\"brand_new\" + 0.012*\"2\" + 0.010*\"beauty_makeup\" + 0.010*\"woman\" + 0.009*\"new\" + 0.008*\"1\" + 0.008*\"man\" + 0.008*\"3\"\n",
      "INFO : topic #9 (0.100): 0.021*\"woman_top\" + 0.019*\"shirt\" + 0.014*\"blouse\" + 0.012*\"size\" + 0.010*\"blouse_t\" + 0.009*\"kid_toy\" + 0.009*\"brand_new\" + 0.009*\"pink\" + 0.008*\"color\" + 0.007*\"woman\"\n",
      "INFO : topic #0 (0.100): 0.021*\"size\" + 0.015*\"1\" + 0.011*\"home\" + 0.011*\"man\" + 0.008*\"color\" + 0.008*\"4\" + 0.008*\"home_dcor\" + 0.008*\"3\" + 0.007*\"2\" + 0.007*\"brand_new\"\n",
      "INFO : topic #6 (0.100): 0.023*\"woman\" + 0.017*\"size\" + 0.009*\"game\" + 0.008*\"brand_new\" + 0.007*\"pink\" + 0.007*\"short\" + 0.007*\"2\" + 0.007*\"item\" + 0.006*\"bundle\" + 0.006*\"3\"\n",
      "INFO : topic diff=0.212783, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 33\n",
      "DEBUG : 6263/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6628/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6608/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 32\n",
      "DEBUG : 6649/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 32\n",
      "DEBUG : 6623/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 32\n",
      "DEBUG : 6628/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #60 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6637/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6630/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 33\n",
      "DEBUG : 6616/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6584/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6649/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.013*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.010*\"lip\" + 0.008*\"box\" + 0.008*\"1\" + 0.008*\"color\" + 0.007*\"item\" + 0.007*\"woman\" + 0.007*\"free_shipping\"\n",
      "INFO : topic #7 (0.100): 0.038*\"size\" + 0.035*\"woman\" + 0.015*\"shirt\" + 0.015*\"new\" + 0.014*\"black\" + 0.011*\"shoe\" + 0.011*\"pink\" + 0.009*\"medium\" + 0.008*\"small\" + 0.008*\"large\"\n",
      "INFO : topic #0 (0.100): 0.020*\"size\" + 0.015*\"1\" + 0.012*\"home\" + 0.010*\"man\" + 0.009*\"home_dcor\" + 0.008*\"4\" + 0.008*\"3\" + 0.008*\"color\" + 0.008*\"2\" + 0.007*\"brand_new\"\n",
      "INFO : topic #2 (0.100): 0.018*\"woman\" + 0.013*\"black\" + 0.011*\"color\" + 0.010*\"bag\" + 0.009*\"beauty_makeup\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"eye\" + 0.007*\"size\" + 0.007*\"bundle\"\n",
      "INFO : topic #1 (0.100): 0.041*\"woman\" + 0.030*\"size\" + 0.016*\"dress\" + 0.011*\"rm\" + 0.011*\"brand_new\" + 0.010*\"new\" + 0.010*\"pant\" + 0.009*\"jean\" + 0.009*\"black\" + 0.009*\"small\"\n",
      "INFO : topic diff=0.184210, rho=0.149071\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 33\n",
      "DEBUG : 6630/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 33\n",
      "DEBUG : 6795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6762/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6765/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 32\n",
      "DEBUG : 6821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 32\n",
      "DEBUG : 6803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6797/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 31\n",
      "DEBUG : 6833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6960/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.100): 0.039*\"size\" + 0.035*\"woman\" + 0.015*\"shirt\" + 0.015*\"black\" + 0.014*\"new\" + 0.012*\"shoe\" + 0.011*\"pink\" + 0.009*\"medium\" + 0.009*\"small\" + 0.008*\"large\"\n",
      "INFO : topic #0 (0.100): 0.018*\"size\" + 0.016*\"1\" + 0.013*\"home\" + 0.010*\"home_dcor\" + 0.010*\"man\" + 0.009*\"4\" + 0.008*\"3\" + 0.008*\"2\" + 0.008*\"color\" + 0.007*\"brand_new\"\n",
      "INFO : topic #5 (0.100): 0.026*\"size\" + 0.018*\"bra\" + 0.014*\"man\" + 0.013*\"shirt\" + 0.012*\"shoe\" + 0.010*\"woman\" + 0.009*\"small\" + 0.008*\"woman_underwear\" + 0.008*\"vintage_collectible\" + 0.006*\"bundle\"\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.014*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.011*\"lip\" + 0.008*\"box\" + 0.008*\"1\" + 0.008*\"color\" + 0.008*\"item\" + 0.007*\"free_shipping\" + 0.007*\"necklace\"\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "INFO : topic #9 (0.100): 0.026*\"woman_top\" + 0.022*\"shirt\" + 0.017*\"blouse\" + 0.012*\"blouse_t\" + 0.012*\"size\" + 0.012*\"kid_toy\" + 0.009*\"brand_new\" + 0.009*\"pink\" + 0.008*\"color\" + 0.007*\"description\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6816/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.170934, rho=0.133631\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 31\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 33\n",
      "DEBUG : 6960/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 33\n",
      "DEBUG : 6950/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : 6925/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 33\n",
      "DEBUG : 6988/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6969/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 33\n",
      "DEBUG : 6967/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6890/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 33\n",
      "DEBUG : 6962/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6967/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7109/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.100): 0.015*\"size\" + 0.014*\"rm\" + 0.012*\"brand_new\" + 0.012*\"2\" + 0.010*\"beauty_makeup\" + 0.010*\"new\" + 0.009*\"woman_jewelry\" + 0.008*\"1\" + 0.008*\"3\" + 0.007*\"man\"\n",
      "INFO : topic #6 (0.100): 0.023*\"woman\" + 0.017*\"size\" + 0.011*\"game\" + 0.010*\"short\" + 0.008*\"brand_new\" + 0.008*\"2\" + 0.007*\"3\" + 0.007*\"item\" + 0.007*\"electronic_video\" + 0.007*\"game_console\"\n",
      "INFO : topic #7 (0.100): 0.040*\"size\" + 0.036*\"woman\" + 0.015*\"shirt\" + 0.015*\"black\" + 0.014*\"new\" + 0.013*\"shoe\" + 0.012*\"pink\" + 0.009*\"medium\" + 0.009*\"small\" + 0.008*\"large\"\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.015*\"beauty_makeup\" + 0.012*\"brand_new\" + 0.011*\"lip\" + 0.008*\"box\" + 0.008*\"color\" + 0.008*\"1\" + 0.008*\"item\" + 0.007*\"necklace\" + 0.007*\"free_shipping\"\n",
      "INFO : topic #5 (0.100): 0.026*\"size\" + 0.020*\"bra\" + 0.014*\"man\" + 0.012*\"shirt\" + 0.012*\"shoe\" + 0.009*\"woman_underwear\" + 0.009*\"woman\" + 0.009*\"small\" + 0.008*\"vintage_collectible\" + 0.006*\"bundle\"\n",
      "INFO : topic diff=0.164538, rho=0.122169\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 33\n",
      "DEBUG : 7057/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 33\n",
      "DEBUG : 7033/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 33\n",
      "DEBUG : 7078/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7039/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7068/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 33DEBUG : processing chunk #93 of 8000 documents\n",
      "\n",
      "DEBUG : 7061/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7072/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7025/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7068/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 33\n",
      "DEBUG : 7112/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7192/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7052/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.100): 0.026*\"size\" + 0.021*\"bra\" + 0.015*\"man\" + 0.012*\"shirt\" + 0.011*\"shoe\" + 0.010*\"woman_underwear\" + 0.009*\"vintage_collectible\" + 0.009*\"small\" + 0.008*\"woman\" + 0.006*\"bundle\"\n",
      "INFO : topic #0 (0.100): 0.017*\"1\" + 0.016*\"size\" + 0.015*\"home\" + 0.012*\"home_dcor\" + 0.009*\"man\" + 0.009*\"2\" + 0.009*\"3\" + 0.009*\"4\" + 0.007*\"color\" + 0.007*\"brand_new\"\n",
      "INFO : topic #4 (0.100): 0.015*\"rm\" + 0.014*\"size\" + 0.012*\"brand_new\" + 0.012*\"2\" + 0.010*\"beauty_makeup\" + 0.010*\"new\" + 0.009*\"woman_jewelry\" + 0.008*\"1\" + 0.008*\"3\" + 0.008*\"ring\"\n",
      "INFO : topic #7 (0.100): 0.040*\"size\" + 0.036*\"woman\" + 0.015*\"shirt\" + 0.015*\"black\" + 0.014*\"new\" + 0.013*\"shoe\" + 0.012*\"pink\" + 0.009*\"medium\" + 0.009*\"small\" + 0.009*\"man\"\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "INFO : topic #9 (0.100): 0.030*\"woman_top\" + 0.024*\"shirt\" + 0.019*\"blouse\" + 0.014*\"blouse_t\" + 0.014*\"kid_toy\" + 0.012*\"size\" + 0.009*\"brand_new\" + 0.009*\"pink\" + 0.007*\"color\" + 0.007*\"description\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.155702, rho=0.113228\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 33\n",
      "DEBUG : 7123/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : 7144/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7165/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7191/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 33\n",
      "DEBUG : 7135/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 33\n",
      "DEBUG : 7171/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7162/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7211/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7190/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #1 (0.100): 0.046*\"woman\" + 0.031*\"size\" + 0.020*\"dress\" + 0.012*\"jean\" + 0.011*\"rm\" + 0.011*\"pant\" + 0.010*\"brand_new\" + 0.010*\"black\" + 0.009*\"jacket\" + 0.009*\"small\"\n",
      "INFO : topic #4 (0.100): 0.015*\"rm\" + 0.014*\"size\" + 0.012*\"2\" + 0.012*\"brand_new\" + 0.010*\"woman_jewelry\" + 0.010*\"beauty_makeup\" + 0.010*\"new\" + 0.009*\"ring\" + 0.008*\"1\" + 0.008*\"3\"\n",
      "INFO : topic #6 (0.100): 0.022*\"woman\" + 0.017*\"size\" + 0.012*\"game\" + 0.011*\"short\" + 0.008*\"3\" + 0.008*\"electronic_video\" + 0.008*\"2\" + 0.008*\"game_console\" + 0.008*\"brand_new\" + 0.007*\"item\"\n",
      "DEBUG : 7166/8000 documents converged within 50 iterations\n",
      "INFO : topic #0 (0.100): 0.017*\"1\" + 0.016*\"home\" + 0.015*\"size\" + 0.013*\"home_dcor\" + 0.009*\"2\" + 0.009*\"3\" + 0.009*\"4\" + 0.009*\"man\" + 0.007*\"brand_new\" + 0.007*\"color\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.016*\"beauty_makeup\" + 0.013*\"brand_new\" + 0.012*\"lip\" + 0.008*\"box\" + 0.008*\"color\" + 0.008*\"item\" + 0.008*\"1\" + 0.007*\"necklace\" + 0.007*\"free_shipping\"\n",
      "INFO : topic diff=0.149099, rho=0.106000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 33\n",
      "DEBUG : 7251/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 33\n",
      "DEBUG : 7256/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 33\n",
      "DEBUG : 7233/8000 documents converged within 50 iterations\n",
      "DEBUG : 7188/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 33\n",
      "DEBUG : 7255/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7278/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 33\n",
      "DEBUG : 7263/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 32\n",
      "DEBUG : 7277/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7251/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7228/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7242/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7288/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.100): 0.022*\"woman\" + 0.017*\"size\" + 0.013*\"game\" + 0.012*\"short\" + 0.009*\"3\" + 0.009*\"electronic_video\" + 0.008*\"2\" + 0.008*\"game_console\" + 0.008*\"0_24\" + 0.008*\"brand_new\"\n",
      "INFO : topic #2 (0.100): 0.019*\"woman\" + 0.018*\"bag\" + 0.012*\"black\" + 0.011*\"color\" + 0.010*\"woman_handbag\" + 0.009*\"eye\" + 0.009*\"beauty_makeup\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"hair\"\n",
      "INFO : topic #8 (0.100): 0.019*\"legging\" + 0.018*\"pink\" + 0.016*\"case\" + 0.015*\"woman_athletic\" + 0.014*\"new\" + 0.014*\"tight_legging\" + 0.014*\"apparel_pant\" + 0.013*\"lularoe\" + 0.012*\"brand_new\" + 0.011*\"game\"\n",
      "INFO : topic #7 (0.100): 0.041*\"size\" + 0.037*\"woman\" + 0.016*\"shirt\" + 0.015*\"black\" + 0.014*\"shoe\" + 0.013*\"new\" + 0.012*\"pink\" + 0.010*\"man\" + 0.010*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #4 (0.100): 0.016*\"rm\" + 0.014*\"size\" + 0.012*\"2\" + 0.012*\"brand_new\" + 0.011*\"woman_jewelry\" + 0.010*\"new\" + 0.010*\"beauty_makeup\" + 0.009*\"ring\" + 0.008*\"3\" + 0.008*\"1\"\n",
      "DEBUG : 7341/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.145526, rho=0.100000\n",
      "DEBUG : 7294/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7322/8000 documents converged within 50 iterations\n",
      "DEBUG : bound: at document #0\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7267/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7332/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7291/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : 7315/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7290/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7296/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7248/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7377/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7403/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7337/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7349/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : 7337/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7388/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7347/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7319/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7363/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7364/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7330/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7363/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7311/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7385/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7348/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7361/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7347/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7359/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7366/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7369/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : -7.734 per-word bound, 212.8 perplexity estimate based on a held-out corpus of 8000 documents with 143156 words\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 176000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.100): 0.020*\"woman\" + 0.019*\"bag\" + 0.012*\"black\" + 0.011*\"color\" + 0.011*\"woman_handbag\" + 0.009*\"eye\" + 0.009*\"beauty_makeup\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"hair\"\n",
      "INFO : topic #5 (0.100): 0.025*\"bra\" + 0.025*\"size\" + 0.015*\"man\" + 0.012*\"woman_underwear\" + 0.011*\"shirt\" + 0.011*\"vintage_collectible\" + 0.010*\"shoe\" + 0.008*\"small\" + 0.007*\"book\" + 0.007*\"woman\"\n",
      "INFO : topic #8 (0.100): 0.019*\"legging\" + 0.018*\"pink\" + 0.017*\"case\" + 0.016*\"woman_athletic\" + 0.015*\"tight_legging\" + 0.015*\"apparel_pant\" + 0.014*\"new\" + 0.014*\"lularoe\" + 0.012*\"brand_new\" + 0.011*\"game\"\n",
      "INFO : topic #1 (0.100): 0.048*\"woman\" + 0.032*\"size\" + 0.022*\"dress\" + 0.013*\"jean\" + 0.011*\"pant\" + 0.011*\"rm\" + 0.010*\"black\" + 0.010*\"brand_new\" + 0.010*\"jacket\" + 0.009*\"small\"\n",
      "INFO : topic #3 (0.100): 0.018*\"new\" + 0.018*\"beauty_makeup\" + 0.013*\"brand_new\" + 0.012*\"lip\" + 0.009*\"color\" + 0.009*\"box\" + 0.008*\"item\" + 0.008*\"1\" + 0.007*\"face\" + 0.007*\"necklace\"\n",
      "INFO : topic diff=0.202581, rho=0.094916\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 11\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 5\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 7\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7364/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7343/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "INFO : topic #8 (0.100): 0.020*\"legging\" + 0.018*\"pink\" + 0.018*\"case\" + 0.016*\"woman_athletic\" + 0.015*\"tight_legging\" + 0.015*\"apparel_pant\" + 0.014*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.011*\"game\"\n",
      "INFO : topic #4 (0.100): 0.016*\"rm\" + 0.013*\"size\" + 0.012*\"2\" + 0.012*\"woman_jewelry\" + 0.012*\"brand_new\" + 0.010*\"ring\" + 0.010*\"new\" + 0.009*\"beauty_makeup\" + 0.008*\"earring\" + 0.008*\"3\"\n",
      "INFO : topic #1 (0.100): 0.049*\"woman\" + 0.032*\"size\" + 0.022*\"dress\" + 0.014*\"jean\" + 0.012*\"pant\" + 0.011*\"rm\" + 0.010*\"black\" + 0.010*\"brand_new\" + 0.010*\"jacket\" + 0.009*\"small\"\n",
      "INFO : topic #6 (0.100): 0.021*\"woman\" + 0.017*\"size\" + 0.014*\"game\" + 0.012*\"short\" + 0.009*\"electronic_video\" + 0.009*\"3\" + 0.009*\"game_console\" + 0.009*\"0_24\" + 0.009*\"2\" + 0.008*\"man\"\n",
      "INFO : topic #5 (0.100): 0.026*\"bra\" + 0.025*\"size\" + 0.015*\"man\" + 0.013*\"woman_underwear\" + 0.011*\"vintage_collectible\" + 0.011*\"shirt\" + 0.010*\"shoe\" + 0.008*\"small\" + 0.008*\"book\" + 0.007*\"woman\"\n",
      "INFO : topic diff=0.136327, rho=0.086711\n",
      "DEBUG : 7412/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7439/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1868/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7403/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7398/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 42028 documents into a model of 1186028 documents\n",
      "INFO : topic #7 (0.100): 0.042*\"size\" + 0.037*\"woman\" + 0.016*\"shirt\" + 0.015*\"shoe\" + 0.015*\"black\" + 0.013*\"new\" + 0.013*\"pink\" + 0.012*\"man\" + 0.010*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #0 (0.100): 0.018*\"home\" + 0.017*\"1\" + 0.014*\"home_dcor\" + 0.012*\"size\" + 0.010*\"2\" + 0.009*\"3\" + 0.009*\"4\" + 0.007*\"brand_new\" + 0.007*\"man\" + 0.006*\"new\"\n",
      "INFO : topic #3 (0.100): 0.019*\"beauty_makeup\" + 0.018*\"new\" + 0.013*\"brand_new\" + 0.012*\"lip\" + 0.009*\"color\" + 0.009*\"box\" + 0.008*\"item\" + 0.008*\"1\" + 0.008*\"face\" + 0.007*\"rm\"\n",
      "INFO : topic #4 (0.100): 0.016*\"rm\" + 0.013*\"size\" + 0.013*\"woman_jewelry\" + 0.012*\"2\" + 0.012*\"brand_new\" + 0.010*\"ring\" + 0.010*\"new\" + 0.009*\"earring\" + 0.009*\"beauty_makeup\" + 0.008*\"bracelet\"\n",
      "INFO : topic #1 (0.100): 0.049*\"woman\" + 0.032*\"size\" + 0.023*\"dress\" + 0.014*\"jean\" + 0.012*\"pant\" + 0.010*\"rm\" + 0.010*\"black\" + 0.010*\"jacket\" + 0.009*\"brand_new\" + 0.009*\"small\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.093314, rho=0.083333\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.543 per-word bound, 186.6 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7507/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7495/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 30\n",
      "DEBUG : 7498/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7500/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 32\n",
      "DEBUG : 7491/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7507/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 31\n",
      "DEBUG : 7489/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7515/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7496/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7509/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 32\n",
      "DEBUG : 7516/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7527/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7515/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #3 (0.100): 0.020*\"beauty_makeup\" + 0.018*\"new\" + 0.014*\"brand_new\" + 0.013*\"lip\" + 0.009*\"color\" + 0.009*\"box\" + 0.008*\"face\" + 0.008*\"1\" + 0.008*\"item\" + 0.007*\"rm\"\n",
      "INFO : topic #5 (0.100): 0.028*\"bra\" + 0.025*\"size\" + 0.015*\"man\" + 0.014*\"woman_underwear\" + 0.012*\"vintage_collectible\" + 0.010*\"shirt\" + 0.009*\"shoe\" + 0.008*\"book\" + 0.008*\"small\" + 0.006*\"brand_new\"\n",
      "INFO : topic #4 (0.100): 0.017*\"rm\" + 0.013*\"woman_jewelry\" + 0.013*\"size\" + 0.012*\"2\" + 0.011*\"brand_new\" + 0.010*\"ring\" + 0.010*\"new\" + 0.009*\"earring\" + 0.009*\"bracelet\" + 0.008*\"gold\"\n",
      "INFO : topic #0 (0.100): 0.018*\"home\" + 0.017*\"1\" + 0.014*\"home_dcor\" + 0.012*\"size\" + 0.010*\"2\" + 0.009*\"3\" + 0.009*\"4\" + 0.007*\"brand_new\" + 0.007*\"man\" + 0.007*\"new\"\n",
      "INFO : topic #8 (0.100): 0.021*\"legging\" + 0.019*\"case\" + 0.018*\"pink\" + 0.017*\"woman_athletic\" + 0.016*\"tight_legging\" + 0.016*\"apparel_pant\" + 0.015*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\"\n",
      "DEBUG : 7479/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.042245, rho=0.081581\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 32\n",
      "DEBUG : 7512/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 33\n",
      "DEBUG : 7497/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7518/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7444/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 33\n",
      "DEBUG : 7505/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7466/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7461/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 33DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7511/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7484/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7519/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7552/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7559/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.100): 0.017*\"rm\" + 0.014*\"woman_jewelry\" + 0.012*\"size\" + 0.012*\"2\" + 0.011*\"brand_new\" + 0.011*\"ring\" + 0.010*\"new\" + 0.009*\"bracelet\" + 0.009*\"earring\" + 0.008*\"gold\"\n",
      "INFO : topic #3 (0.100): 0.020*\"beauty_makeup\" + 0.018*\"new\" + 0.014*\"brand_new\" + 0.013*\"lip\" + 0.010*\"color\" + 0.009*\"box\" + 0.008*\"face\" + 0.008*\"1\" + 0.008*\"item\" + 0.007*\"rm\"\n",
      "INFO : topic #8 (0.100): 0.021*\"legging\" + 0.019*\"case\" + 0.018*\"pink\" + 0.017*\"woman_athletic\" + 0.016*\"tight_legging\" + 0.016*\"apparel_pant\" + 0.015*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.010*\"game\"\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.100): 0.023*\"bag\" + 0.023*\"woman\" + 0.013*\"woman_handbag\" + 0.012*\"black\" + 0.011*\"color\" + 0.009*\"eye\" + 0.009*\"purse\" + 0.009*\"hair\" + 0.008*\"brand_new\" + 0.008*\"new\"\n",
      "INFO : topic #5 (0.100): 0.029*\"bra\" + 0.025*\"size\" + 0.015*\"woman_underwear\" + 0.015*\"man\" + 0.012*\"vintage_collectible\" + 0.010*\"shirt\" + 0.009*\"book\" + 0.008*\"shoe\" + 0.008*\"small\" + 0.007*\"brand_new\"\n",
      "INFO : topic diff=0.048111, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7538/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7550/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7547/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 32DEBUG : getting a new job\n",
      "\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 32\n",
      "DEBUG : 7483/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 33\n",
      "DEBUG : 7554/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7557/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 33\n",
      "DEBUG : 7495/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.100): 0.043*\"size\" + 0.037*\"woman\" + 0.017*\"shoe\" + 0.016*\"shirt\" + 0.015*\"black\" + 0.014*\"man\" + 0.013*\"pink\" + 0.013*\"new\" + 0.010*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #8 (0.100): 0.022*\"legging\" + 0.020*\"case\" + 0.018*\"pink\" + 0.017*\"woman_athletic\" + 0.017*\"tight_legging\" + 0.017*\"apparel_pant\" + 0.016*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.010*\"phone_accessory\"\n",
      "INFO : topic #0 (0.100): 0.019*\"home\" + 0.017*\"1\" + 0.015*\"home_dcor\" + 0.011*\"size\" + 0.010*\"2\" + 0.009*\"3\" + 0.009*\"4\" + 0.007*\"brand_new\" + 0.007*\"new\" + 0.006*\"x\"\n",
      "INFO : topic #5 (0.100): 0.030*\"bra\" + 0.025*\"size\" + 0.015*\"woman_underwear\" + 0.014*\"man\" + 0.013*\"vintage_collectible\" + 0.009*\"shirt\" + 0.009*\"book\" + 0.008*\"small\" + 0.008*\"shoe\" + 0.007*\"brand_new\"\n",
      "INFO : topic #9 (0.100): 0.038*\"woman_top\" + 0.030*\"shirt\" + 0.023*\"blouse\" + 0.018*\"kid_toy\" + 0.017*\"blouse_t\" + 0.012*\"size\" + 0.009*\"pink\" + 0.008*\"brand_new\" + 0.007*\"new\" + 0.007*\"description\"\n",
      "INFO : topic diff=0.052730, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 33\n",
      "DEBUG : 7569/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7546/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7580/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7558/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : 7583/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 32\n",
      "DEBUG : 7551/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7569/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 31\n",
      "DEBUG : 7534/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7548/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 32\n",
      "DEBUG : 7545/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7588/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #6 (0.100): 0.019*\"woman\" + 0.018*\"game\" + 0.016*\"size\" + 0.014*\"short\" + 0.012*\"electronic_video\" + 0.011*\"game_console\" + 0.011*\"0_24\" + 0.011*\"3\" + 0.010*\"2\" + 0.008*\"man\"\n",
      "INFO : topic #0 (0.100): 0.019*\"home\" + 0.017*\"1\" + 0.015*\"home_dcor\" + 0.011*\"2\" + 0.010*\"size\" + 0.009*\"3\" + 0.009*\"4\" + 0.007*\"brand_new\" + 0.007*\"new\" + 0.006*\"x\"\n",
      "INFO : topic #8 (0.100): 0.022*\"legging\" + 0.021*\"case\" + 0.018*\"pink\" + 0.018*\"woman_athletic\" + 0.017*\"tight_legging\" + 0.017*\"apparel_pant\" + 0.016*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.010*\"phone_accessory\"\n",
      "INFO : topic #2 (0.100): 0.025*\"bag\" + 0.024*\"woman\" + 0.014*\"woman_handbag\" + 0.012*\"black\" + 0.011*\"color\" + 0.010*\"purse\" + 0.009*\"hair\" + 0.009*\"eye\" + 0.008*\"new\" + 0.008*\"brand_new\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #3 (0.100): 0.022*\"beauty_makeup\" + 0.018*\"new\" + 0.014*\"brand_new\" + 0.013*\"lip\" + 0.010*\"color\" + 0.009*\"box\" + 0.009*\"face\" + 0.008*\"1\" + 0.008*\"item\" + 0.007*\"rm\"\n",
      "INFO : topic diff=0.053171, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 33\n",
      "DEBUG : 7533/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7606/8000 documents converged within 50 iterations\n",
      "DEBUG : 7632/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7580/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7583/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7597/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 31\n",
      "DEBUG : 7599/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 31DEBUG : processing chunk #59 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 32\n",
      "DEBUG : 7585/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 32\n",
      "DEBUG : 7588/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7590/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7625/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7592/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7610/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #9 (0.100): 0.039*\"woman_top\" + 0.030*\"shirt\" + 0.023*\"blouse\" + 0.019*\"kid_toy\" + 0.017*\"blouse_t\" + 0.012*\"size\" + 0.009*\"pink\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"description\"\n",
      "INFO : topic #7 (0.100): 0.044*\"size\" + 0.037*\"woman\" + 0.017*\"shoe\" + 0.016*\"shirt\" + 0.015*\"black\" + 0.014*\"man\" + 0.013*\"pink\" + 0.013*\"new\" + 0.010*\"small\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic #6 (0.100): 0.019*\"game\" + 0.019*\"woman\" + 0.016*\"size\" + 0.014*\"short\" + 0.012*\"electronic_video\" + 0.012*\"game_console\" + 0.011*\"0_24\" + 0.011*\"3\" + 0.010*\"2\" + 0.009*\"man\"\n",
      "INFO : topic #8 (0.100): 0.023*\"legging\" + 0.021*\"case\" + 0.018*\"pink\" + 0.018*\"woman_athletic\" + 0.018*\"tight_legging\" + 0.018*\"apparel_pant\" + 0.017*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.010*\"phone_accessory\"\n",
      "INFO : topic #0 (0.100): 0.020*\"home\" + 0.017*\"1\" + 0.015*\"home_dcor\" + 0.011*\"2\" + 0.010*\"size\" + 0.009*\"3\" + 0.009*\"4\" + 0.007*\"new\" + 0.007*\"brand_new\" + 0.006*\"x\"\n",
      "INFO : topic diff=0.052401, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7613/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7617/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 32DEBUG : 7631/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7605/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7601/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 31\n",
      "DEBUG : 7584/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 31\n",
      "DEBUG : 7627/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7639/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 31DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 31\n",
      "DEBUG : 7652/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7592/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7576/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : 7616/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7644/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.100): 0.023*\"beauty_makeup\" + 0.018*\"new\" + 0.014*\"brand_new\" + 0.013*\"lip\" + 0.010*\"color\" + 0.009*\"face\" + 0.009*\"box\" + 0.008*\"1\" + 0.008*\"item\" + 0.007*\"rm\"\n",
      "INFO : topic #1 (0.100): 0.052*\"woman\" + 0.034*\"size\" + 0.025*\"dress\" + 0.016*\"jean\" + 0.013*\"pant\" + 0.010*\"black\" + 0.010*\"dress_knee\" + 0.010*\"small\" + 0.010*\"rm\" + 0.009*\"jacket\"\n",
      "INFO : topic #9 (0.100): 0.039*\"woman_top\" + 0.031*\"shirt\" + 0.023*\"blouse\" + 0.019*\"kid_toy\" + 0.017*\"blouse_t\" + 0.012*\"size\" + 0.009*\"pink\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"small\"\n",
      "INFO : topic #7 (0.100): 0.044*\"size\" + 0.037*\"woman\" + 0.017*\"shoe\" + 0.016*\"shirt\" + 0.015*\"black\" + 0.015*\"man\" + 0.014*\"pink\" + 0.012*\"new\" + 0.010*\"small\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic #4 (0.100): 0.017*\"rm\" + 0.016*\"woman_jewelry\" + 0.012*\"2\" + 0.011*\"ring\" + 0.011*\"size\" + 0.011*\"brand_new\" + 0.010*\"bracelet\" + 0.010*\"new\" + 0.010*\"earring\" + 0.009*\"gold\"\n",
      "INFO : topic diff=0.053573, rho=0.081581\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 30\n",
      "DEBUG : 7650/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7594/8000 documents converged within 50 iterations\n",
      "DEBUG : 7657/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 32\n",
      "DEBUG : 7624/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 33\n",
      "DEBUG : 7633/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7629/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7619/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : 7648/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 31DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7645/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7589/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7674/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.100): 0.021*\"game\" + 0.018*\"woman\" + 0.016*\"size\" + 0.014*\"short\" + 0.013*\"electronic_video\" + 0.013*\"game_console\" + 0.012*\"0_24\" + 0.012*\"3\" + 0.010*\"2\" + 0.009*\"man\"\n",
      "INFO : topic #9 (0.100): 0.040*\"woman_top\" + 0.031*\"shirt\" + 0.024*\"blouse\" + 0.019*\"kid_toy\" + 0.018*\"blouse_t\" + 0.012*\"size\" + 0.009*\"pink\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"small\"\n",
      "INFO : topic #3 (0.100): 0.024*\"beauty_makeup\" + 0.018*\"new\" + 0.014*\"brand_new\" + 0.013*\"lip\" + 0.011*\"color\" + 0.010*\"face\" + 0.009*\"box\" + 0.008*\"1\" + 0.008*\"item\" + 0.007*\"rm\"\n",
      "INFO : topic #5 (0.100): 0.032*\"bra\" + 0.024*\"size\" + 0.017*\"woman_underwear\" + 0.014*\"vintage_collectible\" + 0.014*\"man\" + 0.010*\"book\" + 0.008*\"shirt\" + 0.008*\"small\" + 0.007*\"brand_new\" + 0.007*\"black\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.100): 0.027*\"bag\" + 0.026*\"woman\" + 0.015*\"woman_handbag\" + 0.012*\"black\" + 0.011*\"purse\" + 0.011*\"color\" + 0.010*\"hair\" + 0.009*\"eye\" + 0.009*\"new\" + 0.008*\"accessory\"\n",
      "INFO : topic diff=0.053489, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 31\n",
      "DEBUG : 7664/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 32\n",
      "DEBUG : 7634/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7667/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 31DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 32\n",
      "DEBUG : 7654/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7631/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 31DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 32DEBUG : processing chunk #98 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7660/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 32DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7646/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7646/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 33\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7689/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7658/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #3 (0.100): 0.024*\"beauty_makeup\" + 0.018*\"new\" + 0.014*\"brand_new\" + 0.013*\"lip\" + 0.011*\"color\" + 0.010*\"face\" + 0.009*\"box\" + 0.008*\"1\" + 0.008*\"item\" + 0.008*\"rm\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.100): 0.045*\"size\" + 0.037*\"woman\" + 0.018*\"shoe\" + 0.016*\"shirt\" + 0.016*\"man\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"small\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic #9 (0.100): 0.040*\"woman_top\" + 0.032*\"shirt\" + 0.024*\"blouse\" + 0.020*\"kid_toy\" + 0.018*\"blouse_t\" + 0.013*\"size\" + 0.009*\"pink\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"small\"\n",
      "INFO : topic #8 (0.100): 0.024*\"legging\" + 0.023*\"case\" + 0.019*\"woman_athletic\" + 0.019*\"tight_legging\" + 0.019*\"apparel_pant\" + 0.018*\"pink\" + 0.018*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.011*\"phone_accessory\"\n",
      "INFO : topic #4 (0.100): 0.018*\"rm\" + 0.016*\"woman_jewelry\" + 0.012*\"ring\" + 0.012*\"2\" + 0.011*\"size\" + 0.011*\"brand_new\" + 0.011*\"bracelet\" + 0.010*\"necklace\" + 0.010*\"new\" + 0.010*\"earring\"\n",
      "INFO : topic diff=0.053533, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7683/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7672/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 31\n",
      "DEBUG : 7646/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 32DEBUG : 7660/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 32DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7671/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 32\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #1 (0.100): 0.054*\"woman\" + 0.035*\"size\" + 0.026*\"dress\" + 0.016*\"jean\" + 0.013*\"pant\" + 0.011*\"black\" + 0.010*\"dress_knee\" + 0.010*\"small\" + 0.009*\"length\" + 0.009*\"rm\"\n",
      "INFO : topic #5 (0.100): 0.033*\"bra\" + 0.024*\"size\" + 0.017*\"woman_underwear\" + 0.015*\"vintage_collectible\" + 0.013*\"man\" + 0.010*\"book\" + 0.008*\"small\" + 0.007*\"victoria_secret\" + 0.007*\"shirt\" + 0.007*\"brand_new\"\n",
      "INFO : topic #7 (0.100): 0.045*\"size\" + 0.037*\"woman\" + 0.018*\"shoe\" + 0.016*\"shirt\" + 0.016*\"man\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"small\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic #6 (0.100): 0.023*\"game\" + 0.017*\"woman\" + 0.016*\"size\" + 0.014*\"electronic_video\" + 0.014*\"short\" + 0.014*\"game_console\" + 0.013*\"0_24\" + 0.012*\"3\" + 0.010*\"2\" + 0.009*\"kid_girl\"\n",
      "INFO : topic #2 (0.100): 0.028*\"woman\" + 0.028*\"bag\" + 0.016*\"woman_handbag\" + 0.012*\"black\" + 0.011*\"purse\" + 0.010*\"color\" + 0.010*\"hair\" + 0.009*\"accessory\" + 0.009*\"eye\" + 0.009*\"new\"\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : 7691/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.053039, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 33\n",
      "DEBUG : 7684/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7667/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7698/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7683/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : 7690/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7616/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7684/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7689/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.100): 0.029*\"woman\" + 0.028*\"bag\" + 0.016*\"woman_handbag\" + 0.012*\"black\" + 0.011*\"purse\" + 0.010*\"color\" + 0.010*\"hair\" + 0.009*\"accessory\" + 0.009*\"new\" + 0.008*\"eye\"\n",
      "INFO : topic #9 (0.100): 0.041*\"woman_top\" + 0.033*\"shirt\" + 0.024*\"blouse\" + 0.020*\"kid_toy\" + 0.018*\"blouse_t\" + 0.013*\"size\" + 0.009*\"pink\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"small\"\n",
      "DEBUG : 7688/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #8 (0.100): 0.025*\"legging\" + 0.024*\"case\" + 0.020*\"woman_athletic\" + 0.019*\"tight_legging\" + 0.019*\"apparel_pant\" + 0.018*\"pink\" + 0.018*\"lularoe\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.011*\"phone_accessory\"\n",
      "DEBUG : 7691/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7690/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.100): 0.045*\"size\" + 0.037*\"woman\" + 0.019*\"shoe\" + 0.017*\"man\" + 0.016*\"shirt\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"small\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic #3 (0.100): 0.025*\"beauty_makeup\" + 0.017*\"new\" + 0.015*\"brand_new\" + 0.014*\"lip\" + 0.011*\"color\" + 0.010*\"face\" + 0.009*\"box\" + 0.008*\"1\" + 0.008*\"rm\" + 0.008*\"item\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.057044, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7709/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7673/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7717/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7720/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7684/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7683/8000 documents converged within 50 iterations\n",
      "DEBUG : 7690/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.100): 0.018*\"rm\" + 0.017*\"woman_jewelry\" + 0.012*\"ring\" + 0.012*\"necklace\" + 0.012*\"2\" + 0.011*\"bracelet\" + 0.011*\"brand_new\" + 0.010*\"size\" + 0.010*\"new\" + 0.010*\"earring\"\n",
      "INFO : topic #5 (0.100): 0.034*\"bra\" + 0.024*\"size\" + 0.018*\"woman_underwear\" + 0.015*\"vintage_collectible\" + 0.013*\"man\" + 0.011*\"book\" + 0.008*\"victoria_secret\" + 0.008*\"small\" + 0.007*\"brand_new\" + 0.007*\"black\"\n",
      "INFO : topic #1 (0.100): 0.054*\"woman\" + 0.035*\"size\" + 0.027*\"dress\" + 0.017*\"jean\" + 0.013*\"pant\" + 0.011*\"black\" + 0.010*\"dress_knee\" + 0.010*\"small\" + 0.010*\"length\" + 0.009*\"rm\"\n",
      "DEBUG : 7684/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.100): 0.021*\"home\" + 0.017*\"1\" + 0.015*\"home_dcor\" + 0.011*\"2\" + 0.009*\"3\" + 0.008*\"new\" + 0.008*\"4\" + 0.008*\"size\" + 0.007*\"brand_new\" + 0.006*\"x\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.100): 0.025*\"legging\" + 0.024*\"case\" + 0.020*\"woman_athletic\" + 0.020*\"tight_legging\" + 0.020*\"apparel_pant\" + 0.019*\"lularoe\" + 0.018*\"pink\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.011*\"phone_accessory\"\n",
      "INFO : topic diff=0.062698, rho=0.081581\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7731/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7678/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7706/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7756/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7693/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.100): 0.018*\"rm\" + 0.018*\"woman_jewelry\" + 0.012*\"necklace\" + 0.012*\"ring\" + 0.011*\"2\" + 0.011*\"bracelet\" + 0.010*\"brand_new\" + 0.010*\"size\" + 0.010*\"new\" + 0.010*\"earring\"\n",
      "INFO : topic #6 (0.100): 0.025*\"game\" + 0.016*\"electronic_video\" + 0.016*\"woman\" + 0.015*\"game_console\" + 0.015*\"size\" + 0.014*\"short\" + 0.014*\"0_24\" + 0.013*\"3\" + 0.011*\"2\" + 0.010*\"kid_girl\"\n",
      "INFO : topic #8 (0.100): 0.026*\"legging\" + 0.025*\"case\" + 0.020*\"tight_legging\" + 0.020*\"woman_athletic\" + 0.020*\"apparel_pant\" + 0.019*\"lularoe\" + 0.018*\"pink\" + 0.014*\"new\" + 0.013*\"brand_new\" + 0.012*\"phone_accessory\"\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #5 (0.100): 0.034*\"bra\" + 0.024*\"size\" + 0.018*\"woman_underwear\" + 0.015*\"vintage_collectible\" + 0.012*\"man\" + 0.011*\"book\" + 0.008*\"victoria_secret\" + 0.008*\"small\" + 0.007*\"brand_new\" + 0.007*\"black\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.100): 0.030*\"woman\" + 0.029*\"bag\" + 0.017*\"woman_handbag\" + 0.012*\"purse\" + 0.012*\"black\" + 0.010*\"hair\" + 0.010*\"color\" + 0.009*\"accessory\" + 0.009*\"new\" + 0.008*\"brand_new\"\n",
      "INFO : topic diff=0.059532, rho=0.081581\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7730/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7711/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7721/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1938/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7730/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7727/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "INFO : topic #7 (0.100): 0.045*\"size\" + 0.037*\"woman\" + 0.019*\"shoe\" + 0.018*\"man\" + 0.017*\"shirt\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"woman_athletic\" + 0.011*\"small\"\n",
      "INFO : topic #6 (0.100): 0.026*\"game\" + 0.016*\"electronic_video\" + 0.015*\"game_console\" + 0.015*\"woman\" + 0.015*\"size\" + 0.014*\"0_24\" + 0.014*\"short\" + 0.013*\"3\" + 0.011*\"2\" + 0.010*\"kid_girl\"\n",
      "INFO : topic #0 (0.100): 0.021*\"home\" + 0.016*\"1\" + 0.015*\"home_dcor\" + 0.011*\"2\" + 0.009*\"3\" + 0.009*\"new\" + 0.008*\"4\" + 0.007*\"brand_new\" + 0.007*\"size\" + 0.006*\"bath_body\"\n",
      "INFO : topic #5 (0.100): 0.034*\"bra\" + 0.024*\"size\" + 0.018*\"woman_underwear\" + 0.016*\"vintage_collectible\" + 0.012*\"man\" + 0.011*\"book\" + 0.009*\"victoria_secret\" + 0.008*\"small\" + 0.007*\"brand_new\" + 0.007*\"black\"\n",
      "INFO : topic #3 (0.100): 0.027*\"beauty_makeup\" + 0.017*\"new\" + 0.015*\"brand_new\" + 0.014*\"lip\" + 0.011*\"color\" + 0.011*\"face\" + 0.009*\"box\" + 0.008*\"rm\" + 0.008*\"brush\" + 0.008*\"1\"\n",
      "INFO : topic diff=0.050640, rho=0.081581\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.427 per-word bound, 172.0 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7730/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7687/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7733/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7723/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 30\n",
      "DEBUG : 7728/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 31\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 31\n",
      "DEBUG : 7714/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 31\n",
      "DEBUG : 7744/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 32\n",
      "DEBUG : 7716/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 32\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7721/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7720/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7712/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #4 (0.100): 0.018*\"woman_jewelry\" + 0.018*\"rm\" + 0.013*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"new\" + 0.010*\"earring\" + 0.010*\"gold\"\n",
      "INFO : topic #0 (0.100): 0.021*\"home\" + 0.016*\"1\" + 0.015*\"home_dcor\" + 0.012*\"2\" + 0.009*\"3\" + 0.009*\"new\" + 0.008*\"4\" + 0.007*\"brand_new\" + 0.007*\"size\" + 0.006*\"bath_body\"\n",
      "INFO : topic #1 (0.100): 0.055*\"woman\" + 0.036*\"size\" + 0.027*\"dress\" + 0.017*\"jean\" + 0.013*\"pant\" + 0.011*\"black\" + 0.010*\"dress_knee\" + 0.010*\"length\" + 0.010*\"small\" + 0.008*\"rm\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7741/8000 documents converged within 50 iterations\n",
      "INFO : topic #6 (0.100): 0.027*\"game\" + 0.017*\"electronic_video\" + 0.016*\"game_console\" + 0.015*\"woman\" + 0.015*\"size\" + 0.014*\"0_24\" + 0.014*\"3\" + 0.014*\"short\" + 0.012*\"2\" + 0.011*\"kid_girl\"\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.100): 0.032*\"woman\" + 0.031*\"bag\" + 0.017*\"woman_handbag\" + 0.012*\"purse\" + 0.011*\"black\" + 0.011*\"hair\" + 0.010*\"color\" + 0.010*\"accessory\" + 0.009*\"new\" + 0.008*\"brand_new\"\n",
      "INFO : topic diff=0.042492, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7749/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7741/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7706/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7728/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7732/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7707/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7728/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7708/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7740/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 112000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #33 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7747/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.100): 0.033*\"woman\" + 0.031*\"bag\" + 0.018*\"woman_handbag\" + 0.012*\"purse\" + 0.011*\"black\" + 0.011*\"hair\" + 0.010*\"color\" + 0.010*\"accessory\" + 0.009*\"new\" + 0.008*\"brand_new\"\n",
      "INFO : topic #3 (0.100): 0.028*\"beauty_makeup\" + 0.017*\"new\" + 0.015*\"brand_new\" + 0.014*\"lip\" + 0.012*\"color\" + 0.011*\"face\" + 0.009*\"box\" + 0.008*\"brush\" + 0.008*\"rm\" + 0.008*\"1\"\n",
      "INFO : topic #7 (0.100): 0.045*\"size\" + 0.037*\"woman\" + 0.020*\"shoe\" + 0.019*\"man\" + 0.017*\"shirt\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"woman_athletic\" + 0.011*\"small\"\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.100): 0.018*\"woman_jewelry\" + 0.018*\"rm\" + 0.013*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"new\" + 0.010*\"gold\" + 0.010*\"earring\"\n",
      "INFO : topic #1 (0.100): 0.055*\"woman\" + 0.036*\"size\" + 0.027*\"dress\" + 0.017*\"jean\" + 0.013*\"pant\" + 0.011*\"black\" + 0.010*\"dress_knee\" + 0.010*\"length\" + 0.010*\"small\" + 0.008*\"rm\"\n",
      "INFO : topic diff=0.045381, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 28\n",
      "DEBUG : 7737/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7739/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7763/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 25\n",
      "DEBUG : 7740/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 30\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 31DEBUG : result put\n",
      "\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7728/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 32\n",
      "DEBUG : 7749/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7748/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7756/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7733/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7713/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.100): 0.035*\"bra\" + 0.024*\"size\" + 0.019*\"woman_underwear\" + 0.016*\"vintage_collectible\" + 0.012*\"book\" + 0.011*\"man\" + 0.010*\"victoria_secret\" + 0.007*\"small\" + 0.007*\"pantie\" + 0.007*\"pink\"\n",
      "DEBUG : 7749/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #7 (0.100): 0.046*\"size\" + 0.037*\"woman\" + 0.020*\"shoe\" + 0.019*\"man\" + 0.017*\"shirt\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"woman_athletic\" + 0.011*\"small\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7748/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.100): 0.018*\"woman_jewelry\" + 0.018*\"rm\" + 0.014*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"new\" + 0.010*\"gold\" + 0.010*\"earring\"\n",
      "INFO : topic #1 (0.100): 0.055*\"woman\" + 0.036*\"size\" + 0.028*\"dress\" + 0.018*\"jean\" + 0.013*\"pant\" + 0.011*\"black\" + 0.010*\"dress_knee\" + 0.010*\"length\" + 0.010*\"small\" + 0.008*\"mini\"\n",
      "INFO : topic #9 (0.100): 0.043*\"woman_top\" + 0.035*\"shirt\" + 0.025*\"blouse\" + 0.022*\"kid_toy\" + 0.019*\"blouse_t\" + 0.013*\"size\" + 0.010*\"pink\" + 0.008*\"tee\" + 0.008*\"small\" + 0.008*\"new\"\n",
      "DEBUG : 7758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.046487, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 30\n",
      "DEBUG : 7733/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 30\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7746/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7732/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : 7759/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #6 (0.100): 0.028*\"game\" + 0.018*\"electronic_video\" + 0.017*\"game_console\" + 0.015*\"0_24\" + 0.014*\"3\" + 0.014*\"size\" + 0.013*\"woman\" + 0.013*\"short\" + 0.012*\"2\" + 0.011*\"kid_girl\"\n",
      "INFO : topic #3 (0.100): 0.029*\"beauty_makeup\" + 0.017*\"new\" + 0.015*\"brand_new\" + 0.014*\"lip\" + 0.012*\"color\" + 0.012*\"face\" + 0.009*\"box\" + 0.008*\"rm\" + 0.008*\"brush\" + 0.008*\"1\"\n",
      "INFO : topic #7 (0.100): 0.046*\"size\" + 0.037*\"woman\" + 0.020*\"shoe\" + 0.019*\"man\" + 0.017*\"shirt\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"woman_athletic\" + 0.011*\"small\"\n",
      "DEBUG : 7763/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7753/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #8 (0.100): 0.028*\"legging\" + 0.026*\"case\" + 0.021*\"tight_legging\" + 0.021*\"apparel_pant\" + 0.021*\"woman_athletic\" + 0.021*\"lularoe\" + 0.019*\"pink\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.012*\"phone_accessory\"\n",
      "INFO : topic #4 (0.100): 0.018*\"woman_jewelry\" + 0.018*\"rm\" + 0.014*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"earring\"\n",
      "INFO : topic diff=0.045987, rho=0.081311\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 29\n",
      "DEBUG : 7752/8000 documents converged within 50 iterations\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7783/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7756/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 24\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 30DEBUG : 7747/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7752/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7737/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7768/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7752/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.100): 0.036*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.017*\"vintage_collectible\" + 0.012*\"book\" + 0.010*\"victoria_secret\" + 0.010*\"man\" + 0.008*\"pink\" + 0.008*\"pantie\" + 0.007*\"black\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #9 (0.100): 0.043*\"woman_top\" + 0.036*\"shirt\" + 0.025*\"blouse\" + 0.022*\"kid_toy\" + 0.019*\"blouse_t\" + 0.013*\"size\" + 0.010*\"pink\" + 0.009*\"tee\" + 0.009*\"small\" + 0.008*\"tank\"\n",
      "INFO : topic #1 (0.100): 0.056*\"woman\" + 0.037*\"size\" + 0.028*\"dress\" + 0.018*\"jean\" + 0.013*\"pant\" + 0.011*\"black\" + 0.010*\"dress_knee\" + 0.010*\"length\" + 0.010*\"small\" + 0.008*\"mini\"\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "INFO : topic #2 (0.100): 0.035*\"woman\" + 0.032*\"bag\" + 0.018*\"woman_handbag\" + 0.013*\"purse\" + 0.011*\"black\" + 0.011*\"hair\" + 0.010*\"accessory\" + 0.010*\"color\" + 0.009*\"new\" + 0.008*\"brand_new\"\n",
      "INFO : topic #4 (0.100): 0.018*\"woman_jewelry\" + 0.018*\"rm\" + 0.014*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"brand_new\" + 0.010*\"earring\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.045504, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7759/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7757/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 24\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.100): 0.028*\"legging\" + 0.027*\"case\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.021*\"woman_athletic\" + 0.021*\"lularoe\" + 0.019*\"pink\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.012*\"phone_accessory\"\n",
      "INFO : topic #1 (0.100): 0.056*\"woman\" + 0.037*\"size\" + 0.028*\"dress\" + 0.018*\"jean\" + 0.013*\"pant\" + 0.012*\"black\" + 0.010*\"dress_knee\" + 0.010*\"length\" + 0.010*\"small\" + 0.008*\"mini\"\n",
      "INFO : topic #4 (0.100): 0.019*\"woman_jewelry\" + 0.018*\"rm\" + 0.014*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"brand_new\" + 0.010*\"earring\"\n",
      "INFO : topic #0 (0.100): 0.021*\"home\" + 0.016*\"1\" + 0.015*\"home_dcor\" + 0.012*\"2\" + 0.009*\"new\" + 0.009*\"3\" + 0.008*\"4\" + 0.007*\"brand_new\" + 0.006*\"rm\" + 0.006*\"bath_body\"\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #9 (0.100): 0.044*\"woman_top\" + 0.036*\"shirt\" + 0.026*\"blouse\" + 0.022*\"kid_toy\" + 0.019*\"blouse_t\" + 0.013*\"size\" + 0.010*\"pink\" + 0.009*\"tee\" + 0.009*\"small\" + 0.008*\"tank\"\n",
      "DEBUG : 7772/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.046631, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 23\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7763/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 28\n",
      "DEBUG : 7758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7762/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 30\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7749/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.100): 0.056*\"woman\" + 0.037*\"size\" + 0.028*\"dress\" + 0.018*\"jean\" + 0.013*\"pant\" + 0.012*\"black\" + 0.011*\"dress_knee\" + 0.010*\"length\" + 0.010*\"small\" + 0.008*\"mini\"\n",
      "INFO : topic #3 (0.100): 0.030*\"beauty_makeup\" + 0.017*\"new\" + 0.015*\"brand_new\" + 0.015*\"lip\" + 0.013*\"color\" + 0.012*\"face\" + 0.009*\"box\" + 0.008*\"brush\" + 0.008*\"eye\" + 0.008*\"rm\"\n",
      "INFO : topic #2 (0.100): 0.036*\"woman\" + 0.033*\"bag\" + 0.019*\"woman_handbag\" + 0.013*\"purse\" + 0.011*\"black\" + 0.011*\"hair\" + 0.011*\"accessory\" + 0.010*\"color\" + 0.009*\"new\" + 0.008*\"brand_new\"\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.100): 0.021*\"home\" + 0.016*\"1\" + 0.015*\"home_dcor\" + 0.012*\"2\" + 0.009*\"new\" + 0.009*\"3\" + 0.008*\"4\" + 0.007*\"brand_new\" + 0.006*\"rm\" + 0.006*\"bath_body\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7783/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.100): 0.036*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.017*\"vintage_collectible\" + 0.012*\"book\" + 0.011*\"victoria_secret\" + 0.010*\"man\" + 0.008*\"pink\" + 0.008*\"pantie\" + 0.007*\"black\"\n",
      "INFO : topic diff=0.045871, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 31\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 30\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 30\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7786/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.100): 0.036*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.017*\"vintage_collectible\" + 0.012*\"book\" + 0.011*\"victoria_secret\" + 0.009*\"man\" + 0.008*\"pink\" + 0.008*\"pantie\" + 0.007*\"black\"\n",
      "INFO : topic #9 (0.100): 0.044*\"woman_top\" + 0.036*\"shirt\" + 0.026*\"blouse\" + 0.022*\"kid_toy\" + 0.019*\"blouse_t\" + 0.014*\"size\" + 0.010*\"pink\" + 0.009*\"tee\" + 0.009*\"small\" + 0.009*\"tank\"\n",
      "DEBUG : 7793/8000 documents converged within 50 iterations\n",
      "INFO : topic #8 (0.100): 0.029*\"legging\" + 0.028*\"case\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.022*\"lularoe\" + 0.022*\"woman_athletic\" + 0.018*\"pink\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.013*\"phone_accessory\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.100): 0.031*\"game\" + 0.019*\"electronic_video\" + 0.018*\"game_console\" + 0.016*\"0_24\" + 0.015*\"3\" + 0.013*\"size\" + 0.013*\"2\" + 0.012*\"kid_girl\" + 0.012*\"short\" + 0.012*\"woman\"\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7785/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.100): 0.030*\"beauty_makeup\" + 0.017*\"new\" + 0.015*\"brand_new\" + 0.015*\"lip\" + 0.013*\"color\" + 0.012*\"face\" + 0.009*\"box\" + 0.008*\"brush\" + 0.008*\"eye\" + 0.008*\"rm\"\n",
      "INFO : topic diff=0.045973, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 29\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 29\n",
      "DEBUG : 7772/8000 documents converged within 50 iterations\n",
      "DEBUG : 7778/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.100): 0.037*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.017*\"vintage_collectible\" + 0.012*\"book\" + 0.012*\"victoria_secret\" + 0.009*\"man\" + 0.008*\"pink\" + 0.008*\"pantie\" + 0.008*\"black\"\n",
      "INFO : topic #4 (0.100): 0.019*\"woman_jewelry\" + 0.018*\"rm\" + 0.015*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic #1 (0.100): 0.057*\"woman\" + 0.038*\"size\" + 0.028*\"dress\" + 0.018*\"jean\" + 0.013*\"pant\" + 0.012*\"black\" + 0.011*\"dress_knee\" + 0.011*\"length\" + 0.010*\"small\" + 0.008*\"mini\"\n",
      "INFO : topic #8 (0.100): 0.029*\"legging\" + 0.028*\"case\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.022*\"lularoe\" + 0.022*\"woman_athletic\" + 0.018*\"pink\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.013*\"phone_accessory\"\n",
      "INFO : topic #9 (0.100): 0.044*\"woman_top\" + 0.037*\"shirt\" + 0.026*\"blouse\" + 0.022*\"kid_toy\" + 0.019*\"blouse_t\" + 0.014*\"size\" + 0.010*\"pink\" + 0.009*\"tee\" + 0.009*\"small\" + 0.009*\"tank\"\n",
      "INFO : topic diff=0.044289, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 28\n",
      "DEBUG : 7791/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7802/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 28\n",
      "DEBUG : 7794/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7778/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 27\n",
      "DEBUG : 7783/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 28\n",
      "DEBUG : 7766/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 28\n",
      "DEBUG : 7767/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #121 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7769/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #6 (0.100): 0.032*\"game\" + 0.019*\"electronic_video\" + 0.018*\"game_console\" + 0.016*\"0_24\" + 0.016*\"3\" + 0.013*\"2\" + 0.013*\"size\" + 0.013*\"kid_girl\" + 0.012*\"short\" + 0.012*\"baby\"\n",
      "INFO : topic #9 (0.100): 0.044*\"woman_top\" + 0.037*\"shirt\" + 0.026*\"blouse\" + 0.023*\"kid_toy\" + 0.019*\"blouse_t\" + 0.014*\"size\" + 0.010*\"pink\" + 0.009*\"tee\" + 0.009*\"small\" + 0.009*\"tank\"\n",
      "INFO : topic #3 (0.100): 0.031*\"beauty_makeup\" + 0.017*\"new\" + 0.016*\"brand_new\" + 0.015*\"lip\" + 0.013*\"color\" + 0.013*\"face\" + 0.009*\"box\" + 0.009*\"eye\" + 0.008*\"brush\" + 0.008*\"rm\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.100): 0.019*\"woman_jewelry\" + 0.018*\"rm\" + 0.015*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic #5 (0.100): 0.037*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.017*\"vintage_collectible\" + 0.012*\"book\" + 0.012*\"victoria_secret\" + 0.009*\"man\" + 0.008*\"pink\" + 0.008*\"pantie\" + 0.008*\"black\"\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.044597, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7773/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 25\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7791/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7800/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7814/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.100): 0.019*\"woman_jewelry\" + 0.018*\"rm\" + 0.015*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic #3 (0.100): 0.031*\"beauty_makeup\" + 0.017*\"new\" + 0.016*\"brand_new\" + 0.015*\"lip\" + 0.013*\"color\" + 0.013*\"face\" + 0.009*\"box\" + 0.009*\"eye\" + 0.008*\"brush\" + 0.008*\"rm\"\n",
      "INFO : topic #5 (0.100): 0.037*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.018*\"vintage_collectible\" + 0.013*\"book\" + 0.012*\"victoria_secret\" + 0.009*\"man\" + 0.009*\"pink\" + 0.008*\"pantie\" + 0.008*\"black\"\n",
      "INFO : topic #2 (0.100): 0.039*\"woman\" + 0.035*\"bag\" + 0.019*\"woman_handbag\" + 0.014*\"purse\" + 0.012*\"hair\" + 0.011*\"black\" + 0.011*\"accessory\" + 0.009*\"color\" + 0.009*\"new\" + 0.008*\"brand_new\"\n",
      "INFO : topic #1 (0.100): 0.057*\"woman\" + 0.038*\"size\" + 0.028*\"dress\" + 0.018*\"jean\" + 0.013*\"pant\" + 0.012*\"black\" + 0.011*\"length\" + 0.011*\"dress_knee\" + 0.010*\"small\" + 0.008*\"mini\"\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.044532, rho=0.081311\n",
      "DEBUG : 7818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7797/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7773/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7802/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7797/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7783/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.100): 0.019*\"woman_jewelry\" + 0.017*\"rm\" + 0.015*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic #0 (0.100): 0.021*\"home\" + 0.015*\"1\" + 0.015*\"home_dcor\" + 0.012*\"2\" + 0.010*\"new\" + 0.009*\"3\" + 0.008*\"4\" + 0.008*\"brand_new\" + 0.007*\"rm\" + 0.006*\"bath_body\"\n",
      "INFO : topic #5 (0.100): 0.037*\"bra\" + 0.023*\"size\" + 0.020*\"woman_underwear\" + 0.018*\"vintage_collectible\" + 0.013*\"victoria_secret\" + 0.013*\"book\" + 0.009*\"pink\" + 0.008*\"man\" + 0.008*\"pantie\" + 0.008*\"black\"\n",
      "INFO : topic #9 (0.100): 0.045*\"woman_top\" + 0.038*\"shirt\" + 0.026*\"blouse\" + 0.023*\"kid_toy\" + 0.019*\"blouse_t\" + 0.014*\"size\" + 0.010*\"pink\" + 0.009*\"tank\" + 0.009*\"tee\" + 0.009*\"small\"\n",
      "INFO : topic #2 (0.100): 0.040*\"woman\" + 0.035*\"bag\" + 0.019*\"woman_handbag\" + 0.014*\"purse\" + 0.012*\"hair\" + 0.011*\"accessory\" + 0.011*\"black\" + 0.009*\"color\" + 0.009*\"new\" + 0.008*\"pocket\"\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.043858, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1975/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 74028 documents into a model of 1186028 documents\n",
      "INFO : topic #4 (0.100): 0.019*\"woman_jewelry\" + 0.017*\"rm\" + 0.015*\"necklace\" + 0.012*\"ring\" + 0.012*\"bracelet\" + 0.011*\"2\" + 0.010*\"gold\" + 0.010*\"new\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic #7 (0.100): 0.047*\"size\" + 0.037*\"woman\" + 0.023*\"man\" + 0.022*\"shoe\" + 0.017*\"shirt\" + 0.015*\"black\" + 0.014*\"pink\" + 0.012*\"woman_athletic\" + 0.012*\"new\" + 0.011*\"small\"\n",
      "INFO : topic #8 (0.100): 0.030*\"legging\" + 0.029*\"case\" + 0.023*\"tight_legging\" + 0.023*\"apparel_pant\" + 0.023*\"lularoe\" + 0.022*\"woman_athletic\" + 0.018*\"pink\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.013*\"phone_accessory\"\n",
      "INFO : topic #9 (0.100): 0.045*\"woman_top\" + 0.038*\"shirt\" + 0.026*\"blouse\" + 0.023*\"kid_toy\" + 0.019*\"blouse_t\" + 0.014*\"size\" + 0.010*\"pink\" + 0.009*\"tank\" + 0.009*\"tee\" + 0.009*\"small\"\n",
      "INFO : topic #6 (0.100): 0.032*\"game\" + 0.020*\"electronic_video\" + 0.019*\"game_console\" + 0.017*\"0_24\" + 0.016*\"3\" + 0.014*\"kid_girl\" + 0.014*\"2\" + 0.013*\"size\" + 0.012*\"baby\" + 0.011*\"short\"\n",
      "INFO : topic diff=0.042113, rho=0.081311\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.363 per-word bound, 164.7 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=329852, num_topics=10, decay=0.5, chunksize=8000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 585000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 586000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 587000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 588000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 589000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 590000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 591000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 592000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 593000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 594000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 595000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 596000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 597000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 598000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 599000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 600000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 601000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 602000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 603000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 604000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 605000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 606000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 607000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 608000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 609000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 610000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 611000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 612000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 613000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 614000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 615000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 616000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 617000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 618000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 619000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 620000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 621000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 622000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 623000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 624000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 625000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 626000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 627000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 628000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 629000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 630000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 631000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 632000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 633000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 634000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 635000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 636000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 637000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 638000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 639000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 640000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 641000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 642000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 643000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 644000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 645000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 646000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 647000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 648000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 649000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 650000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 651000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 652000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 653000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 654000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 655000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 656000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 657000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 658000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 659000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 660000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 661000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 662000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 663000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 664000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 665000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 666000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 667000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 668000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 669000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 670000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 671000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 672000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 673000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 674000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 675000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 676000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 677000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 678000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 679000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 680000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 681000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 682000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 683000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 684000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 685000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 686000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 687000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 688000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 689000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 690000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 691000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 692000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 693000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 694000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 695000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 696000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 697000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 698000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 699000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 700000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 701000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 702000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 703000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 704000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 705000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 706000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 707000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 708000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 709000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 710000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 711000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 712000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 713000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 714000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 715000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 716000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 717000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 718000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 719000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 720000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 721000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 722000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 723000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 724000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 725000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 726000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 727000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 728000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 729000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 730000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 731000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 732000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 733000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 734000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 735000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 736000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 737000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 738000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 739000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 740000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 741000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 742000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 743000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 744000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 745000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 746000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 747000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 748000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 749000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 750000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 751000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 752000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 753000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 754000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 755000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 756000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 757000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 758000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 759000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 760000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 761000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 762000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 763000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 764000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 765000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 766000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 767000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 768000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 769000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 770000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 771000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 772000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 773000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 774000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 775000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 776000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 777000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 778000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 779000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 780000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 781000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 782000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 783000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 784000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 785000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 786000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 787000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 788000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 789000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 790000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 791000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 792000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 793000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 794000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 795000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 796000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 797000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 798000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 799000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 800000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 801000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 802000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 803000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 804000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 805000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 806000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 807000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 808000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 809000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 810000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 811000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 812000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 813000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 814000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 815000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 816000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 817000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 818000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 819000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 820000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 821000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 822000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 823000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 824000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 825000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 826000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 827000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 828000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 829000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 830000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 831000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 832000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 833000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 834000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 835000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 836000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 837000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 838000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 839000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 840000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 841000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 842000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 843000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 844000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 845000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 846000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 847000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 848000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 849000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 850000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 851000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 852000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 853000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 854000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 855000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 856000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 857000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 858000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 859000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 860000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 861000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 862000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 863000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 864000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 865000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 866000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 867000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 868000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 869000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 870000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 871000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 872000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 873000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 874000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 875000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 876000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 877000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 878000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 879000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 880000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 881000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 882000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 883000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 884000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 885000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 886000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 887000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 888000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 889000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 890000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 891000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 892000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 893000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 894000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 895000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 896000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 897000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 898000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 899000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 900000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 901000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 902000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 903000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 904000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 905000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 906000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 907000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 908000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 909000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 910000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 911000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 912000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 913000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 914000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 915000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 916000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 917000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 918000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 919000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 920000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 921000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 922000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 923000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 924000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 925000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 926000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 927000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 928000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 929000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 930000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 931000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 932000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 933000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 934000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 935000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 936000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 937000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 938000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 939000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 940000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 941000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 942000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 943000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 944000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 945000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 946000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 947000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 948000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 949000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 950000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 951000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 952000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 953000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 954000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 955000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 956000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 957000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 958000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 959000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 960000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 961000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 962000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 963000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 964000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 965000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 966000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 967000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 968000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 969000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 970000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 971000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 972000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 973000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 974000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 975000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 976000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 977000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 978000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 979000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 980000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 981000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 982000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 983000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 984000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 985000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 986000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 987000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 988000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 989000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 990000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 991000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 992000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 993000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 994000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 995000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 996000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 997000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 998000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 999000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1000000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1001000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1002000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1003000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1004000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1005000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1006000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1007000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1008000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1009000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1010000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1011000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1012000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1013000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1014000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1015000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1016000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1017000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1018000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1019000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1020000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1021000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1022000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1023000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1024000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1025000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1026000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1027000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1028000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1029000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1030000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1031000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1032000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1033000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1034000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1035000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1036000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1037000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1038000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1039000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1040000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1041000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1042000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1043000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1044000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1045000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1046000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1047000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1048000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1049000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1050000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1051000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1052000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1053000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1054000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1055000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1056000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1057000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1058000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1059000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1060000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1061000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1062000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1063000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1064000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1065000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1066000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1067000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1068000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1069000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1070000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1071000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1072000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1073000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1074000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1075000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1076000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1077000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1078000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1079000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1080000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1081000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1082000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1083000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1084000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1085000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1086000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1087000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1088000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1089000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1090000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1091000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1092000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1093000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1094000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1095000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1096000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1097000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1098000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1099000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1186000 documents\n",
      "DEBUG : performing inference on a chunk of 1186028 documents\n",
      "DEBUG : 1157709/1186028 documents converged within 50 iterations\n",
      "\n",
      "\n",
      " 40%|      | 2/5 [16:44<25:06, 502.12s/it]\u001b[A\u001b[AINFO : using symmetric alpha at 0.06666666666666667\n",
      "INFO : using symmetric eta at 0.06666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 15 topics, 3 passes over the supplied corpus of 1186028 documents, updating every 88000 documents, evaluating every ~880000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 11 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29DEBUG : processing chunk #6 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6183/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 33\n",
      "DEBUG : 6176/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 33\n",
      "DEBUG : 6048/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6156/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6126/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6081/8000 documents converged within 50 iterations\n",
      "DEBUG : 6168/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 32\n",
      "DEBUG : 6117/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6083/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 33\n",
      "DEBUG : 6156/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : 6099/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6090/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6102/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6106/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #8 (0.067): 0.014*\"new\" + 0.014*\"size\" + 0.013*\"woman\" + 0.011*\"legging\" + 0.011*\"pink\" + 0.011*\"brand_new\" + 0.011*\"game\" + 0.010*\"black\" + 0.010*\"rm\" + 0.009*\"case\"\n",
      "DEBUG : result put\n",
      "INFO : topic #14 (0.067): 0.018*\"beauty_makeup\" + 0.015*\"woman\" + 0.010*\"brand_new\" + 0.009*\"black\" + 0.008*\"lip\" + 0.008*\"item\" + 0.008*\"face\" + 0.008*\"lularoe\" + 0.007*\"size\" + 0.007*\"bundle\"\n",
      "INFO : topic #7 (0.067): 0.035*\"size\" + 0.031*\"woman\" + 0.015*\"new\" + 0.015*\"shirt\" + 0.014*\"black\" + 0.008*\"2\" + 0.008*\"large\" + 0.007*\"medium\" + 0.007*\"shoe\" + 0.007*\"pink\"\n",
      "INFO : topic #0 (0.067): 0.025*\"size\" + 0.013*\"1\" + 0.011*\"man\" + 0.010*\"color\" + 0.008*\"4\" + 0.007*\"home\" + 0.007*\"black\" + 0.007*\"brand_new\" + 0.006*\"2\" + 0.006*\"3\"\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.067): 0.035*\"woman\" + 0.030*\"size\" + 0.014*\"dress\" + 0.013*\"brand_new\" + 0.011*\"new\" + 0.010*\"rm\" + 0.010*\"small\" + 0.010*\"2\" + 0.009*\"black\" + 0.009*\"pant\"\n",
      "DEBUG : 6073/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=15.596167, rho=1.000000\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 34\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6097/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 33\n",
      "DEBUG : 6205/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 33\n",
      "DEBUG : 6165/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6164/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : 6104/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6186/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6160/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6096/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6551/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #12 (0.067): 0.022*\"woman\" + 0.013*\"size\" + 0.012*\"3\" + 0.009*\"pink\" + 0.008*\"box\" + 0.008*\"accessory\" + 0.008*\"2\" + 0.008*\"5\" + 0.007*\"1\" + 0.007*\"brand_new\"\n",
      "INFO : topic #1 (0.067): 0.035*\"woman\" + 0.029*\"size\" + 0.013*\"dress\" + 0.013*\"brand_new\" + 0.011*\"new\" + 0.010*\"rm\" + 0.010*\"small\" + 0.010*\"2\" + 0.009*\"black\" + 0.009*\"pant\"\n",
      "INFO : topic #4 (0.067): 0.018*\"size\" + 0.013*\"brand_new\" + 0.012*\"woman\" + 0.012*\"2\" + 0.012*\"rm\" + 0.009*\"new\" + 0.009*\"man\" + 0.009*\"1\" + 0.008*\"small\" + 0.008*\"beauty_makeup\"\n",
      "INFO : topic #7 (0.067): 0.034*\"size\" + 0.030*\"woman\" + 0.015*\"new\" + 0.015*\"shirt\" + 0.014*\"black\" + 0.008*\"2\" + 0.008*\"large\" + 0.007*\"medium\" + 0.007*\"shoe\" + 0.007*\"pink\"\n",
      "INFO : topic #6 (0.067): 0.023*\"woman\" + 0.018*\"size\" + 0.009*\"brand_new\" + 0.009*\"game\" + 0.007*\"2\" + 0.007*\"shirt\" + 0.006*\"black\" + 0.006*\"man\" + 0.006*\"case\" + 0.006*\"rm\"\n",
      "INFO : topic diff=0.385520, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "DEBUG : 6102/8000 documents converged within 50 iterations\n",
      "DEBUG : 6489/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6544/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 32DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 33\n",
      "DEBUG : 6513/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 6525/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6546/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 33\n",
      "DEBUG : 6521/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6572/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 33\n",
      "DEBUG : 6490/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6547/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6496/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6447/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.067): 0.015*\"new\" + 0.014*\"legging\" + 0.014*\"size\" + 0.012*\"game\" + 0.012*\"woman\" + 0.012*\"brand_new\" + 0.012*\"pink\" + 0.011*\"case\" + 0.010*\"black\" + 0.009*\"woman_athletic\"\n",
      "INFO : topic #11 (0.067): 0.030*\"size\" + 0.015*\"new\" + 0.011*\"2\" + 0.011*\"short\" + 0.010*\"woman\" + 0.010*\"woman_athletic\" + 0.010*\"3\" + 0.008*\"8\" + 0.008*\"6\" + 0.008*\"shoe\"\n",
      "INFO : topic #13 (0.067): 0.029*\"pink\" + 0.022*\"woman\" + 0.019*\"size\" + 0.014*\"new\" + 0.012*\"man\" + 0.010*\"victoria_secret\" + 0.009*\"shirt\" + 0.009*\"color\" + 0.007*\"small\" + 0.006*\"woman_athletic\"\n",
      "INFO : topic #7 (0.067): 0.037*\"size\" + 0.033*\"woman\" + 0.016*\"shirt\" + 0.015*\"black\" + 0.015*\"new\" + 0.009*\"shoe\" + 0.008*\"medium\" + 0.008*\"large\" + 0.008*\"small\" + 0.007*\"2\"\n",
      "INFO : topic #12 (0.067): 0.021*\"woman\" + 0.013*\"size\" + 0.013*\"3\" + 0.010*\"accessory\" + 0.009*\"box\" + 0.008*\"pink\" + 0.008*\"2\" + 0.008*\"baby\" + 0.008*\"5\" + 0.008*\"brush\"\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "INFO : topic diff=0.293023, rho=0.208514\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6458/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6468/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 33\n",
      "DEBUG : 6478/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 33\n",
      "DEBUG : 6443/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 33\n",
      "DEBUG : 6480/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 33DEBUG : processing chunk #51 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6498/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6459/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 33\n",
      "DEBUG : 6539/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6507/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6442/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.023*\"woman\" + 0.017*\"size\" + 0.011*\"game\" + 0.009*\"brand_new\" + 0.007*\"2\" + 0.007*\"electronic_video\" + 0.007*\"game_console\" + 0.007*\"black\" + 0.006*\"man\" + 0.006*\"case\"\n",
      "DEBUG : 6864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.018*\"woman\" + 0.015*\"black\" + 0.011*\"color\" + 0.010*\"brand_new\" + 0.009*\"size\" + 0.008*\"new\" + 0.008*\"bag\" + 0.007*\"bundle\" + 0.006*\"beauty_makeup\" + 0.006*\"2\"\n",
      "INFO : topic #8 (0.067): 0.017*\"legging\" + 0.015*\"new\" + 0.013*\"game\" + 0.013*\"size\" + 0.012*\"case\" + 0.012*\"brand_new\" + 0.012*\"pink\" + 0.011*\"woman_athletic\" + 0.011*\"woman\" + 0.011*\"black\"\n",
      "INFO : topic #4 (0.067): 0.017*\"size\" + 0.013*\"brand_new\" + 0.013*\"rm\" + 0.012*\"2\" + 0.010*\"woman\" + 0.010*\"new\" + 0.009*\"man\" + 0.009*\"1\" + 0.007*\"small\" + 0.007*\"woman_jewelry\"\n",
      "INFO : topic #5 (0.067): 0.030*\"size\" + 0.017*\"bra\" + 0.012*\"shirt\" + 0.011*\"small\" + 0.011*\"man\" + 0.011*\"woman\" + 0.009*\"shoe\" + 0.008*\"woman_underwear\" + 0.007*\"dress\" + 0.007*\"vintage_collectible\"\n",
      "INFO : topic diff=0.226884, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6909/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6897/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 33\n",
      "DEBUG : 6907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 33\n",
      "DEBUG : 6834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6871/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6907/8000 documents converged within 50 iterations\n",
      "DEBUG : 6869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : 6913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 6891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7032/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7101/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.067): 0.043*\"woman\" + 0.032*\"size\" + 0.021*\"dress\" + 0.012*\"brand_new\" + 0.011*\"pant\" + 0.011*\"jean\" + 0.010*\"small\" + 0.010*\"black\" + 0.010*\"new\" + 0.010*\"rm\"\n",
      "INFO : topic #7 (0.067): 0.040*\"size\" + 0.037*\"woman\" + 0.017*\"shirt\" + 0.016*\"black\" + 0.014*\"new\" + 0.011*\"shoe\" + 0.009*\"medium\" + 0.009*\"large\" + 0.009*\"sweater\" + 0.009*\"small\"\n",
      "INFO : topic #8 (0.067): 0.019*\"legging\" + 0.015*\"new\" + 0.014*\"game\" + 0.014*\"case\" + 0.013*\"size\" + 0.013*\"woman_athletic\" + 0.012*\"brand_new\" + 0.012*\"lularoe\" + 0.012*\"tight_legging\" + 0.012*\"apparel_pant\"\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #11 (0.067): 0.031*\"size\" + 0.016*\"short\" + 0.015*\"new\" + 0.012*\"woman_athletic\" + 0.011*\"2\" + 0.010*\"necklace\" + 0.010*\"3\" + 0.009*\"woman\" + 0.008*\"8\" + 0.008*\"6\"\n",
      "DEBUG : 7058/8000 documents converged within 50 iterations\n",
      "INFO : topic #13 (0.067): 0.034*\"pink\" + 0.023*\"woman\" + 0.019*\"size\" + 0.015*\"new\" + 0.014*\"man\" + 0.014*\"victoria_secret\" + 0.009*\"shirt\" + 0.009*\"color\" + 0.008*\"small\" + 0.007*\"woman_athletic\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.197045, rho=0.149071\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7081/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7081/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 31\n",
      "DEBUG : 7075/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7049/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7081/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 32\n",
      "DEBUG : 7102/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6984/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7101/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7046/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.067): 0.032*\"size\" + 0.018*\"short\" + 0.015*\"new\" + 0.014*\"woman_athletic\" + 0.012*\"necklace\" + 0.011*\"2\" + 0.010*\"3\" + 0.009*\"8\" + 0.009*\"6\" + 0.009*\"woman\"\n",
      "INFO : topic #9 (0.067): 0.032*\"woman_top\" + 0.027*\"shirt\" + 0.021*\"blouse\" + 0.015*\"blouse_t\" + 0.014*\"size\" + 0.010*\"kid_toy\" + 0.009*\"brand_new\" + 0.008*\"color\" + 0.007*\"description\" + 0.007*\"new\"\n",
      "INFO : topic #1 (0.067): 0.045*\"woman\" + 0.033*\"size\" + 0.022*\"dress\" + 0.012*\"jean\" + 0.012*\"pant\" + 0.011*\"brand_new\" + 0.010*\"jacket\" + 0.010*\"small\" + 0.010*\"black\" + 0.010*\"rm\"\n",
      "INFO : topic #10 (0.067): 0.021*\"woman\" + 0.018*\"item\" + 0.017*\"rm\" + 0.014*\"bag\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"woman_handbag\" + 0.008*\"purse\" + 0.008*\"shipping\" + 0.007*\"size\"\n",
      "INFO : topic #7 (0.067): 0.041*\"size\" + 0.038*\"woman\" + 0.017*\"shirt\" + 0.016*\"black\" + 0.013*\"new\" + 0.012*\"shoe\" + 0.010*\"sweater\" + 0.009*\"medium\" + 0.009*\"large\" + 0.009*\"small\"\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7206/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.183001, rho=0.133631\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7237/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7164/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7251/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7270/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7219/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 33DEBUG : processing chunk #84 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7254/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7197/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 33\n",
      "DEBUG : 7185/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7336/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7240/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7283/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.067): 0.015*\"woman\" + 0.014*\"black\" + 0.011*\"color\" + 0.009*\"brand_new\" + 0.008*\"bag\" + 0.008*\"new\" + 0.007*\"size\" + 0.006*\"hair\" + 0.006*\"bundle\" + 0.006*\"2\"\n",
      "INFO : topic #8 (0.067): 0.022*\"legging\" + 0.018*\"case\" + 0.016*\"game\" + 0.015*\"woman_athletic\" + 0.015*\"tight_legging\" + 0.015*\"apparel_pant\" + 0.015*\"lularoe\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.012*\"size\"\n",
      "INFO : topic #6 (0.067): 0.024*\"woman\" + 0.017*\"size\" + 0.015*\"game\" + 0.010*\"electronic_video\" + 0.009*\"game_console\" + 0.009*\"brand_new\" + 0.007*\"2\" + 0.007*\"piece\" + 0.007*\"skinny\" + 0.007*\"man\"\n",
      "INFO : topic #10 (0.067): 0.022*\"woman\" + 0.019*\"item\" + 0.017*\"rm\" + 0.015*\"bag\" + 0.009*\"brand_new\" + 0.009*\"woman_handbag\" + 0.008*\"purse\" + 0.008*\"new\" + 0.008*\"shipping\" + 0.007*\"size\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #5 (0.067): 0.030*\"size\" + 0.027*\"bra\" + 0.013*\"woman_underwear\" + 0.012*\"man\" + 0.011*\"small\" + 0.011*\"shirt\" + 0.009*\"vintage_collectible\" + 0.009*\"woman\" + 0.008*\"shoe\" + 0.007*\"black\"\n",
      "INFO : topic diff=0.173275, rho=0.122169\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7324/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7324/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7344/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7318/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 30\n",
      "DEBUG : 7330/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 31DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 7244/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 31\n",
      "DEBUG : 7360/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 32DEBUG : 7307/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7363/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7313/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.025*\"woman\" + 0.016*\"size\" + 0.016*\"game\" + 0.011*\"electronic_video\" + 0.010*\"game_console\" + 0.009*\"brand_new\" + 0.007*\"2\" + 0.007*\"skinny\" + 0.007*\"piece\" + 0.007*\"man\"\n",
      "INFO : topic #12 (0.067): 0.015*\"woman\" + 0.015*\"3\" + 0.014*\"accessory\" + 0.014*\"size\" + 0.014*\"baby\" + 0.013*\"0_24\" + 0.013*\"kid_girl\" + 0.010*\"brush\" + 0.010*\"2\" + 0.010*\"shoe\"\n",
      "INFO : topic #4 (0.067): 0.015*\"size\" + 0.014*\"rm\" + 0.013*\"2\" + 0.012*\"brand_new\" + 0.012*\"ring\" + 0.011*\"woman_jewelry\" + 0.010*\"bracelet\" + 0.010*\"new\" + 0.009*\"1\" + 0.008*\"man\"\n",
      "INFO : topic #5 (0.067): 0.030*\"size\" + 0.029*\"bra\" + 0.014*\"woman_underwear\" + 0.012*\"man\" + 0.011*\"small\" + 0.010*\"shirt\" + 0.010*\"vintage_collectible\" + 0.008*\"woman\" + 0.008*\"shoe\" + 0.007*\"black\"\n",
      "INFO : topic #10 (0.067): 0.023*\"woman\" + 0.019*\"item\" + 0.017*\"rm\" + 0.017*\"bag\" + 0.010*\"woman_handbag\" + 0.009*\"purse\" + 0.009*\"brand_new\" + 0.008*\"shipping\" + 0.008*\"new\" + 0.006*\"leather\"\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.164267, rho=0.113228\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 32\n",
      "DEBUG : 7418/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7350/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7395/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 33DEBUG : 7371/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7389/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7412/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7385/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 33\n",
      "DEBUG : 7379/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 33\n",
      "DEBUG : 7424/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7455/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7413/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #10 (0.067): 0.024*\"woman\" + 0.019*\"item\" + 0.018*\"bag\" + 0.017*\"rm\" + 0.011*\"woman_handbag\" + 0.009*\"purse\" + 0.009*\"brand_new\" + 0.008*\"shipping\" + 0.008*\"new\" + 0.007*\"leather\"\n",
      "DEBUG : 7447/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.067): 0.048*\"woman\" + 0.034*\"size\" + 0.027*\"dress\" + 0.015*\"jean\" + 0.013*\"pant\" + 0.011*\"jacket\" + 0.011*\"brand_new\" + 0.010*\"black\" + 0.010*\"small\" + 0.010*\"dress_knee\"\n",
      "INFO : topic #4 (0.067): 0.015*\"size\" + 0.015*\"rm\" + 0.013*\"ring\" + 0.013*\"2\" + 0.012*\"brand_new\" + 0.012*\"woman_jewelry\" + 0.011*\"bracelet\" + 0.010*\"new\" + 0.009*\"1\" + 0.008*\"gold\"\n",
      "INFO : topic #13 (0.067): 0.042*\"pink\" + 0.026*\"woman\" + 0.020*\"victoria_secret\" + 0.019*\"size\" + 0.016*\"man\" + 0.016*\"new\" + 0.009*\"shirt\" + 0.008*\"color\" + 0.008*\"small\" + 0.007*\"woman_athletic\"\n",
      "INFO : topic #14 (0.067): 0.042*\"beauty_makeup\" + 0.018*\"lip\" + 0.016*\"face\" + 0.015*\"brand_new\" + 0.013*\"color\" + 0.013*\"eye\" + 0.011*\"new\" + 0.009*\"bundle\" + 0.008*\"shade\" + 0.007*\"makeup\"\n",
      "INFO : topic diff=0.159542, rho=0.106000\n",
      "DEBUG : 7464/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 33\n",
      "DEBUG : 7454/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7456/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7436/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7458/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7444/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7457/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7442/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 26\n",
      "DEBUG : 7443/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7508/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7422/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #0 (0.067): 0.019*\"home\" + 0.018*\"1\" + 0.017*\"home_dcor\" + 0.014*\"size\" + 0.010*\"man\" + 0.009*\"2\" + 0.008*\"4\" + 0.007*\"3\" + 0.007*\"brand_new\" + 0.007*\"color\"\n",
      "INFO : topic #6 (0.067): 0.026*\"woman\" + 0.019*\"game\" + 0.016*\"size\" + 0.012*\"electronic_video\" + 0.012*\"game_console\" + 0.009*\"skinny\" + 0.008*\"brand_new\" + 0.008*\"2\" + 0.007*\"piece\" + 0.007*\"man\"\n",
      "INFO : topic #2 (0.067): 0.013*\"woman\" + 0.013*\"black\" + 0.011*\"color\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"hair\" + 0.008*\"bag\" + 0.006*\"2\" + 0.006*\"size\" + 0.006*\"bundle\"\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7487/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.018*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"2\" + 0.007*\"free_shipping\" + 0.007*\"rm\" + 0.006*\"color\" + 0.006*\"price\" + 0.006*\"item\"\n",
      "INFO : topic #9 (0.067): 0.041*\"woman_top\" + 0.033*\"shirt\" + 0.026*\"blouse\" + 0.018*\"blouse_t\" + 0.015*\"size\" + 0.013*\"kid_toy\" + 0.009*\"brand_new\" + 0.008*\"description\" + 0.007*\"small\" + 0.007*\"color\"\n",
      "INFO : topic diff=0.164184, rho=0.100000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 25\n",
      "DEBUG : 7484/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 27\n",
      "DEBUG : 7476/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7532/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 23\n",
      "DEBUG : 7514/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #126 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 29DEBUG : 7522/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7511/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7517/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 28\n",
      "DEBUG : 7511/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7572/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7531/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7563/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.067): 0.032*\"size\" + 0.026*\"short\" + 0.019*\"necklace\" + 0.019*\"woman_athletic\" + 0.015*\"new\" + 0.012*\"woman_jewelry\" + 0.011*\"2\" + 0.011*\"apparel_short\" + 0.011*\"sport_bra\" + 0.009*\"3\"\n",
      "INFO : topic #4 (0.067): 0.015*\"rm\" + 0.015*\"size\" + 0.014*\"ring\" + 0.013*\"woman_jewelry\" + 0.012*\"2\" + 0.012*\"bracelet\" + 0.012*\"brand_new\" + 0.010*\"new\" + 0.009*\"gold\" + 0.009*\"1\"\n",
      "INFO : topic #8 (0.067): 0.028*\"legging\" + 0.023*\"case\" + 0.020*\"tight_legging\" + 0.020*\"apparel_pant\" + 0.020*\"woman_athletic\" + 0.019*\"lularoe\" + 0.016*\"game\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.012*\"size\"\n",
      "INFO : topic #3 (0.067): 0.018*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"2\" + 0.007*\"free_shipping\" + 0.007*\"rm\" + 0.006*\"color\" + 0.006*\"price\" + 0.006*\"item\"\n",
      "INFO : topic #6 (0.067): 0.026*\"woman\" + 0.021*\"game\" + 0.015*\"size\" + 0.013*\"electronic_video\" + 0.013*\"game_console\" + 0.009*\"skinny\" + 0.008*\"brand_new\" + 0.008*\"2\" + 0.007*\"piece\" + 0.007*\"man\"\n",
      "INFO : topic diff=0.147909, rho=0.094491\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7544/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7568/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7537/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7526/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : 7543/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7517/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7544/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7556/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7555/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7561/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7490/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7565/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7584/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.044*\"size\" + 0.043*\"woman\" + 0.018*\"shirt\" + 0.017*\"black\" + 0.016*\"shoe\" + 0.014*\"man\" + 0.012*\"new\" + 0.011*\"sweater\" + 0.010*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #12 (0.067): 0.017*\"kid_girl\" + 0.016*\"baby\" + 0.016*\"3\" + 0.016*\"0_24\" + 0.016*\"accessory\" + 0.015*\"size\" + 0.012*\"woman\" + 0.012*\"kid_boy\" + 0.012*\"girl\" + 0.011*\"mo\"\n",
      "INFO : topic #2 (0.067): 0.012*\"black\" + 0.012*\"woman\" + 0.011*\"color\" + 0.009*\"brand_new\" + 0.009*\"hair\" + 0.008*\"new\" + 0.007*\"bag\" + 0.007*\"2\" + 0.006*\"bundle\" + 0.006*\"size\"\n",
      "DEBUG : 7560/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #5 (0.067): 0.034*\"bra\" + 0.029*\"size\" + 0.017*\"woman_underwear\" + 0.012*\"vintage_collectible\" + 0.011*\"man\" + 0.010*\"small\" + 0.009*\"book\" + 0.008*\"shirt\" + 0.008*\"black\" + 0.007*\"brand_new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7569/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.067): 0.029*\"legging\" + 0.024*\"case\" + 0.021*\"tight_legging\" + 0.021*\"apparel_pant\" + 0.020*\"woman_athletic\" + 0.020*\"lularoe\" + 0.016*\"game\" + 0.015*\"new\" + 0.013*\"brand_new\" + 0.012*\"phone_accessory\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.153838, rho=0.090167\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7561/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7576/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7587/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7608/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7573/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 1935/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #9 (0.067): 0.045*\"woman_top\" + 0.036*\"shirt\" + 0.028*\"blouse\" + 0.020*\"blouse_t\" + 0.015*\"size\" + 0.014*\"kid_toy\" + 0.009*\"brand_new\" + 0.008*\"small\" + 0.008*\"description\" + 0.007*\"blouse_tank\"\n",
      "INFO : topic #2 (0.067): 0.012*\"black\" + 0.011*\"woman\" + 0.011*\"color\" + 0.009*\"hair\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"2\" + 0.007*\"bag\" + 0.006*\"bundle\" + 0.005*\"body\"\n",
      "INFO : topic #12 (0.067): 0.017*\"kid_girl\" + 0.017*\"baby\" + 0.016*\"0_24\" + 0.016*\"3\" + 0.016*\"accessory\" + 0.015*\"size\" + 0.012*\"kid_boy\" + 0.012*\"girl\" + 0.012*\"mo\" + 0.011*\"woman\"\n",
      "INFO : topic #4 (0.067): 0.016*\"rm\" + 0.016*\"ring\" + 0.015*\"woman_jewelry\" + 0.014*\"size\" + 0.014*\"bracelet\" + 0.012*\"2\" + 0.012*\"brand_new\" + 0.010*\"gold\" + 0.010*\"new\" + 0.008*\"1\"\n",
      "INFO : topic #1 (0.067): 0.050*\"woman\" + 0.035*\"size\" + 0.030*\"dress\" + 0.018*\"jean\" + 0.015*\"pant\" + 0.012*\"jacket\" + 0.011*\"dress_knee\" + 0.011*\"black\" + 0.010*\"small\" + 0.010*\"length\"\n",
      "INFO : topic diff=0.143227, rho=0.086066\n",
      "DEBUG : 7609/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7583/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7602/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 26028 documents into a model of 1186028 documents\n",
      "INFO : topic #14 (0.067): 0.046*\"beauty_makeup\" + 0.020*\"lip\" + 0.017*\"face\" + 0.016*\"brand_new\" + 0.015*\"color\" + 0.015*\"eye\" + 0.013*\"new\" + 0.009*\"makeup\" + 0.009*\"bundle\" + 0.009*\"palette\"\n",
      "INFO : topic #8 (0.067): 0.031*\"legging\" + 0.026*\"case\" + 0.023*\"tight_legging\" + 0.023*\"apparel_pant\" + 0.022*\"lularoe\" + 0.022*\"woman_athletic\" + 0.015*\"game\" + 0.015*\"new\" + 0.014*\"brand_new\" + 0.013*\"phone_accessory\"\n",
      "INFO : topic #0 (0.067): 0.022*\"home\" + 0.019*\"home_dcor\" + 0.018*\"1\" + 0.011*\"size\" + 0.010*\"2\" + 0.008*\"4\" + 0.008*\"man\" + 0.008*\"3\" + 0.007*\"glass\" + 0.007*\"accent\"\n",
      "INFO : topic #7 (0.067): 0.045*\"size\" + 0.044*\"woman\" + 0.019*\"shirt\" + 0.017*\"shoe\" + 0.017*\"black\" + 0.015*\"man\" + 0.012*\"new\" + 0.011*\"sweater\" + 0.010*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #2 (0.067): 0.011*\"black\" + 0.011*\"woman\" + 0.011*\"color\" + 0.010*\"hair\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"2\" + 0.006*\"bag\" + 0.006*\"kid_toy\" + 0.006*\"body\"\n",
      "INFO : topic diff=0.077490, rho=0.082761\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.430 per-word bound, 172.4 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7644/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7664/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7677/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7649/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7644/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7654/8000 documents converged within 50 iterations\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7675/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 30\n",
      "DEBUG : 7686/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7653/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 29DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7637/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7673/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7642/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7663/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7649/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.067): 0.046*\"beauty_makeup\" + 0.020*\"lip\" + 0.017*\"face\" + 0.016*\"brand_new\" + 0.015*\"eye\" + 0.015*\"color\" + 0.013*\"new\" + 0.009*\"makeup\" + 0.009*\"bundle\" + 0.009*\"palette\"\n",
      "INFO : topic #2 (0.067): 0.011*\"black\" + 0.011*\"color\" + 0.010*\"woman\" + 0.010*\"hair\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"2\" + 0.006*\"bag\" + 0.006*\"kid_toy\" + 0.006*\"home_kitchen\"\n",
      "INFO : topic #5 (0.067): 0.037*\"bra\" + 0.028*\"size\" + 0.018*\"woman_underwear\" + 0.014*\"vintage_collectible\" + 0.010*\"man\" + 0.010*\"book\" + 0.010*\"small\" + 0.008*\"black\" + 0.007*\"brand_new\" + 0.007*\"shirt\"\n",
      "INFO : topic #6 (0.067): 0.028*\"game\" + 0.025*\"woman\" + 0.018*\"electronic_video\" + 0.017*\"game_console\" + 0.014*\"size\" + 0.010*\"skinny\" + 0.008*\"2\" + 0.008*\"brand_new\" + 0.008*\"jean_slim\" + 0.007*\"man\"\n",
      "INFO : topic #13 (0.067): 0.051*\"pink\" + 0.029*\"woman\" + 0.025*\"victoria_secret\" + 0.019*\"size\" + 0.019*\"man\" + 0.017*\"new\" + 0.008*\"beauty_fragrance\" + 0.008*\"small\" + 0.008*\"color\" + 0.008*\"brand_new\"\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.040566, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 29\n",
      "DEBUG : 7701/8000 documents converged within 50 iterations\n",
      "DEBUG : 7645/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7682/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7659/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7651/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7668/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7725/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7670/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.067): 0.029*\"woman\" + 0.025*\"bag\" + 0.020*\"item\" + 0.018*\"rm\" + 0.015*\"woman_handbag\" + 0.012*\"purse\" + 0.010*\"shipping\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"leather\"\n",
      "INFO : topic #4 (0.067): 0.017*\"ring\" + 0.017*\"rm\" + 0.016*\"woman_jewelry\" + 0.015*\"bracelet\" + 0.014*\"size\" + 0.012*\"2\" + 0.011*\"brand_new\" + 0.011*\"gold\" + 0.010*\"new\" + 0.008*\"1\"\n",
      "INFO : topic #8 (0.067): 0.033*\"legging\" + 0.028*\"case\" + 0.024*\"tight_legging\" + 0.024*\"apparel_pant\" + 0.024*\"lularoe\" + 0.023*\"woman_athletic\" + 0.015*\"new\" + 0.014*\"phone_accessory\" + 0.014*\"electronic_cell\" + 0.014*\"brand_new\"\n",
      "INFO : topic #12 (0.067): 0.020*\"kid_girl\" + 0.018*\"baby\" + 0.018*\"0_24\" + 0.017*\"3\" + 0.017*\"accessory\" + 0.016*\"size\" + 0.014*\"girl\" + 0.014*\"kid_boy\" + 0.013*\"mo\" + 0.012*\"2\"\n",
      "INFO : topic #14 (0.067): 0.047*\"beauty_makeup\" + 0.020*\"lip\" + 0.017*\"face\" + 0.016*\"brand_new\" + 0.015*\"color\" + 0.015*\"eye\" + 0.013*\"new\" + 0.010*\"makeup\" + 0.009*\"bundle\" + 0.009*\"palette\"\n",
      "INFO : topic diff=0.044703, rho=0.081581\n",
      "DEBUG : 7691/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7705/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 29\n",
      "DEBUG : 7688/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7707/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7698/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7708/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7730/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7703/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7725/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7689/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.017*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"2\" + 0.008*\"box\" + 0.006*\"free_shipping\" + 0.006*\"rm\" + 0.006*\"color\" + 0.006*\"skin\" + 0.006*\"item\"\n",
      "INFO : topic #4 (0.067): 0.017*\"ring\" + 0.017*\"rm\" + 0.017*\"woman_jewelry\" + 0.016*\"bracelet\" + 0.014*\"size\" + 0.012*\"2\" + 0.011*\"gold\" + 0.011*\"brand_new\" + 0.010*\"new\" + 0.008*\"1\"\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "INFO : topic #2 (0.067): 0.011*\"black\" + 0.010*\"hair\" + 0.010*\"color\" + 0.009*\"woman\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"2\" + 0.006*\"home_kitchen\" + 0.006*\"kid_toy\" + 0.006*\"body\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7699/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #7 (0.067): 0.045*\"size\" + 0.045*\"woman\" + 0.019*\"shirt\" + 0.018*\"shoe\" + 0.017*\"man\" + 0.017*\"black\" + 0.012*\"sweater\" + 0.011*\"new\" + 0.010*\"small\" + 0.009*\"medium\"\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #9 (0.067): 0.049*\"woman_top\" + 0.040*\"shirt\" + 0.029*\"blouse\" + 0.021*\"blouse_t\" + 0.017*\"size\" + 0.015*\"kid_toy\" + 0.009*\"small\" + 0.009*\"tank\" + 0.008*\"blouse_tank\" + 0.008*\"tee\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.049210, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7708/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7720/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7699/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 29DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7715/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7735/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7778/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7714/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7722/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.067): 0.018*\"ring\" + 0.017*\"woman_jewelry\" + 0.017*\"rm\" + 0.016*\"bracelet\" + 0.014*\"size\" + 0.012*\"2\" + 0.012*\"gold\" + 0.011*\"brand_new\" + 0.010*\"new\" + 0.008*\"1\"\n",
      "INFO : topic #1 (0.067): 0.052*\"woman\" + 0.037*\"size\" + 0.033*\"dress\" + 0.020*\"jean\" + 0.016*\"pant\" + 0.013*\"dress_knee\" + 0.012*\"jacket\" + 0.011*\"length\" + 0.011*\"black\" + 0.011*\"small\"\n",
      "INFO : topic #14 (0.067): 0.047*\"beauty_makeup\" + 0.020*\"lip\" + 0.017*\"face\" + 0.017*\"brand_new\" + 0.016*\"eye\" + 0.016*\"color\" + 0.014*\"new\" + 0.010*\"makeup\" + 0.009*\"palette\" + 0.009*\"bundle\"\n",
      "INFO : topic #2 (0.067): 0.011*\"hair\" + 0.010*\"color\" + 0.010*\"black\" + 0.009*\"woman\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"2\" + 0.007*\"home_kitchen\" + 0.007*\"kid_toy\" + 0.006*\"1\"\n",
      "INFO : topic #8 (0.067): 0.034*\"legging\" + 0.030*\"case\" + 0.026*\"tight_legging\" + 0.026*\"apparel_pant\" + 0.025*\"lularoe\" + 0.024*\"woman_athletic\" + 0.015*\"phone_accessory\" + 0.015*\"electronic_cell\" + 0.014*\"new\" + 0.014*\"brand_new\"\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.049107, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 28\n",
      "DEBUG : 7740/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7699/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7738/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7746/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 29\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7753/8000 documents converged within 50 iterations\n",
      "DEBUG : 7735/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7729/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #8 (0.067): 0.035*\"legging\" + 0.031*\"case\" + 0.027*\"tight_legging\" + 0.027*\"apparel_pant\" + 0.026*\"lularoe\" + 0.025*\"woman_athletic\" + 0.015*\"phone_accessory\" + 0.015*\"electronic_cell\" + 0.014*\"new\" + 0.014*\"brand_new\"\n",
      "INFO : topic #2 (0.067): 0.011*\"hair\" + 0.010*\"color\" + 0.010*\"black\" + 0.009*\"brand_new\" + 0.009*\"woman\" + 0.008*\"new\" + 0.007*\"2\" + 0.007*\"home_kitchen\" + 0.007*\"kid_toy\" + 0.006*\"1\"\n",
      "INFO : topic #4 (0.067): 0.018*\"ring\" + 0.018*\"woman_jewelry\" + 0.017*\"rm\" + 0.017*\"bracelet\" + 0.013*\"size\" + 0.012*\"2\" + 0.012*\"gold\" + 0.011*\"brand_new\" + 0.010*\"new\" + 0.008*\"earring\"\n",
      "INFO : topic #12 (0.067): 0.022*\"kid_girl\" + 0.019*\"baby\" + 0.018*\"0_24\" + 0.017*\"3\" + 0.017*\"size\" + 0.017*\"accessory\" + 0.015*\"girl\" + 0.015*\"kid_boy\" + 0.013*\"mo\" + 0.012*\"2\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.025*\"home\" + 0.021*\"home_dcor\" + 0.018*\"1\" + 0.010*\"2\" + 0.009*\"size\" + 0.008*\"4\" + 0.008*\"accent\" + 0.008*\"bath_body\" + 0.008*\"glass\" + 0.008*\"3\"\n",
      "DEBUG : 7730/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.048963, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7725/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7740/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 27\n",
      "DEBUG : 7711/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7735/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7749/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.067): 0.030*\"woman\" + 0.028*\"bag\" + 0.021*\"item\" + 0.018*\"rm\" + 0.016*\"woman_handbag\" + 0.012*\"purse\" + 0.010*\"shipping\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"wallet\"\n",
      "INFO : topic #13 (0.067): 0.057*\"pink\" + 0.032*\"woman\" + 0.029*\"victoria_secret\" + 0.020*\"man\" + 0.019*\"size\" + 0.017*\"new\" + 0.010*\"beauty_fragrance\" + 0.009*\"brand_new\" + 0.009*\"small\" + 0.008*\"color\"\n",
      "INFO : topic #5 (0.067): 0.040*\"bra\" + 0.028*\"size\" + 0.019*\"woman_underwear\" + 0.015*\"vintage_collectible\" + 0.012*\"book\" + 0.009*\"small\" + 0.009*\"man\" + 0.008*\"black\" + 0.007*\"brand_new\" + 0.007*\"victoria_secret\"\n",
      "INFO : topic #4 (0.067): 0.019*\"ring\" + 0.018*\"woman_jewelry\" + 0.017*\"rm\" + 0.017*\"bracelet\" + 0.013*\"size\" + 0.012*\"gold\" + 0.012*\"2\" + 0.011*\"brand_new\" + 0.010*\"new\" + 0.008*\"earring\"\n",
      "DEBUG : 7740/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #14 (0.067): 0.048*\"beauty_makeup\" + 0.021*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.016*\"eye\" + 0.016*\"color\" + 0.014*\"new\" + 0.010*\"makeup\" + 0.009*\"palette\" + 0.009*\"bundle\"\n",
      "INFO : topic diff=0.049478, rho=0.081581\n",
      "DEBUG : 7760/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7739/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7750/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7736/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 29\n",
      "DEBUG : 7747/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7749/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7741/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7753/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7767/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.046*\"size\" + 0.046*\"woman\" + 0.019*\"shoe\" + 0.019*\"man\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.012*\"sweater\" + 0.011*\"new\" + 0.011*\"small\" + 0.009*\"nike\"\n",
      "INFO : topic #2 (0.067): 0.011*\"hair\" + 0.010*\"color\" + 0.010*\"black\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"home_kitchen\" + 0.008*\"woman\" + 0.007*\"2\" + 0.007*\"kid_toy\" + 0.006*\"rae\"\n",
      "INFO : topic #1 (0.067): 0.053*\"woman\" + 0.037*\"size\" + 0.035*\"dress\" + 0.021*\"jean\" + 0.016*\"pant\" + 0.013*\"dress_knee\" + 0.012*\"length\" + 0.011*\"black\" + 0.011*\"jacket\" + 0.011*\"small\"\n",
      "INFO : topic #3 (0.067): 0.016*\"new\" + 0.011*\"brand_new\" + 0.009*\"2\" + 0.009*\"1\" + 0.008*\"box\" + 0.006*\"iphone\" + 0.006*\"free_shipping\" + 0.006*\"color\" + 0.006*\"rm\" + 0.006*\"charger\"\n",
      "INFO : topic #8 (0.067): 0.037*\"legging\" + 0.032*\"case\" + 0.028*\"tight_legging\" + 0.028*\"apparel_pant\" + 0.027*\"lularoe\" + 0.026*\"woman_athletic\" + 0.016*\"phone_accessory\" + 0.016*\"electronic_cell\" + 0.014*\"new\" + 0.014*\"brand_new\"\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.049560, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 27\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7734/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 28\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7791/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 29\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7777/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7768/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7773/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.043*\"game\" + 0.026*\"electronic_video\" + 0.025*\"game_console\" + 0.024*\"woman\" + 0.011*\"skinny\" + 0.010*\"size\" + 0.009*\"2\" + 0.008*\"jean_slim\" + 0.008*\"brand_new\" + 0.007*\"case\"\n",
      "INFO : topic #14 (0.067): 0.049*\"beauty_makeup\" + 0.021*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.016*\"eye\" + 0.016*\"color\" + 0.014*\"new\" + 0.011*\"makeup\" + 0.009*\"palette\" + 0.009*\"bundle\"\n",
      "INFO : topic #11 (0.067): 0.038*\"short\" + 0.032*\"size\" + 0.030*\"necklace\" + 0.027*\"woman_athletic\" + 0.016*\"woman_jewelry\" + 0.015*\"apparel_short\" + 0.015*\"sport_bra\" + 0.014*\"new\" + 0.012*\"sock\" + 0.012*\"apparel\"\n",
      "INFO : topic #2 (0.067): 0.012*\"hair\" + 0.010*\"color\" + 0.009*\"black\" + 0.009*\"brand_new\" + 0.008*\"home_kitchen\" + 0.008*\"new\" + 0.008*\"woman\" + 0.007*\"2\" + 0.007*\"kid_toy\" + 0.007*\"rae\"\n",
      "INFO : topic #4 (0.067): 0.020*\"ring\" + 0.019*\"woman_jewelry\" + 0.018*\"bracelet\" + 0.018*\"rm\" + 0.013*\"size\" + 0.013*\"gold\" + 0.012*\"2\" + 0.011*\"brand_new\" + 0.010*\"new\" + 0.009*\"jewelry\"\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.049376, rho=0.081581\n",
      "DEBUG : 7756/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7760/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7756/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 24\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 27\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.012*\"hair\" + 0.010*\"color\" + 0.009*\"black\" + 0.009*\"home_kitchen\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"kid_toy\" + 0.007*\"2\" + 0.007*\"woman\" + 0.007*\"rae\"\n",
      "INFO : topic #4 (0.067): 0.020*\"ring\" + 0.019*\"woman_jewelry\" + 0.018*\"bracelet\" + 0.018*\"rm\" + 0.013*\"size\" + 0.013*\"gold\" + 0.012*\"2\" + 0.010*\"brand_new\" + 0.010*\"new\" + 0.009*\"jewelry\"\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #11 (0.067): 0.039*\"short\" + 0.032*\"size\" + 0.031*\"necklace\" + 0.027*\"woman_athletic\" + 0.016*\"woman_jewelry\" + 0.015*\"apparel_short\" + 0.015*\"sport_bra\" + 0.014*\"new\" + 0.013*\"sock\" + 0.012*\"apparel\"\n",
      "INFO : topic #12 (0.067): 0.024*\"kid_girl\" + 0.021*\"baby\" + 0.019*\"0_24\" + 0.018*\"size\" + 0.018*\"3\" + 0.017*\"girl\" + 0.017*\"accessory\" + 0.016*\"kid_boy\" + 0.014*\"mo\" + 0.013*\"2\"\n",
      "INFO : topic #8 (0.067): 0.038*\"legging\" + 0.034*\"case\" + 0.029*\"tight_legging\" + 0.029*\"apparel_pant\" + 0.028*\"lularoe\" + 0.027*\"woman_athletic\" + 0.017*\"phone_accessory\" + 0.017*\"electronic_cell\" + 0.014*\"brand_new\" + 0.014*\"new\"\n",
      "INFO : topic diff=0.050267, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 27\n",
      "DEBUG : 7757/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7785/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 28\n",
      "DEBUG : 7800/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 29\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7791/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7769/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7773/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.012*\"hair\" + 0.010*\"color\" + 0.009*\"home_kitchen\" + 0.009*\"black\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"kid_toy\" + 0.008*\"2\" + 0.007*\"rae\" + 0.007*\"woman\"\n",
      "INFO : topic #5 (0.067): 0.041*\"bra\" + 0.027*\"size\" + 0.020*\"woman_underwear\" + 0.017*\"vintage_collectible\" + 0.013*\"book\" + 0.009*\"small\" + 0.009*\"black\" + 0.008*\"man\" + 0.008*\"victoria_secret\" + 0.008*\"lace\"\n",
      "INFO : topic #7 (0.067): 0.046*\"woman\" + 0.046*\"size\" + 0.021*\"man\" + 0.020*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.012*\"sweater\" + 0.011*\"new\" + 0.011*\"small\" + 0.010*\"nike\"\n",
      "INFO : topic #3 (0.067): 0.016*\"new\" + 0.011*\"brand_new\" + 0.009*\"2\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"charger\" + 0.007*\"iphone\" + 0.006*\"color\" + 0.006*\"free_shipping\" + 0.006*\"rm\"\n",
      "INFO : topic #10 (0.067): 0.032*\"woman\" + 0.030*\"bag\" + 0.021*\"item\" + 0.018*\"rm\" + 0.017*\"woman_handbag\" + 0.013*\"purse\" + 0.011*\"shipping\" + 0.008*\"wallet\" + 0.008*\"new\" + 0.008*\"pocket\"\n",
      "INFO : topic diff=0.050327, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7770/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7787/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #12 (0.067): 0.025*\"kid_girl\" + 0.021*\"baby\" + 0.020*\"0_24\" + 0.019*\"size\" + 0.018*\"girl\" + 0.018*\"3\" + 0.017*\"accessory\" + 0.017*\"kid_boy\" + 0.014*\"mo\" + 0.013*\"2\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.015*\"new\" + 0.011*\"brand_new\" + 0.009*\"2\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"charger\" + 0.007*\"iphone\" + 0.006*\"color\" + 0.006*\"free_shipping\" + 0.006*\"rm\"\n",
      "DEBUG : processing chunk #137 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #9 (0.067): 0.053*\"woman_top\" + 0.046*\"shirt\" + 0.031*\"blouse\" + 0.023*\"blouse_t\" + 0.018*\"size\" + 0.016*\"kid_toy\" + 0.011*\"tank\" + 0.010*\"small\" + 0.010*\"tee\" + 0.010*\"blouse_tank\"\n",
      "INFO : topic #6 (0.067): 0.048*\"game\" + 0.029*\"electronic_video\" + 0.028*\"game_console\" + 0.022*\"woman\" + 0.011*\"skinny\" + 0.010*\"2\" + 0.009*\"size\" + 0.008*\"jean_slim\" + 0.008*\"brand_new\" + 0.007*\"accessory_sunglass\"\n",
      "INFO : topic #4 (0.067): 0.020*\"ring\" + 0.020*\"woman_jewelry\" + 0.019*\"bracelet\" + 0.018*\"rm\" + 0.013*\"gold\" + 0.013*\"size\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"jewelry\" + 0.010*\"new\"\n",
      "INFO : topic diff=0.049638, rho=0.081581\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7814/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7793/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.067): 0.021*\"ring\" + 0.020*\"woman_jewelry\" + 0.020*\"bracelet\" + 0.018*\"rm\" + 0.014*\"gold\" + 0.013*\"size\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"jewelry\" + 0.010*\"new\"\n",
      "INFO : topic #13 (0.067): 0.063*\"pink\" + 0.035*\"woman\" + 0.033*\"victoria_secret\" + 0.021*\"man\" + 0.019*\"size\" + 0.018*\"new\" + 0.011*\"beauty_fragrance\" + 0.010*\"brand_new\" + 0.009*\"small\" + 0.008*\"sweat\"\n",
      "INFO : topic #14 (0.067): 0.049*\"beauty_makeup\" + 0.021*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.017*\"eye\" + 0.017*\"color\" + 0.015*\"new\" + 0.011*\"makeup\" + 0.010*\"palette\" + 0.009*\"bundle\"\n",
      "INFO : topic #11 (0.067): 0.042*\"short\" + 0.033*\"necklace\" + 0.032*\"size\" + 0.029*\"woman_athletic\" + 0.017*\"woman_jewelry\" + 0.016*\"sport_bra\" + 0.016*\"apparel_short\" + 0.013*\"apparel\" + 0.013*\"new\" + 0.013*\"sock\"\n",
      "INFO : topic #8 (0.067): 0.041*\"legging\" + 0.036*\"case\" + 0.031*\"tight_legging\" + 0.031*\"apparel_pant\" + 0.030*\"lularoe\" + 0.029*\"woman_athletic\" + 0.018*\"phone_accessory\" + 0.018*\"electronic_cell\" + 0.014*\"brand_new\" + 0.014*\"new\"\n",
      "INFO : topic diff=0.049667, rho=0.081581\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1979/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7800/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 66028 documents into a model of 1186028 documents\n",
      "INFO : topic #0 (0.067): 0.028*\"home\" + 0.022*\"home_dcor\" + 0.017*\"1\" + 0.011*\"2\" + 0.009*\"bath_body\" + 0.009*\"accent\" + 0.008*\"new\" + 0.008*\"3\" + 0.008*\"glass\" + 0.008*\"4\"\n",
      "INFO : topic #11 (0.067): 0.043*\"short\" + 0.033*\"necklace\" + 0.032*\"size\" + 0.030*\"woman_athletic\" + 0.017*\"woman_jewelry\" + 0.016*\"sport_bra\" + 0.016*\"apparel_short\" + 0.014*\"apparel\" + 0.014*\"pair\" + 0.013*\"sock\"\n",
      "INFO : topic #10 (0.067): 0.032*\"woman\" + 0.031*\"bag\" + 0.021*\"item\" + 0.018*\"rm\" + 0.018*\"woman_handbag\" + 0.013*\"purse\" + 0.011*\"shipping\" + 0.008*\"wallet\" + 0.008*\"pocket\" + 0.008*\"new\"\n",
      "INFO : topic #4 (0.067): 0.021*\"ring\" + 0.021*\"woman_jewelry\" + 0.020*\"bracelet\" + 0.018*\"rm\" + 0.014*\"gold\" + 0.013*\"size\" + 0.011*\"2\" + 0.010*\"brand_new\" + 0.010*\"jewelry\" + 0.010*\"new\"\n",
      "INFO : topic #6 (0.067): 0.051*\"game\" + 0.031*\"electronic_video\" + 0.029*\"game_console\" + 0.022*\"woman\" + 0.010*\"skinny\" + 0.010*\"2\" + 0.008*\"size\" + 0.008*\"jean_slim\" + 0.008*\"brand_new\" + 0.008*\"accessory_sunglass\"\n",
      "INFO : topic diff=0.045073, rho=0.081581\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.316 per-word bound, 159.4 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 28\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7829/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 28\n",
      "DEBUG : 7822/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 28\n",
      "DEBUG : 7786/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 28\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7757/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "INFO : topic #1 (0.067): 0.055*\"woman\" + 0.039*\"size\" + 0.037*\"dress\" + 0.023*\"jean\" + 0.017*\"pant\" + 0.014*\"dress_knee\" + 0.013*\"length\" + 0.012*\"black\" + 0.011*\"small\" + 0.011*\"mini\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #11 (0.067): 0.044*\"short\" + 0.034*\"necklace\" + 0.032*\"size\" + 0.031*\"woman_athletic\" + 0.017*\"woman_jewelry\" + 0.017*\"sport_bra\" + 0.016*\"apparel_short\" + 0.014*\"apparel\" + 0.014*\"pair\" + 0.013*\"sock\"\n",
      "INFO : topic #7 (0.067): 0.047*\"woman\" + 0.046*\"size\" + 0.023*\"man\" + 0.021*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.013*\"sweater\" + 0.011*\"new\" + 0.011*\"small\" + 0.010*\"nike\"\n",
      "INFO : topic #3 (0.067): 0.015*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"charger\" + 0.007*\"iphone\" + 0.006*\"color\" + 0.006*\"free_shipping\" + 0.006*\"3\"\n",
      "INFO : topic #2 (0.067): 0.013*\"hair\" + 0.010*\"home_kitchen\" + 0.010*\"color\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"kid_toy\" + 0.008*\"black\" + 0.008*\"rae\" + 0.008*\"2\" + 0.007*\"slime\"\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : 7797/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.039749, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 28\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.067): 0.055*\"woman\" + 0.039*\"size\" + 0.037*\"dress\" + 0.023*\"jean\" + 0.017*\"pant\" + 0.014*\"dress_knee\" + 0.013*\"length\" + 0.012*\"black\" + 0.011*\"mini\" + 0.011*\"small\"\n",
      "INFO : topic #8 (0.067): 0.042*\"legging\" + 0.037*\"case\" + 0.032*\"tight_legging\" + 0.032*\"apparel_pant\" + 0.032*\"lularoe\" + 0.030*\"woman_athletic\" + 0.019*\"phone_accessory\" + 0.019*\"electronic_cell\" + 0.015*\"brand_new\" + 0.014*\"new\"\n",
      "INFO : topic #14 (0.067): 0.050*\"beauty_makeup\" + 0.022*\"lip\" + 0.018*\"face\" + 0.017*\"eye\" + 0.017*\"color\" + 0.017*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "INFO : topic #7 (0.067): 0.047*\"woman\" + 0.046*\"size\" + 0.023*\"man\" + 0.021*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.013*\"sweater\" + 0.011*\"new\" + 0.011*\"small\" + 0.010*\"nike\"\n",
      "INFO : topic #9 (0.067): 0.055*\"woman_top\" + 0.048*\"shirt\" + 0.032*\"blouse\" + 0.024*\"blouse_t\" + 0.019*\"size\" + 0.016*\"kid_toy\" + 0.012*\"tank\" + 0.011*\"tee\" + 0.011*\"small\" + 0.010*\"medium\"\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.041404, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7783/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7820/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.067): 0.048*\"woman\" + 0.046*\"size\" + 0.024*\"man\" + 0.022*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.013*\"sweater\" + 0.011*\"new\" + 0.010*\"small\" + 0.010*\"nike\"\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.067): 0.013*\"hair\" + 0.011*\"home_kitchen\" + 0.009*\"color\" + 0.009*\"kid_toy\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.008*\"rae\" + 0.008*\"2\" + 0.008*\"black\" + 0.007*\"slime\"\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.067): 0.050*\"beauty_makeup\" + 0.022*\"lip\" + 0.018*\"face\" + 0.017*\"eye\" + 0.017*\"color\" + 0.017*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "INFO : topic #3 (0.067): 0.015*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.009*\"1\" + 0.008*\"charger\" + 0.008*\"box\" + 0.008*\"iphone\" + 0.006*\"color\" + 0.006*\"3\" + 0.006*\"accessory\"\n",
      "INFO : topic #12 (0.067): 0.027*\"kid_girl\" + 0.022*\"baby\" + 0.020*\"0_24\" + 0.020*\"size\" + 0.020*\"girl\" + 0.019*\"kid_boy\" + 0.018*\"3\" + 0.017*\"accessory\" + 0.015*\"mo\" + 0.013*\"2\"\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.043468, rho=0.081311\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 20\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 22\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7822/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #11 (0.067): 0.046*\"short\" + 0.035*\"necklace\" + 0.033*\"woman_athletic\" + 0.032*\"size\" + 0.017*\"sport_bra\" + 0.017*\"apparel_short\" + 0.017*\"woman_jewelry\" + 0.015*\"apparel\" + 0.015*\"pair\" + 0.014*\"sock\"\n",
      "INFO : topic #10 (0.067): 0.033*\"woman\" + 0.032*\"bag\" + 0.022*\"item\" + 0.018*\"woman_handbag\" + 0.018*\"rm\" + 0.013*\"purse\" + 0.012*\"shipping\" + 0.009*\"wallet\" + 0.008*\"pocket\" + 0.008*\"new\"\n",
      "INFO : topic #3 (0.067): 0.015*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.009*\"1\" + 0.008*\"charger\" + 0.008*\"iphone\" + 0.008*\"box\" + 0.006*\"color\" + 0.006*\"3\" + 0.006*\"accessory\"\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #7 (0.067): 0.048*\"woman\" + 0.047*\"size\" + 0.024*\"man\" + 0.022*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.013*\"sweater\" + 0.011*\"new\" + 0.010*\"small\" + 0.010*\"nike\"\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.067): 0.022*\"woman_jewelry\" + 0.022*\"ring\" + 0.021*\"bracelet\" + 0.019*\"rm\" + 0.015*\"gold\" + 0.013*\"size\" + 0.011*\"2\" + 0.011*\"jewelry\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic diff=0.043184, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : 7820/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.014*\"hair\" + 0.011*\"home_kitchen\" + 0.009*\"color\" + 0.009*\"kid_toy\" + 0.009*\"rae\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.008*\"black\"\n",
      "INFO : topic #6 (0.067): 0.057*\"game\" + 0.034*\"electronic_video\" + 0.032*\"game_console\" + 0.020*\"woman\" + 0.010*\"2\" + 0.009*\"skinny\" + 0.008*\"accessory_sunglass\" + 0.008*\"case\" + 0.008*\"brand_new\" + 0.007*\"sunglass\"\n",
      "INFO : topic #0 (0.067): 0.029*\"home\" + 0.023*\"home_dcor\" + 0.017*\"1\" + 0.011*\"2\" + 0.009*\"bath_body\" + 0.009*\"accent\" + 0.009*\"new\" + 0.008*\"glass\" + 0.008*\"3\" + 0.008*\"4\"\n",
      "INFO : topic #8 (0.067): 0.044*\"legging\" + 0.039*\"case\" + 0.033*\"lularoe\" + 0.033*\"tight_legging\" + 0.033*\"apparel_pant\" + 0.031*\"woman_athletic\" + 0.020*\"phone_accessory\" + 0.020*\"electronic_cell\" + 0.015*\"brand_new\" + 0.014*\"case_skin\"\n",
      "INFO : topic #4 (0.067): 0.022*\"woman_jewelry\" + 0.022*\"ring\" + 0.021*\"bracelet\" + 0.019*\"rm\" + 0.015*\"gold\" + 0.013*\"size\" + 0.011*\"2\" + 0.011*\"jewelry\" + 0.010*\"earring\" + 0.010*\"brand_new\"\n",
      "INFO : topic diff=0.042625, rho=0.081311\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7814/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 29\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.014*\"hair\" + 0.012*\"home_kitchen\" + 0.009*\"color\" + 0.009*\"kid_toy\" + 0.009*\"rae\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.007*\"black\"\n",
      "INFO : topic #11 (0.067): 0.048*\"short\" + 0.035*\"necklace\" + 0.034*\"woman_athletic\" + 0.032*\"size\" + 0.018*\"sport_bra\" + 0.018*\"apparel_short\" + 0.017*\"woman_jewelry\" + 0.016*\"apparel\" + 0.016*\"pair\" + 0.015*\"sock\"\n",
      "INFO : topic #3 (0.067): 0.014*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.009*\"1\" + 0.008*\"charger\" + 0.008*\"iphone\" + 0.008*\"box\" + 0.006*\"3\" + 0.006*\"color\" + 0.006*\"accessory\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.067): 0.043*\"bra\" + 0.026*\"size\" + 0.021*\"woman_underwear\" + 0.019*\"vintage_collectible\" + 0.014*\"book\" + 0.009*\"black\" + 0.009*\"victoria_secret\" + 0.008*\"lace\" + 0.008*\"small\" + 0.008*\"figure\"\n",
      "INFO : topic #14 (0.067): 0.050*\"beauty_makeup\" + 0.022*\"lip\" + 0.018*\"face\" + 0.018*\"eye\" + 0.017*\"color\" + 0.017*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "INFO : topic diff=0.042812, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 26\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 29\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 29\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 112000 documents into a model of 1186028 documents\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.048*\"woman\" + 0.047*\"size\" + 0.026*\"man\" + 0.023*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.013*\"sweater\" + 0.011*\"new\" + 0.010*\"small\" + 0.010*\"nike\"\n",
      "INFO : topic #9 (0.067): 0.056*\"woman_top\" + 0.051*\"shirt\" + 0.033*\"blouse\" + 0.024*\"blouse_t\" + 0.020*\"size\" + 0.017*\"kid_toy\" + 0.013*\"tank\" + 0.012*\"tee\" + 0.012*\"small\" + 0.011*\"medium\"\n",
      "INFO : topic #11 (0.067): 0.048*\"short\" + 0.036*\"necklace\" + 0.035*\"woman_athletic\" + 0.032*\"size\" + 0.018*\"sport_bra\" + 0.018*\"apparel_short\" + 0.017*\"woman_jewelry\" + 0.016*\"apparel\" + 0.016*\"pair\" + 0.015*\"sock\"\n",
      "INFO : topic #13 (0.067): 0.070*\"pink\" + 0.038*\"woman\" + 0.036*\"victoria_secret\" + 0.021*\"man\" + 0.019*\"size\" + 0.018*\"new\" + 0.012*\"beauty_fragrance\" + 0.011*\"brand_new\" + 0.009*\"small\" + 0.009*\"perfume\"\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.067): 0.023*\"woman_jewelry\" + 0.022*\"ring\" + 0.021*\"bracelet\" + 0.019*\"rm\" + 0.015*\"gold\" + 0.013*\"size\" + 0.011*\"jewelry\" + 0.011*\"2\" + 0.011*\"earring\" + 0.010*\"silver\"\n",
      "INFO : topic diff=0.043018, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 26\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 28\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #9 (0.067): 0.056*\"woman_top\" + 0.051*\"shirt\" + 0.033*\"blouse\" + 0.024*\"blouse_t\" + 0.020*\"size\" + 0.017*\"kid_toy\" + 0.013*\"tank\" + 0.012*\"tee\" + 0.012*\"small\" + 0.011*\"medium\"\n",
      "INFO : topic #7 (0.067): 0.048*\"woman\" + 0.047*\"size\" + 0.026*\"man\" + 0.023*\"shoe\" + 0.018*\"shirt\" + 0.016*\"black\" + 0.013*\"sweater\" + 0.011*\"new\" + 0.010*\"small\" + 0.010*\"nike\"\n",
      "INFO : topic #0 (0.067): 0.029*\"home\" + 0.023*\"home_dcor\" + 0.017*\"1\" + 0.011*\"2\" + 0.010*\"bath_body\" + 0.009*\"accent\" + 0.009*\"new\" + 0.008*\"3\" + 0.008*\"glass\" + 0.008*\"brand_new\"\n",
      "INFO : topic #3 (0.067): 0.014*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.009*\"1\" + 0.009*\"charger\" + 0.008*\"iphone\" + 0.008*\"box\" + 0.006*\"3\" + 0.006*\"accessory\" + 0.006*\"color\"\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #2 (0.067): 0.014*\"hair\" + 0.012*\"home_kitchen\" + 0.009*\"rae\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.007*\"black\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.041776, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 27\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 28\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 28DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 28\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7844/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.014*\"hair\" + 0.012*\"home_kitchen\" + 0.009*\"rae\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.007*\"nail\"\n",
      "INFO : topic #12 (0.067): 0.029*\"kid_girl\" + 0.023*\"baby\" + 0.022*\"girl\" + 0.022*\"size\" + 0.021*\"0_24\" + 0.020*\"kid_boy\" + 0.019*\"3\" + 0.017*\"accessory\" + 0.015*\"mo\" + 0.014*\"2\"\n",
      "INFO : topic #6 (0.067): 0.061*\"game\" + 0.036*\"electronic_video\" + 0.034*\"game_console\" + 0.018*\"woman\" + 0.011*\"2\" + 0.009*\"accessory_sunglass\" + 0.008*\"case\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"skinny\"\n",
      "INFO : topic #0 (0.067): 0.030*\"home\" + 0.023*\"home_dcor\" + 0.016*\"1\" + 0.011*\"2\" + 0.010*\"bath_body\" + 0.009*\"accent\" + 0.009*\"new\" + 0.008*\"3\" + 0.008*\"glass\" + 0.008*\"brand_new\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #11 (0.067): 0.050*\"short\" + 0.036*\"necklace\" + 0.036*\"woman_athletic\" + 0.032*\"size\" + 0.018*\"sport_bra\" + 0.018*\"apparel_short\" + 0.017*\"apparel\" + 0.017*\"woman_jewelry\" + 0.017*\"pair\" + 0.015*\"sock\"\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "INFO : topic diff=0.041707, rho=0.081311\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 28\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 28\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #4 (0.067): 0.023*\"woman_jewelry\" + 0.022*\"ring\" + 0.022*\"bracelet\" + 0.019*\"rm\" + 0.016*\"gold\" + 0.013*\"size\" + 0.011*\"jewelry\" + 0.011*\"earring\" + 0.011*\"2\" + 0.011*\"silver\"\n",
      "INFO : topic #11 (0.067): 0.050*\"short\" + 0.036*\"woman_athletic\" + 0.036*\"necklace\" + 0.032*\"size\" + 0.019*\"sport_bra\" + 0.018*\"apparel_short\" + 0.018*\"apparel\" + 0.017*\"woman_jewelry\" + 0.017*\"pair\" + 0.015*\"sock\"\n",
      "INFO : topic #14 (0.067): 0.050*\"beauty_makeup\" + 0.022*\"lip\" + 0.019*\"face\" + 0.018*\"eye\" + 0.018*\"color\" + 0.017*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.067): 0.046*\"legging\" + 0.041*\"case\" + 0.035*\"lularoe\" + 0.035*\"tight_legging\" + 0.035*\"apparel_pant\" + 0.032*\"woman_athletic\" + 0.021*\"phone_accessory\" + 0.021*\"electronic_cell\" + 0.015*\"brand_new\" + 0.014*\"case_skin\"\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.067): 0.014*\"hair\" + 0.012*\"home_kitchen\" + 0.010*\"rae\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.007*\"nail\"\n",
      "INFO : topic diff=0.041849, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 27\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7863/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #12 (0.067): 0.029*\"kid_girl\" + 0.024*\"baby\" + 0.023*\"girl\" + 0.022*\"size\" + 0.021*\"0_24\" + 0.020*\"kid_boy\" + 0.019*\"3\" + 0.017*\"accessory\" + 0.015*\"mo\" + 0.014*\"2\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #9 (0.067): 0.057*\"woman_top\" + 0.053*\"shirt\" + 0.033*\"blouse\" + 0.025*\"blouse_t\" + 0.021*\"size\" + 0.017*\"kid_toy\" + 0.014*\"tank\" + 0.012*\"tee\" + 0.012*\"small\" + 0.011*\"medium\"\n",
      "INFO : topic #2 (0.067): 0.014*\"hair\" + 0.012*\"home_kitchen\" + 0.010*\"rae\" + 0.009*\"kid_toy\" + 0.009*\"color\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.007*\"nail\"\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.067): 0.044*\"bra\" + 0.025*\"size\" + 0.021*\"woman_underwear\" + 0.019*\"vintage_collectible\" + 0.015*\"book\" + 0.009*\"black\" + 0.009*\"victoria_secret\" + 0.009*\"lace\" + 0.008*\"figure\" + 0.008*\"small\"\n",
      "INFO : topic #1 (0.067): 0.057*\"woman\" + 0.040*\"size\" + 0.038*\"dress\" + 0.025*\"jean\" + 0.017*\"pant\" + 0.014*\"length\" + 0.014*\"dress_knee\" + 0.012*\"black\" + 0.011*\"mini\" + 0.011*\"small\"\n",
      "INFO : topic diff=0.040596, rho=0.081311\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7852/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7824/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.029*\"kid_girl\" + 0.024*\"baby\" + 0.023*\"girl\" + 0.022*\"size\" + 0.021*\"0_24\" + 0.021*\"kid_boy\" + 0.019*\"3\" + 0.017*\"accessory\" + 0.015*\"mo\" + 0.014*\"2\"\n",
      "INFO : topic #6 (0.067): 0.063*\"game\" + 0.038*\"electronic_video\" + 0.036*\"game_console\" + 0.017*\"woman\" + 0.011*\"2\" + 0.009*\"accessory_sunglass\" + 0.009*\"case\" + 0.008*\"new\" + 0.008*\"sunglass\" + 0.008*\"brand_new\"\n",
      "INFO : topic #13 (0.067): 0.074*\"pink\" + 0.040*\"woman\" + 0.038*\"victoria_secret\" + 0.021*\"man\" + 0.019*\"size\" + 0.019*\"new\" + 0.013*\"beauty_fragrance\" + 0.012*\"brand_new\" + 0.009*\"perfume\" + 0.009*\"sweat\"\n",
      "INFO : topic #9 (0.067): 0.057*\"woman_top\" + 0.053*\"shirt\" + 0.033*\"blouse\" + 0.025*\"blouse_t\" + 0.021*\"size\" + 0.017*\"kid_toy\" + 0.014*\"tank\" + 0.012*\"tee\" + 0.012*\"small\" + 0.011*\"medium\"\n",
      "INFO : topic #3 (0.067): 0.014*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.009*\"charger\" + 0.009*\"1\" + 0.008*\"iphone\" + 0.007*\"box\" + 0.007*\"phone_accessory\" + 0.007*\"electronic_cell\" + 0.007*\"3\"\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.040316, rho=0.081311\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7849/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2000/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.067): 0.074*\"pink\" + 0.041*\"woman\" + 0.038*\"victoria_secret\" + 0.020*\"man\" + 0.019*\"size\" + 0.019*\"new\" + 0.013*\"beauty_fragrance\" + 0.012*\"brand_new\" + 0.009*\"perfume\" + 0.009*\"sweat\"\n",
      "INFO : topic #10 (0.067): 0.034*\"woman\" + 0.034*\"bag\" + 0.023*\"item\" + 0.019*\"woman_handbag\" + 0.019*\"rm\" + 0.014*\"purse\" + 0.013*\"shipping\" + 0.009*\"pocket\" + 0.009*\"wallet\" + 0.008*\"leather\"\n",
      "INFO : topic #12 (0.067): 0.030*\"kid_girl\" + 0.024*\"baby\" + 0.023*\"girl\" + 0.022*\"size\" + 0.021*\"0_24\" + 0.021*\"kid_boy\" + 0.019*\"3\" + 0.017*\"accessory\" + 0.015*\"mo\" + 0.014*\"2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.067): 0.015*\"hair\" + 0.013*\"home_kitchen\" + 0.010*\"rae\" + 0.010*\"kid_toy\" + 0.009*\"color\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"2\" + 0.008*\"slime\" + 0.007*\"nail\"\n",
      "INFO : topic #0 (0.067): 0.030*\"home\" + 0.023*\"home_dcor\" + 0.016*\"1\" + 0.011*\"2\" + 0.010*\"bath_body\" + 0.010*\"new\" + 0.009*\"accent\" + 0.008*\"3\" + 0.008*\"glass\" + 0.008*\"brand_new\"\n",
      "INFO : topic diff=0.038452, rho=0.081311\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 8000 documents into a model of 1186028 documents\n",
      "INFO : topic #2 (0.067): 0.015*\"hair\" + 0.013*\"home_kitchen\" + 0.010*\"rae\" + 0.010*\"kid_toy\" + 0.009*\"color\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.009*\"slime\" + 0.008*\"2\" + 0.007*\"nail\"\n",
      "INFO : topic #10 (0.067): 0.034*\"bag\" + 0.034*\"woman\" + 0.023*\"item\" + 0.019*\"woman_handbag\" + 0.019*\"rm\" + 0.014*\"purse\" + 0.013*\"shipping\" + 0.009*\"wallet\" + 0.009*\"pocket\" + 0.008*\"new\"\n",
      "INFO : topic #0 (0.067): 0.030*\"home\" + 0.023*\"home_dcor\" + 0.016*\"1\" + 0.011*\"2\" + 0.010*\"bath_body\" + 0.010*\"new\" + 0.009*\"accent\" + 0.008*\"3\" + 0.008*\"glass\" + 0.008*\"brand_new\"\n",
      "INFO : topic #9 (0.067): 0.057*\"woman_top\" + 0.054*\"shirt\" + 0.033*\"blouse\" + 0.025*\"blouse_t\" + 0.021*\"size\" + 0.016*\"kid_toy\" + 0.014*\"tank\" + 0.013*\"tee\" + 0.012*\"small\" + 0.011*\"medium\"\n",
      "INFO : topic #8 (0.067): 0.047*\"legging\" + 0.042*\"case\" + 0.037*\"lularoe\" + 0.036*\"tight_legging\" + 0.036*\"apparel_pant\" + 0.033*\"woman_athletic\" + 0.021*\"phone_accessory\" + 0.021*\"electronic_cell\" + 0.015*\"brand_new\" + 0.015*\"case_skin\"\n",
      "INFO : topic diff=0.036370, rho=0.081311\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.267 per-word bound, 154.1 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=329852, num_topics=15, decay=0.5, chunksize=8000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 585000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 586000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 587000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 588000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 589000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 590000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 591000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 592000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 593000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 594000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 595000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 596000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 597000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 598000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 599000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 600000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 601000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 602000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 603000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 604000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 605000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 606000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 607000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 608000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 609000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 610000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 611000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 612000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 613000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 614000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 615000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 616000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 617000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 618000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 619000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 620000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 621000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 622000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 623000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 624000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 625000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 626000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 627000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 628000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 629000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 630000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 631000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 632000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 633000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 634000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 635000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 636000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 637000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 638000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 639000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 640000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 641000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 642000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 643000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 644000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 645000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 646000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 647000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 648000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 649000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 650000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 651000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 652000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 653000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 654000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 655000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 656000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 657000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 658000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 659000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 660000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 661000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 662000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 663000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 664000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 665000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 666000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 667000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 668000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 669000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 670000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 671000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 672000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 673000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 674000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 675000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 676000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 677000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 678000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 679000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 680000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 681000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 682000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 683000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 684000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 685000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 686000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 687000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 688000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 689000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 690000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 691000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 692000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 693000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 694000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 695000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 696000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 697000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 698000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 699000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 700000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 701000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 702000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 703000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 704000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 705000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 706000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 707000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 708000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 709000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 710000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 711000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 712000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 713000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 714000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 715000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 716000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 717000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 718000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 719000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 720000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 721000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 722000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 723000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 724000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 725000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 726000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 727000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 728000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 729000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 730000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 731000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 732000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 733000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 734000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 735000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 736000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 737000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 738000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 739000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 740000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 741000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 742000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 743000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 744000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 745000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 746000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 747000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 748000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 749000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 750000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 751000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 752000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 753000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 754000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 755000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 756000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 757000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 758000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 759000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 760000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 761000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 762000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 763000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 764000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 765000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 766000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 767000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 768000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 769000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 770000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 771000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 772000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 773000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 774000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 775000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 776000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 777000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 778000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 779000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 780000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 781000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 782000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 783000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 784000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 785000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 786000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 787000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 788000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 789000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 790000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 791000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 792000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 793000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 794000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 795000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 796000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 797000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 798000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 799000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 800000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 801000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 802000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 803000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 804000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 805000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 806000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 807000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 808000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 809000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 810000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 811000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 812000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 813000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 814000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 815000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 816000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 817000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 818000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 819000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 820000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 821000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 822000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 823000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 824000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 825000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 826000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 827000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 828000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 829000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 830000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 831000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 832000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 833000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 834000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 835000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 836000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 837000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 838000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 839000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 840000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 841000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 842000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 843000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 844000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 845000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 846000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 847000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 848000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 849000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 850000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 851000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 852000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 853000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 854000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 855000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 856000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 857000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 858000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 859000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 860000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 861000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 862000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 863000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 864000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 865000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 866000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 867000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 868000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 869000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 870000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 871000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 872000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 873000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 874000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 875000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 876000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 877000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 878000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 879000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 880000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 881000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 882000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 883000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 884000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 885000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 886000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 887000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 888000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 889000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 890000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 891000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 892000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 893000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 894000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 895000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 896000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 897000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 898000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 899000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 900000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 901000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 902000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 903000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 904000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 905000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 906000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 907000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 908000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 909000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 910000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 911000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 912000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 913000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 914000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 915000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 916000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 917000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 918000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 919000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 920000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 921000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 922000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 923000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 924000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 925000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 926000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 927000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 928000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 929000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 930000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 931000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 932000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 933000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 934000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 935000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 936000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 937000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 938000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 939000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 940000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 941000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 942000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 943000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 944000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 945000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 946000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 947000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 948000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 949000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 950000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 951000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 952000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 953000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 954000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 955000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 956000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 957000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 958000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 959000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 960000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 961000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 962000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 963000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 964000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 965000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 966000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 967000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 968000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 969000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 970000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 971000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 972000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 973000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 974000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 975000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 976000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 977000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 978000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 979000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 980000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 981000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 982000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 983000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 984000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 985000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 986000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 987000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 988000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 989000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 990000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 991000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 992000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 993000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 994000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 995000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 996000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 997000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 998000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 999000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1000000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1001000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1002000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1003000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1004000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1005000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1006000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1007000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1008000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1009000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1010000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1011000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1012000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1013000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1014000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1015000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1016000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1017000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1018000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1019000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1020000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1021000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1022000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1023000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1024000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1025000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1026000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1027000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1028000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1029000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1030000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1031000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1032000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1033000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1034000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1035000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1036000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1037000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1038000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1039000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1040000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1041000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1042000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1043000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1044000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1045000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1046000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1047000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1048000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1049000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1050000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1051000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1052000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1053000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1054000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1055000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1056000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1057000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1058000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1059000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1060000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1061000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1062000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1063000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1064000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1065000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1066000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1067000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1068000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1069000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1070000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1071000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1072000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1073000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1074000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1075000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1076000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1077000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1078000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1079000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1080000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1081000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1082000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1083000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1084000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1085000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1086000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1087000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1088000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1089000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1090000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1091000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1092000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1093000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1094000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1095000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1096000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1097000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1098000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1099000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1115000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1186000 documents\n",
      "DEBUG : performing inference on a chunk of 1186028 documents\n",
      "DEBUG : 1164812/1186028 documents converged within 50 iterations\n",
      "\n",
      "\n",
      " 60%|    | 3/5 [25:09<16:46, 503.03s/it]\u001b[A\u001b[AINFO : using symmetric alpha at 0.05\n",
      "INFO : using symmetric eta at 0.05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 20 topics, 3 passes over the supplied corpus of 1186028 documents, updating every 88000 documents, evaluating every ~880000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 11 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6273/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 31DEBUG : processing chunk #9 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 33\n",
      "DEBUG : 6322/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 33\n",
      "DEBUG : 6215/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 33\n",
      "DEBUG : 6278/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6321/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6259/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6344/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6286/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6252/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6278/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 32\n",
      "DEBUG : 6257/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 6242/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6269/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6303/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #10 (0.050): 0.019*\"woman\" + 0.017*\"item\" + 0.015*\"rm\" + 0.010*\"size\" + 0.010*\"brand_new\" + 0.008*\"new\" + 0.007*\"shipping\" + 0.006*\"woman_athletic\" + 0.006*\"black\" + 0.005*\"color\"\n",
      "INFO : topic #1 (0.050): 0.034*\"woman\" + 0.030*\"size\" + 0.013*\"dress\" + 0.013*\"brand_new\" + 0.011*\"rm\" + 0.011*\"new\" + 0.010*\"black\" + 0.009*\"2\" + 0.009*\"small\" + 0.009*\"pant\"\n",
      "INFO : topic #14 (0.050): 0.018*\"beauty_makeup\" + 0.015*\"woman\" + 0.011*\"brand_new\" + 0.010*\"black\" + 0.008*\"size\" + 0.008*\"item\" + 0.008*\"face\" + 0.008*\"lip\" + 0.007*\"lularoe\" + 0.007*\"bundle\"\n",
      "INFO : topic #3 (0.050): 0.018*\"new\" + 0.012*\"brand_new\" + 0.009*\"woman\" + 0.008*\"1\" + 0.008*\"beauty_makeup\" + 0.007*\"lip\" + 0.007*\"man\" + 0.007*\"color\" + 0.006*\"size\" + 0.006*\"free_shipping\"\n",
      "INFO : topic #15 (0.050): 0.015*\"size\" + 0.014*\"shirt\" + 0.013*\"brand_new\" + 0.013*\"legging\" + 0.012*\"new\" + 0.011*\"woman\" + 0.008*\"black\" + 0.008*\"lularoe\" + 0.007*\"rm\" + 0.007*\"woman_athletic\"\n",
      "INFO : topic diff=20.246132, rho=1.000000\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 33\n",
      "DEBUG : 6250/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6283/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 31\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 33\n",
      "DEBUG : 6319/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 33\n",
      "DEBUG : 6293/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6341/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6330/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6319/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 32\n",
      "DEBUG : 6294/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6246/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6611/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.050): 0.027*\"pink\" + 0.022*\"woman\" + 0.020*\"size\" + 0.014*\"new\" + 0.012*\"man\" + 0.009*\"color\" + 0.009*\"victoria_secret\" + 0.008*\"shirt\" + 0.007*\"white\" + 0.007*\"bundle\"\n",
      "INFO : topic #14 (0.050): 0.018*\"beauty_makeup\" + 0.014*\"woman\" + 0.011*\"brand_new\" + 0.010*\"black\" + 0.009*\"item\" + 0.008*\"size\" + 0.008*\"face\" + 0.008*\"lip\" + 0.007*\"lularoe\" + 0.007*\"bundle\"\n",
      "INFO : topic #11 (0.050): 0.031*\"size\" + 0.016*\"new\" + 0.012*\"woman\" + 0.011*\"2\" + 0.010*\"3\" + 0.008*\"8\" + 0.008*\"short\" + 0.008*\"woman_athletic\" + 0.007*\"white\" + 0.007*\"black\"\n",
      "INFO : topic #18 (0.050): 0.019*\"woman\" + 0.019*\"size\" + 0.014*\"lularoe\" + 0.013*\"woman_athletic\" + 0.010*\"apparel_pant\" + 0.010*\"tight_legging\" + 0.009*\"legging\" + 0.009*\"shirt\" + 0.009*\"home_dcor\" + 0.008*\"new\"\n",
      "INFO : topic #5 (0.050): 0.030*\"size\" + 0.013*\"woman\" + 0.011*\"small\" + 0.010*\"shirt\" + 0.010*\"bra\" + 0.010*\"man\" + 0.009*\"shoe\" + 0.008*\"dress\" + 0.008*\"color\" + 0.007*\"brand_new\"\n",
      "INFO : topic diff=0.421912, rho=0.288675\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "DEBUG : 6267/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6658/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6699/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 30\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6658/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 32\n",
      "DEBUG : 6731/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6656/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 33\n",
      "DEBUG : 6690/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6699/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6664/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6631/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6645/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.050): 0.031*\"size\" + 0.016*\"bra\" + 0.012*\"woman\" + 0.011*\"small\" + 0.010*\"man\" + 0.010*\"shirt\" + 0.009*\"shoe\" + 0.008*\"color\" + 0.008*\"dress\" + 0.007*\"black\"\n",
      "INFO : topic #1 (0.050): 0.038*\"woman\" + 0.031*\"size\" + 0.017*\"dress\" + 0.013*\"brand_new\" + 0.011*\"rm\" + 0.010*\"pant\" + 0.010*\"new\" + 0.010*\"black\" + 0.009*\"small\" + 0.009*\"2\"\n",
      "INFO : topic #16 (0.050): 0.030*\"woman\" + 0.015*\"bag\" + 0.013*\"size\" + 0.011*\"box\" + 0.011*\"rm\" + 0.010*\"woman_handbag\" + 0.010*\"2\" + 0.009*\"black\" + 0.009*\"small\" + 0.009*\"new\"\n",
      "INFO : topic #9 (0.050): 0.020*\"woman_top\" + 0.017*\"shirt\" + 0.016*\"blouse\" + 0.014*\"size\" + 0.010*\"brand_new\" + 0.008*\"color\" + 0.008*\"woman\" + 0.008*\"blouse_t\" + 0.006*\"new\" + 0.006*\"kid_toy\"\n",
      "INFO : topic #11 (0.050): 0.031*\"size\" + 0.016*\"new\" + 0.011*\"woman\" + 0.011*\"2\" + 0.010*\"short\" + 0.010*\"3\" + 0.009*\"woman_athletic\" + 0.009*\"8\" + 0.008*\"necklace\" + 0.008*\"6\"\n",
      "INFO : topic diff=0.309019, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 33\n",
      "DEBUG : 6590/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 32\n",
      "DEBUG : 6576/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 33\n",
      "DEBUG : 6620/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 33\n",
      "DEBUG : 6553/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6539/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6643/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6592/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6601/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6660/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 6595/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6986/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6954/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7027/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #15 (0.050): 0.019*\"legging\" + 0.015*\"size\" + 0.014*\"brand_new\" + 0.013*\"shirt\" + 0.013*\"new\" + 0.012*\"lularoe\" + 0.011*\"woman\" + 0.009*\"jean\" + 0.008*\"black\" + 0.008*\"woman_athletic\"\n",
      "INFO : topic #0 (0.050): 0.024*\"size\" + 0.016*\"1\" + 0.014*\"man\" + 0.010*\"color\" + 0.008*\"black\" + 0.008*\"home\" + 0.008*\"4\" + 0.007*\"brand_new\" + 0.007*\"2\" + 0.007*\"3\"\n",
      "INFO : topic #10 (0.050): 0.020*\"item\" + 0.017*\"woman\" + 0.017*\"rm\" + 0.010*\"brand_new\" + 0.008*\"size\" + 0.008*\"new\" + 0.008*\"shipping\" + 0.006*\"bag\" + 0.006*\"10\" + 0.006*\"black\"\n",
      "INFO : topic #13 (0.050): 0.034*\"pink\" + 0.023*\"woman\" + 0.021*\"size\" + 0.014*\"new\" + 0.014*\"man\" + 0.013*\"victoria_secret\" + 0.009*\"color\" + 0.008*\"shirt\" + 0.007*\"small\" + 0.007*\"white\"\n",
      "INFO : topic #1 (0.050): 0.041*\"woman\" + 0.032*\"size\" + 0.020*\"dress\" + 0.012*\"brand_new\" + 0.012*\"pant\" + 0.011*\"rm\" + 0.010*\"black\" + 0.010*\"new\" + 0.010*\"jean\" + 0.009*\"small\"\n",
      "DEBUG : 6978/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.235337, rho=0.171499\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6958/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7005/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6990/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6956/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6992/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6977/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7025/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #17 (0.050): 0.018*\"woman\" + 0.017*\"size\" + 0.013*\"jacket\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"2\" + 0.009*\"brand_new\" + 0.008*\"woman_top\" + 0.007*\"shirt\" + 0.007*\"black\"\n",
      "INFO : topic #2 (0.050): 0.016*\"woman\" + 0.015*\"black\" + 0.011*\"color\" + 0.011*\"brand_new\" + 0.009*\"size\" + 0.008*\"new\" + 0.007*\"hair\" + 0.007*\"bundle\" + 0.006*\"gold\" + 0.006*\"beauty_makeup\"\n",
      "INFO : topic #16 (0.050): 0.034*\"woman\" + 0.023*\"bag\" + 0.016*\"woman_handbag\" + 0.011*\"box\" + 0.011*\"size\" + 0.009*\"2\" + 0.009*\"rm\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"small\"\n",
      "INFO : topic #5 (0.050): 0.032*\"size\" + 0.027*\"bra\" + 0.012*\"woman_underwear\" + 0.012*\"man\" + 0.010*\"woman\" + 0.010*\"small\" + 0.010*\"shoe\" + 0.009*\"shirt\" + 0.008*\"black\" + 0.007*\"color\"\n",
      "INFO : topic #19 (0.050): 0.027*\"shirt\" + 0.013*\"2\" + 0.013*\"size\" + 0.010*\"new\" + 0.010*\"top\" + 0.009*\"1\" + 0.009*\"4\" + 0.009*\"apparel\" + 0.009*\"bundle\" + 0.009*\"3\"\n",
      "INFO : topic diff=0.209827, rho=0.149071\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 33\n",
      "DEBUG : 7194/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6920/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 32\n",
      "DEBUG : 7145/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7103/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7154/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7188/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7137/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7194/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7173/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 32\n",
      "DEBUG : 7123/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7153/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7278/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #18 (0.050): 0.031*\"lularoe\" + 0.028*\"woman_athletic\" + 0.026*\"tight_legging\" + 0.026*\"apparel_pant\" + 0.026*\"legging\" + 0.020*\"size\" + 0.017*\"woman\" + 0.016*\"home_dcor\" + 0.014*\"home\" + 0.011*\"skirt\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.050): 0.023*\"item\" + 0.019*\"rm\" + 0.017*\"woman\" + 0.009*\"shipping\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"size\" + 0.006*\"10\" + 0.006*\"x\" + 0.006*\"listing\"\n",
      "INFO : topic #8 (0.050): 0.020*\"case\" + 0.017*\"game\" + 0.014*\"new\" + 0.012*\"pink\" + 0.012*\"brand_new\" + 0.011*\"size\" + 0.010*\"electronic_cell\" + 0.010*\"phone_accessory\" + 0.010*\"rm\" + 0.010*\"black\"\n",
      "INFO : topic #9 (0.050): 0.033*\"woman_top\" + 0.025*\"shirt\" + 0.025*\"blouse\" + 0.014*\"size\" + 0.014*\"blouse_t\" + 0.009*\"brand_new\" + 0.008*\"color\" + 0.008*\"kid_toy\" + 0.007*\"electronic\" + 0.007*\"new\"\n",
      "INFO : topic #15 (0.050): 0.021*\"legging\" + 0.014*\"brand_new\" + 0.014*\"size\" + 0.013*\"shirt\" + 0.013*\"lularoe\" + 0.013*\"new\" + 0.011*\"woman\" + 0.010*\"jean\" + 0.009*\"black\" + 0.008*\"woman_athletic\"\n",
      "DEBUG : 7286/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.195975, rho=0.133631\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 32\n",
      "DEBUG : 7323/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7305/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7287/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7281/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 32\n",
      "DEBUG : 7391/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7301/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7315/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 30\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 32\n",
      "DEBUG : 7330/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7321/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7377/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7397/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #4 (0.050): 0.016*\"size\" + 0.014*\"rm\" + 0.014*\"woman_jewelry\" + 0.013*\"brand_new\" + 0.012*\"ring\" + 0.012*\"2\" + 0.012*\"bracelet\" + 0.010*\"new\" + 0.010*\"man\" + 0.009*\"gold\"\n",
      "INFO : topic #17 (0.050): 0.017*\"woman\" + 0.017*\"jacket\" + 0.017*\"size\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"2\" + 0.009*\"brand_new\" + 0.007*\"woman_top\" + 0.007*\"beauty_skin\" + 0.007*\"black\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.050): 0.033*\"size\" + 0.019*\"short\" + 0.015*\"new\" + 0.015*\"woman_athletic\" + 0.014*\"necklace\" + 0.011*\"2\" + 0.010*\"3\" + 0.010*\"8\" + 0.009*\"woman\" + 0.009*\"6\"\n",
      "INFO : topic #12 (0.050): 0.017*\"woman\" + 0.015*\"3\" + 0.015*\"size\" + 0.015*\"accessory\" + 0.014*\"baby\" + 0.014*\"0_24\" + 0.013*\"kid_girl\" + 0.010*\"brush\" + 0.010*\"mo\" + 0.010*\"girl\"\n",
      "INFO : topic #2 (0.050): 0.014*\"black\" + 0.014*\"woman\" + 0.011*\"color\" + 0.011*\"brand_new\" + 0.008*\"hair\" + 0.008*\"new\" + 0.008*\"size\" + 0.006*\"bundle\" + 0.006*\"2\" + 0.006*\"gold\"\n",
      "INFO : topic diff=0.186830, rho=0.122169\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 32\n",
      "DEBUG : 7392/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7404/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7389/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7407/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 29DEBUG : 7375/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 30\n",
      "DEBUG : 7390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 31DEBUG : 7388/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7475/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7483/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7490/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7486/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.050): 0.048*\"woman\" + 0.035*\"size\" + 0.030*\"dress\" + 0.015*\"jean\" + 0.014*\"pant\" + 0.011*\"black\" + 0.011*\"brand_new\" + 0.010*\"dress_knee\" + 0.010*\"rm\" + 0.009*\"small\"\n",
      "INFO : topic #4 (0.050): 0.016*\"size\" + 0.015*\"woman_jewelry\" + 0.014*\"rm\" + 0.013*\"ring\" + 0.013*\"bracelet\" + 0.012*\"brand_new\" + 0.012*\"2\" + 0.010*\"new\" + 0.010*\"earring\" + 0.009*\"man\"\n",
      "INFO : topic #6 (0.050): 0.025*\"woman\" + 0.021*\"game\" + 0.016*\"size\" + 0.014*\"electronic_video\" + 0.014*\"game_console\" + 0.009*\"brand_new\" + 0.008*\"2\" + 0.007*\"skinny\" + 0.007*\"case\" + 0.007*\"man\"\n",
      "INFO : topic #18 (0.050): 0.037*\"lularoe\" + 0.032*\"legging\" + 0.031*\"woman_athletic\" + 0.030*\"tight_legging\" + 0.030*\"apparel_pant\" + 0.020*\"size\" + 0.017*\"woman\" + 0.017*\"home_dcor\" + 0.015*\"home\" + 0.012*\"skirt\"\n",
      "INFO : topic #5 (0.050): 0.037*\"bra\" + 0.033*\"size\" + 0.017*\"woman_underwear\" + 0.011*\"man\" + 0.010*\"small\" + 0.009*\"woman\" + 0.009*\"shoe\" + 0.009*\"black\" + 0.008*\"vintage_collectible\" + 0.007*\"brand_new\"\n",
      "INFO : topic diff=0.176985, rho=0.113228\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7519/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7447/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 26\n",
      "DEBUG : 7466/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7460/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 26\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7500/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 29DEBUG : getting a new job\n",
      "\n",
      "DEBUG : 7461/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7468/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7490/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7554/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7523/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #13 (0.050): 0.047*\"pink\" + 0.025*\"woman\" + 0.022*\"victoria_secret\" + 0.022*\"size\" + 0.018*\"man\" + 0.015*\"new\" + 0.009*\"small\" + 0.009*\"color\" + 0.008*\"shirt\" + 0.007*\"woman_athletic\"\n",
      "INFO : topic #5 (0.050): 0.039*\"bra\" + 0.033*\"size\" + 0.018*\"woman_underwear\" + 0.011*\"man\" + 0.010*\"small\" + 0.009*\"black\" + 0.008*\"woman\" + 0.008*\"vintage_collectible\" + 0.008*\"shoe\" + 0.007*\"brand_new\"\n",
      "INFO : topic #14 (0.050): 0.045*\"beauty_makeup\" + 0.018*\"lip\" + 0.017*\"face\" + 0.016*\"brand_new\" + 0.014*\"color\" + 0.014*\"eye\" + 0.012*\"new\" + 0.009*\"bundle\" + 0.009*\"shade\" + 0.008*\"makeup\"\n",
      "INFO : topic #17 (0.050): 0.021*\"jacket\" + 0.017*\"size\" + 0.016*\"woman\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"2\" + 0.009*\"brand_new\" + 0.008*\"beauty_skin\" + 0.008*\"book\" + 0.008*\"care_face\"\n",
      "INFO : topic #11 (0.050): 0.034*\"size\" + 0.024*\"short\" + 0.017*\"woman_athletic\" + 0.017*\"necklace\" + 0.015*\"new\" + 0.011*\"2\" + 0.010*\"3\" + 0.010*\"apparel_short\" + 0.010*\"sport_bra\" + 0.010*\"8\"\n",
      "INFO : topic diff=0.171592, rho=0.106000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7512/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7512/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #113 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7516/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7543/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7524/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7528/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 31\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7575/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7586/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7557/8000 documents converged within 50 iterations\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7591/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7579/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7565/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #17 (0.050): 0.023*\"jacket\" + 0.017*\"size\" + 0.016*\"woman\" + 0.011*\"small\" + 0.011*\"new\" + 0.009*\"2\" + 0.009*\"book\" + 0.009*\"beauty_skin\" + 0.009*\"brand_new\" + 0.008*\"care_face\"\n",
      "INFO : topic #1 (0.050): 0.050*\"woman\" + 0.036*\"size\" + 0.033*\"dress\" + 0.017*\"jean\" + 0.015*\"pant\" + 0.011*\"dress_knee\" + 0.011*\"black\" + 0.010*\"brand_new\" + 0.010*\"rm\" + 0.010*\"length\"\n",
      "INFO : topic #10 (0.050): 0.028*\"item\" + 0.023*\"rm\" + 0.014*\"woman\" + 0.013*\"shipping\" + 0.009*\"brand_new\" + 0.008*\"new\" + 0.007*\"listing\" + 0.006*\"size\" + 0.006*\"sticker\" + 0.006*\"10\"\n",
      "INFO : topic #7 (0.050): 0.046*\"size\" + 0.045*\"woman\" + 0.019*\"shoe\" + 0.018*\"black\" + 0.015*\"man\" + 0.013*\"new\" + 0.012*\"sweater\" + 0.012*\"shirt\" + 0.009*\"medium\" + 0.009*\"small\"\n",
      "INFO : topic #2 (0.050): 0.013*\"black\" + 0.011*\"woman\" + 0.011*\"color\" + 0.010*\"brand_new\" + 0.010*\"hair\" + 0.008*\"new\" + 0.007*\"home_kitchen\" + 0.006*\"skin\" + 0.006*\"size\" + 0.006*\"2\"\n",
      "INFO : topic diff=0.168210, rho=0.100000\n",
      "DEBUG : 7574/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : bound: at document #0\n",
      "DEBUG : 7590/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7555/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7593/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7587/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7577/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7548/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7615/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7623/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7624/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7612/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7639/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7606/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7638/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7633/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7631/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7627/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7602/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7589/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7588/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7657/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7637/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7615/8000 documents converged within 50 iterations\n",
      "DEBUG : 7601/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7637/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : -7.767 per-word bound, 217.8 perplexity estimate based on a held-out corpus of 8000 documents with 142375 words\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 240000 documents into a model of 1186028 documents\n",
      "INFO : topic #18 (0.050): 0.042*\"lularoe\" + 0.039*\"legging\" + 0.035*\"woman_athletic\" + 0.034*\"tight_legging\" + 0.034*\"apparel_pant\" + 0.020*\"size\" + 0.017*\"home_dcor\" + 0.016*\"home\" + 0.016*\"woman\" + 0.012*\"skirt\"\n",
      "INFO : topic #3 (0.050): 0.017*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"color\" + 0.007*\"2\" + 0.007*\"rm\" + 0.007*\"free_shipping\" + 0.007*\"price\" + 0.006*\"item\"\n",
      "INFO : topic #11 (0.050): 0.034*\"size\" + 0.028*\"short\" + 0.020*\"woman_athletic\" + 0.019*\"necklace\" + 0.014*\"new\" + 0.012*\"apparel_short\" + 0.012*\"sport_bra\" + 0.011*\"2\" + 0.010*\"sock\" + 0.010*\"3\"\n",
      "INFO : topic #14 (0.050): 0.047*\"beauty_makeup\" + 0.019*\"lip\" + 0.017*\"face\" + 0.016*\"brand_new\" + 0.015*\"eye\" + 0.015*\"color\" + 0.012*\"new\" + 0.009*\"makeup\" + 0.009*\"bundle\" + 0.009*\"shade\"\n",
      "INFO : topic #19 (0.050): 0.030*\"shirt\" + 0.016*\"2\" + 0.012*\"size\" + 0.011*\"top\" + 0.011*\"new\" + 0.010*\"bundle\" + 0.010*\"4\" + 0.010*\"1\" + 0.010*\"oz\" + 0.010*\"3\"\n",
      "INFO : topic diff=0.300777, rho=0.094916\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 9\n",
      "DEBUG : 7620/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7626/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7654/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7660/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1930/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7660/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7627/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7648/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 66028 documents into a model of 1186028 documents\n",
      "INFO : topic #17 (0.050): 0.027*\"jacket\" + 0.017*\"size\" + 0.015*\"woman\" + 0.011*\"book\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"beauty_skin\" + 0.009*\"care_face\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic #7 (0.050): 0.047*\"size\" + 0.047*\"woman\" + 0.020*\"shoe\" + 0.018*\"black\" + 0.017*\"man\" + 0.013*\"sweater\" + 0.012*\"new\" + 0.012*\"shirt\" + 0.010*\"nike\" + 0.009*\"small\"\n",
      "INFO : topic #2 (0.050): 0.013*\"black\" + 0.011*\"hair\" + 0.011*\"color\" + 0.010*\"brand_new\" + 0.010*\"woman\" + 0.008*\"new\" + 0.008*\"home_kitchen\" + 0.007*\"skin\" + 0.006*\"2\" + 0.006*\"body\"\n",
      "INFO : topic #4 (0.050): 0.020*\"woman_jewelry\" + 0.017*\"ring\" + 0.016*\"bracelet\" + 0.015*\"size\" + 0.014*\"rm\" + 0.013*\"earring\" + 0.012*\"brand_new\" + 0.012*\"gold\" + 0.011*\"2\" + 0.010*\"new\"\n",
      "INFO : topic #16 (0.050): 0.041*\"woman\" + 0.037*\"bag\" + 0.023*\"woman_handbag\" + 0.014*\"purse\" + 0.010*\"kid_toy\" + 0.009*\"box\" + 0.009*\"new\" + 0.009*\"black\" + 0.009*\"pocket\" + 0.007*\"small\"\n",
      "INFO : topic diff=0.131065, rho=0.084215\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.455 per-word bound, 175.5 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7687/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7713/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7681/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7696/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7687/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 30\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7694/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7668/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 29\n",
      "DEBUG : 7683/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 30DEBUG : 7706/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7724/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7650/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7672/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7678/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.050): 0.031*\"item\" + 0.026*\"rm\" + 0.015*\"shipping\" + 0.012*\"woman\" + 0.009*\"brand_new\" + 0.008*\"listing\" + 0.008*\"new\" + 0.007*\"sticker\" + 0.006*\"free\" + 0.006*\"10\"\n",
      "INFO : topic #17 (0.050): 0.029*\"jacket\" + 0.016*\"size\" + 0.015*\"woman\" + 0.012*\"book\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"beauty_skin\" + 0.010*\"care_face\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic #19 (0.050): 0.030*\"shirt\" + 0.016*\"2\" + 0.012*\"size\" + 0.011*\"new\" + 0.011*\"top\" + 0.011*\"1\" + 0.011*\"bundle\" + 0.011*\"oz\" + 0.011*\"4\" + 0.010*\"3\"\n",
      "INFO : topic #0 (0.050): 0.020*\"1\" + 0.015*\"size\" + 0.013*\"man\" + 0.012*\"home\" + 0.009*\"glass\" + 0.009*\"x\" + 0.009*\"color\" + 0.008*\"2\" + 0.008*\"4\" + 0.007*\"home_dcor\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #16 (0.050): 0.042*\"woman\" + 0.038*\"bag\" + 0.024*\"woman_handbag\" + 0.015*\"purse\" + 0.010*\"kid_toy\" + 0.009*\"box\" + 0.009*\"new\" + 0.009*\"pocket\" + 0.009*\"black\" + 0.007*\"small\"\n",
      "INFO : topic diff=0.041031, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 30\n",
      "DEBUG : 7681/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7698/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7674/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7706/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 28\n",
      "DEBUG : 7691/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7705/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 28\n",
      "DEBUG : 7712/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7732/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7715/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7715/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7720/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #9 (0.050): 0.051*\"woman_top\" + 0.041*\"shirt\" + 0.033*\"blouse\" + 0.022*\"blouse_t\" + 0.018*\"size\" + 0.009*\"kid_toy\" + 0.009*\"electronic\" + 0.009*\"small\" + 0.008*\"medium\" + 0.008*\"tank\"\n",
      "INFO : topic #10 (0.050): 0.032*\"item\" + 0.027*\"rm\" + 0.016*\"shipping\" + 0.012*\"woman\" + 0.009*\"brand_new\" + 0.008*\"listing\" + 0.008*\"new\" + 0.007*\"sticker\" + 0.006*\"free\" + 0.006*\"10\"\n",
      "INFO : topic #15 (0.050): 0.018*\"legging\" + 0.015*\"brand_new\" + 0.013*\"new\" + 0.012*\"size\" + 0.012*\"lularoe\" + 0.011*\"shirt\" + 0.009*\"jean\" + 0.008*\"woman\" + 0.008*\"free_shipping\" + 0.008*\"black\"\n",
      "INFO : topic #17 (0.050): 0.031*\"jacket\" + 0.016*\"size\" + 0.014*\"woman\" + 0.013*\"book\" + 0.011*\"beauty_skin\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"care_face\" + 0.009*\"2\" + 0.009*\"brand_new\"\n",
      "INFO : topic #6 (0.050): 0.033*\"game\" + 0.024*\"woman\" + 0.022*\"electronic_video\" + 0.021*\"game_console\" + 0.013*\"size\" + 0.009*\"brand_new\" + 0.009*\"skinny\" + 0.008*\"2\" + 0.007*\"case\" + 0.007*\"jean_slim\"\n",
      "INFO : topic diff=0.043637, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7702/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7708/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7737/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 28\n",
      "DEBUG : 7727/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 29\n",
      "DEBUG : 7743/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7732/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7748/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7730/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #19 (0.050): 0.029*\"shirt\" + 0.016*\"2\" + 0.011*\"new\" + 0.011*\"top\" + 0.011*\"size\" + 0.011*\"4\" + 0.011*\"bundle\" + 0.011*\"oz\" + 0.011*\"1\" + 0.011*\"3\"\n",
      "INFO : topic #17 (0.050): 0.033*\"jacket\" + 0.016*\"size\" + 0.014*\"woman\" + 0.014*\"book\" + 0.011*\"beauty_skin\" + 0.011*\"care_face\" + 0.011*\"small\" + 0.011*\"new\" + 0.009*\"2\" + 0.009*\"apparel\"\n",
      "INFO : topic #6 (0.050): 0.035*\"game\" + 0.024*\"woman\" + 0.024*\"electronic_video\" + 0.023*\"game_console\" + 0.013*\"size\" + 0.009*\"brand_new\" + 0.009*\"skinny\" + 0.009*\"2\" + 0.007*\"case\" + 0.007*\"man\"\n",
      "INFO : topic #9 (0.050): 0.052*\"woman_top\" + 0.042*\"shirt\" + 0.033*\"blouse\" + 0.022*\"blouse_t\" + 0.018*\"size\" + 0.009*\"kid_toy\" + 0.009*\"electronic\" + 0.009*\"tank\" + 0.009*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #1 (0.050): 0.052*\"woman\" + 0.038*\"size\" + 0.037*\"dress\" + 0.021*\"jean\" + 0.016*\"pant\" + 0.013*\"dress_knee\" + 0.012*\"black\" + 0.011*\"length\" + 0.010*\"mini\" + 0.010*\"small\"\n",
      "INFO : topic diff=0.046970, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7739/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 26\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 27\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : 7778/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 28\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7757/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7767/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7793/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.050): 0.016*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"box\" + 0.007*\"2\" + 0.007*\"color\" + 0.006*\"free_shipping\" + 0.006*\"rm\" + 0.006*\"price\" + 0.006*\"item\"\n",
      "INFO : topic #17 (0.050): 0.034*\"jacket\" + 0.016*\"size\" + 0.014*\"book\" + 0.014*\"woman\" + 0.012*\"beauty_skin\" + 0.011*\"care_face\" + 0.011*\"new\" + 0.011*\"small\" + 0.009*\"2\" + 0.009*\"apparel\"\n",
      "INFO : topic #19 (0.050): 0.029*\"shirt\" + 0.017*\"2\" + 0.012*\"new\" + 0.011*\"top\" + 0.011*\"4\" + 0.011*\"oz\" + 0.011*\"bundle\" + 0.011*\"1\" + 0.011*\"size\" + 0.011*\"3\"\n",
      "INFO : topic #13 (0.050): 0.058*\"pink\" + 0.029*\"woman\" + 0.029*\"victoria_secret\" + 0.022*\"size\" + 0.020*\"man\" + 0.016*\"new\" + 0.009*\"beauty_fragrance\" + 0.009*\"small\" + 0.009*\"brand_new\" + 0.008*\"color\"\n",
      "INFO : topic #4 (0.050): 0.024*\"woman_jewelry\" + 0.019*\"ring\" + 0.019*\"bracelet\" + 0.015*\"earring\" + 0.015*\"size\" + 0.014*\"rm\" + 0.013*\"gold\" + 0.011*\"brand_new\" + 0.011*\"2\" + 0.010*\"new\"\n",
      "DEBUG : 7737/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.047106, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7763/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 29\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : 7797/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7772/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7772/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7786/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7793/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7760/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #13 (0.050): 0.059*\"pink\" + 0.030*\"victoria_secret\" + 0.030*\"woman\" + 0.022*\"size\" + 0.020*\"man\" + 0.016*\"new\" + 0.010*\"beauty_fragrance\" + 0.009*\"small\" + 0.009*\"brand_new\" + 0.008*\"color\"\n",
      "INFO : topic #6 (0.050): 0.040*\"game\" + 0.026*\"electronic_video\" + 0.025*\"game_console\" + 0.023*\"woman\" + 0.012*\"size\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.008*\"skinny\" + 0.007*\"case\" + 0.007*\"accessory_sunglass\"\n",
      "INFO : topic #5 (0.050): 0.050*\"bra\" + 0.034*\"size\" + 0.024*\"woman_underwear\" + 0.011*\"black\" + 0.010*\"vintage_collectible\" + 0.010*\"small\" + 0.009*\"lace\" + 0.009*\"victoria_secret\" + 0.009*\"belt\" + 0.008*\"man\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.050): 0.051*\"beauty_makeup\" + 0.021*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.016*\"color\" + 0.016*\"eye\" + 0.014*\"new\" + 0.011*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "INFO : topic #4 (0.050): 0.025*\"woman_jewelry\" + 0.020*\"ring\" + 0.019*\"bracelet\" + 0.015*\"earring\" + 0.014*\"size\" + 0.014*\"rm\" + 0.014*\"gold\" + 0.011*\"brand_new\" + 0.011*\"2\" + 0.010*\"new\"\n",
      "INFO : topic diff=0.047259, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 27\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7768/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7768/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7753/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 29\n",
      "DEBUG : 7770/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7796/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7768/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.050): 0.051*\"beauty_makeup\" + 0.021*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.017*\"eye\" + 0.017*\"color\" + 0.014*\"new\" + 0.011*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #8 (0.050): 0.046*\"case\" + 0.027*\"phone_accessory\" + 0.027*\"electronic_cell\" + 0.017*\"game\" + 0.016*\"case_skin\" + 0.013*\"new\" + 0.013*\"phone\" + 0.011*\"brand_new\" + 0.011*\"iphone\" + 0.010*\"iphone_6\"\n",
      "INFO : topic #18 (0.050): 0.049*\"lularoe\" + 0.048*\"legging\" + 0.040*\"woman_athletic\" + 0.039*\"tight_legging\" + 0.039*\"apparel_pant\" + 0.021*\"size\" + 0.017*\"home_dcor\" + 0.017*\"home\" + 0.016*\"woman\" + 0.013*\"skirt\"\n",
      "INFO : topic #12 (0.050): 0.023*\"kid_girl\" + 0.021*\"0_24\" + 0.021*\"baby\" + 0.019*\"accessory\" + 0.018*\"size\" + 0.018*\"girl\" + 0.017*\"3\" + 0.016*\"mo\" + 0.014*\"kid_boy\" + 0.012*\"brush\"\n",
      "INFO : topic #15 (0.050): 0.016*\"legging\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.011*\"size\" + 0.010*\"lularoe\" + 0.010*\"kid\" + 0.010*\"shirt\" + 0.008*\"free_shipping\" + 0.008*\"rm\" + 0.008*\"jean\"\n",
      "INFO : topic diff=0.047728, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 28\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7791/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 27\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 28\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7802/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.050): 0.062*\"pink\" + 0.031*\"victoria_secret\" + 0.031*\"woman\" + 0.022*\"size\" + 0.021*\"man\" + 0.017*\"new\" + 0.010*\"beauty_fragrance\" + 0.010*\"small\" + 0.009*\"brand_new\" + 0.008*\"color\"\n",
      "INFO : topic #14 (0.050): 0.051*\"beauty_makeup\" + 0.021*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.017*\"color\" + 0.017*\"eye\" + 0.014*\"new\" + 0.011*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "INFO : topic #10 (0.050): 0.037*\"item\" + 0.031*\"rm\" + 0.019*\"shipping\" + 0.010*\"listing\" + 0.009*\"woman\" + 0.008*\"sticker\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"price\" + 0.007*\"free\"\n",
      "INFO : topic #19 (0.050): 0.028*\"shirt\" + 0.017*\"2\" + 0.012*\"new\" + 0.012*\"oz\" + 0.012*\"1\" + 0.011*\"bundle\" + 0.011*\"top\" + 0.011*\"4\" + 0.011*\"3\" + 0.010*\"size\"\n",
      "INFO : topic #18 (0.050): 0.050*\"lularoe\" + 0.048*\"legging\" + 0.040*\"woman_athletic\" + 0.039*\"tight_legging\" + 0.039*\"apparel_pant\" + 0.021*\"size\" + 0.017*\"home_dcor\" + 0.017*\"home\" + 0.016*\"woman\" + 0.013*\"skirt\"\n",
      "INFO : topic diff=0.047554, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7824/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7794/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 28\n",
      "DEBUG : 7786/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 28\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 28DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #17 (0.050): 0.040*\"jacket\" + 0.017*\"book\" + 0.016*\"size\" + 0.013*\"beauty_skin\" + 0.013*\"woman\" + 0.013*\"care_face\" + 0.011*\"new\" + 0.010*\"small\" + 0.010*\"apparel\" + 0.010*\"vest\"\n",
      "INFO : topic #6 (0.050): 0.047*\"game\" + 0.031*\"electronic_video\" + 0.029*\"game_console\" + 0.022*\"woman\" + 0.010*\"size\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.008*\"skinny\" + 0.007*\"accessory_sunglass\" + 0.007*\"case\"\n",
      "INFO : topic #13 (0.050): 0.064*\"pink\" + 0.032*\"victoria_secret\" + 0.031*\"woman\" + 0.022*\"size\" + 0.021*\"man\" + 0.017*\"new\" + 0.011*\"beauty_fragrance\" + 0.010*\"brand_new\" + 0.010*\"small\" + 0.008*\"large\"\n",
      "INFO : topic #19 (0.050): 0.028*\"shirt\" + 0.017*\"2\" + 0.012*\"new\" + 0.012*\"oz\" + 0.012*\"1\" + 0.012*\"bundle\" + 0.011*\"top\" + 0.011*\"4\" + 0.011*\"3\" + 0.010*\"size\"\n",
      "INFO : topic #11 (0.050): 0.044*\"short\" + 0.036*\"size\" + 0.031*\"woman_athletic\" + 0.021*\"necklace\" + 0.018*\"apparel_short\" + 0.017*\"sport_bra\" + 0.015*\"apparel\" + 0.015*\"sock\" + 0.013*\"new\" + 0.012*\"2\"\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.047679, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 27\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 22\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 29\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7824/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #17 (0.050): 0.042*\"jacket\" + 0.018*\"book\" + 0.016*\"size\" + 0.013*\"beauty_skin\" + 0.013*\"care_face\" + 0.012*\"woman\" + 0.011*\"new\" + 0.010*\"small\" + 0.010*\"apparel\" + 0.010*\"coat_jacket\"\n",
      "INFO : topic #0 (0.050): 0.021*\"1\" + 0.015*\"home\" + 0.012*\"glass\" + 0.011*\"x\" + 0.011*\"home_dcor\" + 0.011*\"size\" + 0.010*\"man\" + 0.009*\"2\" + 0.008*\"4\" + 0.008*\"color\"\n",
      "INFO : topic #3 (0.050): 0.016*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"box\" + 0.008*\"2\" + 0.008*\"color\" + 0.006*\"headphone\" + 0.006*\"accessory\" + 0.006*\"free_shipping\" + 0.006*\"charger\"\n",
      "INFO : topic #9 (0.050): 0.057*\"woman_top\" + 0.050*\"shirt\" + 0.035*\"blouse\" + 0.025*\"blouse_t\" + 0.020*\"size\" + 0.011*\"tank\" + 0.011*\"small\" + 0.010*\"medium\" + 0.010*\"tee\" + 0.010*\"blouse_tank\"\n",
      "INFO : topic #15 (0.050): 0.014*\"brand_new\" + 0.014*\"legging\" + 0.013*\"new\" + 0.012*\"kid\" + 0.010*\"size\" + 0.009*\"lularoe\" + 0.009*\"shirt\" + 0.008*\"free_shipping\" + 0.008*\"rm\" + 0.007*\"cat\"\n",
      "INFO : topic diff=0.047960, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 25\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 28DEBUG : processing chunk #114 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7810/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.050): 0.051*\"woman\" + 0.049*\"size\" + 0.026*\"shoe\" + 0.024*\"man\" + 0.018*\"black\" + 0.014*\"sweater\" + 0.012*\"new\" + 0.011*\"nike\" + 0.010*\"shirt\" + 0.010*\"athletic\"\n",
      "INFO : topic #0 (0.050): 0.021*\"1\" + 0.016*\"home\" + 0.012*\"glass\" + 0.012*\"x\" + 0.011*\"home_dcor\" + 0.010*\"size\" + 0.009*\"man\" + 0.009*\"2\" + 0.008*\"4\" + 0.008*\"color\"\n",
      "INFO : topic #11 (0.050): 0.047*\"short\" + 0.037*\"size\" + 0.033*\"woman_athletic\" + 0.020*\"necklace\" + 0.019*\"apparel_short\" + 0.018*\"sport_bra\" + 0.017*\"apparel\" + 0.016*\"sock\" + 0.013*\"pair\" + 0.013*\"new\"\n",
      "INFO : topic #3 (0.050): 0.015*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"2\" + 0.008*\"box\" + 0.007*\"color\" + 0.007*\"headphone\" + 0.007*\"accessory\" + 0.006*\"charger\" + 0.006*\"free_shipping\"\n",
      "INFO : topic #16 (0.050): 0.047*\"woman\" + 0.043*\"bag\" + 0.026*\"woman_handbag\" + 0.017*\"purse\" + 0.011*\"kid_toy\" + 0.011*\"pocket\" + 0.009*\"new\" + 0.009*\"black\" + 0.009*\"coach\" + 0.008*\"box\"\n",
      "INFO : topic diff=0.049418, rho=0.081581\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 22\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 27\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 27\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7829/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.050): 0.015*\"hair\" + 0.013*\"home_kitchen\" + 0.011*\"skin\" + 0.010*\"black\" + 0.010*\"brand_new\" + 0.010*\"color\" + 0.009*\"rae\" + 0.008*\"new\" + 0.007*\"nail\" + 0.007*\"beauty\"\n",
      "INFO : topic #19 (0.050): 0.027*\"shirt\" + 0.017*\"2\" + 0.012*\"new\" + 0.012*\"oz\" + 0.012*\"1\" + 0.012*\"bundle\" + 0.011*\"top\" + 0.011*\"4\" + 0.011*\"3\" + 0.010*\"home\"\n",
      "INFO : topic #4 (0.050): 0.030*\"woman_jewelry\" + 0.022*\"ring\" + 0.022*\"bracelet\" + 0.017*\"earring\" + 0.016*\"gold\" + 0.016*\"necklace\" + 0.014*\"size\" + 0.013*\"rm\" + 0.011*\"jewelry\" + 0.010*\"brand_new\"\n",
      "INFO : topic #18 (0.050): 0.053*\"lularoe\" + 0.052*\"legging\" + 0.042*\"woman_athletic\" + 0.041*\"tight_legging\" + 0.041*\"apparel_pant\" + 0.021*\"size\" + 0.017*\"home\" + 0.016*\"home_dcor\" + 0.016*\"woman\" + 0.013*\"black\"\n",
      "INFO : topic #9 (0.050): 0.058*\"woman_top\" + 0.053*\"shirt\" + 0.035*\"blouse\" + 0.025*\"blouse_t\" + 0.021*\"size\" + 0.012*\"tank\" + 0.011*\"small\" + 0.011*\"medium\" + 0.011*\"tee\" + 0.010*\"blouse_tank\"\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.057218, rho=0.081581\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #2 (0.050): 0.015*\"hair\" + 0.013*\"home_kitchen\" + 0.011*\"skin\" + 0.010*\"black\" + 0.010*\"brand_new\" + 0.010*\"color\" + 0.009*\"rae\" + 0.008*\"new\" + 0.007*\"nail\" + 0.007*\"beauty\"\n",
      "INFO : topic #14 (0.050): 0.053*\"beauty_makeup\" + 0.022*\"lip\" + 0.019*\"face\" + 0.018*\"eye\" + 0.018*\"color\" + 0.017*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.010*\"palette\" + 0.010*\"shade\"\n",
      "INFO : topic #3 (0.050): 0.015*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"2\" + 0.008*\"box\" + 0.008*\"color\" + 0.007*\"headphone\" + 0.007*\"accessory\" + 0.007*\"charger\" + 0.006*\"free_shipping\"\n",
      "INFO : topic #9 (0.050): 0.059*\"woman_top\" + 0.054*\"shirt\" + 0.036*\"blouse\" + 0.025*\"blouse_t\" + 0.021*\"size\" + 0.013*\"tank\" + 0.011*\"small\" + 0.011*\"medium\" + 0.011*\"tee\" + 0.010*\"blouse_tank\"\n",
      "INFO : topic #6 (0.050): 0.055*\"game\" + 0.035*\"electronic_video\" + 0.034*\"game_console\" + 0.020*\"woman\" + 0.010*\"2\" + 0.008*\"accessory_sunglass\" + 0.008*\"brand_new\" + 0.008*\"size\" + 0.008*\"sunglass\" + 0.007*\"case\"\n",
      "INFO : topic diff=0.058031, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7844/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7849/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1995/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.050): 0.015*\"new\" + 0.011*\"brand_new\" + 0.009*\"1\" + 0.008*\"2\" + 0.008*\"box\" + 0.008*\"color\" + 0.007*\"headphone\" + 0.007*\"charger\" + 0.007*\"accessory\" + 0.006*\"free_shipping\"\n",
      "INFO : topic #10 (0.050): 0.041*\"item\" + 0.036*\"rm\" + 0.023*\"shipping\" + 0.011*\"listing\" + 0.009*\"price\" + 0.009*\"sticker\" + 0.008*\"bundle\" + 0.008*\"free\" + 0.007*\"new\" + 0.007*\"brand_new\"\n",
      "INFO : topic #11 (0.050): 0.052*\"short\" + 0.038*\"size\" + 0.038*\"woman_athletic\" + 0.020*\"apparel_short\" + 0.020*\"sport_bra\" + 0.020*\"apparel\" + 0.018*\"necklace\" + 0.017*\"sock\" + 0.015*\"pair\" + 0.012*\"2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.050): 0.020*\"1\" + 0.017*\"home\" + 0.013*\"home_dcor\" + 0.012*\"x\" + 0.012*\"glass\" + 0.009*\"2\" + 0.009*\"size\" + 0.008*\"man\" + 0.008*\"color\" + 0.008*\"4\"\n",
      "INFO : topic #12 (0.050): 0.028*\"kid_girl\" + 0.024*\"baby\" + 0.023*\"0_24\" + 0.021*\"girl\" + 0.021*\"size\" + 0.019*\"accessory\" + 0.018*\"3\" + 0.017*\"kid_boy\" + 0.017*\"mo\" + 0.012*\"shoe\"\n",
      "INFO : topic diff=0.052448, rho=0.081581\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 26028 documents into a model of 1186028 documents\n",
      "INFO : topic #11 (0.050): 0.053*\"short\" + 0.039*\"woman_athletic\" + 0.038*\"size\" + 0.020*\"apparel\" + 0.020*\"apparel_short\" + 0.020*\"sport_bra\" + 0.017*\"necklace\" + 0.017*\"sock\" + 0.015*\"pair\" + 0.013*\"2\"\n",
      "INFO : topic #9 (0.050): 0.060*\"woman_top\" + 0.057*\"shirt\" + 0.036*\"blouse\" + 0.026*\"blouse_t\" + 0.022*\"size\" + 0.014*\"tank\" + 0.012*\"tee\" + 0.012*\"small\" + 0.011*\"medium\" + 0.011*\"blouse_tank\"\n",
      "INFO : topic #5 (0.050): 0.056*\"bra\" + 0.036*\"size\" + 0.027*\"woman_underwear\" + 0.012*\"black\" + 0.012*\"victoria_secret\" + 0.012*\"lace\" + 0.010*\"vintage_collectible\" + 0.010*\"small\" + 0.010*\"belt\" + 0.008*\"cup\"\n",
      "INFO : topic #7 (0.050): 0.053*\"woman\" + 0.049*\"size\" + 0.027*\"shoe\" + 0.027*\"man\" + 0.017*\"black\" + 0.015*\"sweater\" + 0.012*\"new\" + 0.011*\"nike\" + 0.010*\"athletic\" + 0.010*\"shirt\"\n",
      "INFO : topic #4 (0.050): 0.032*\"woman_jewelry\" + 0.023*\"ring\" + 0.023*\"bracelet\" + 0.019*\"necklace\" + 0.018*\"earring\" + 0.017*\"gold\" + 0.014*\"size\" + 0.012*\"rm\" + 0.011*\"jewelry\" + 0.011*\"silver\"\n",
      "INFO : topic diff=0.042108, rho=0.081581\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.250 per-word bound, 152.2 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 29\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 28\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7856/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #16 (0.050): 0.049*\"woman\" + 0.045*\"bag\" + 0.026*\"woman_handbag\" + 0.018*\"purse\" + 0.011*\"pocket\" + 0.011*\"kid_toy\" + 0.010*\"new\" + 0.009*\"coach\" + 0.009*\"wallet\" + 0.009*\"leather\"\n",
      "INFO : topic #6 (0.050): 0.060*\"game\" + 0.038*\"electronic_video\" + 0.036*\"game_console\" + 0.019*\"woman\" + 0.010*\"2\" + 0.009*\"accessory_sunglass\" + 0.008*\"brand_new\" + 0.008*\"sunglass\" + 0.007*\"case\" + 0.007*\"new\"\n",
      "INFO : topic #4 (0.050): 0.033*\"woman_jewelry\" + 0.023*\"ring\" + 0.023*\"bracelet\" + 0.020*\"necklace\" + 0.018*\"earring\" + 0.017*\"gold\" + 0.014*\"size\" + 0.012*\"rm\" + 0.012*\"jewelry\" + 0.011*\"silver\"\n",
      "INFO : topic #14 (0.050): 0.054*\"beauty_makeup\" + 0.023*\"lip\" + 0.019*\"face\" + 0.018*\"eye\" + 0.018*\"color\" + 0.017*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.010*\"palette\" + 0.010*\"shade\"\n",
      "INFO : topic #10 (0.050): 0.042*\"item\" + 0.037*\"rm\" + 0.024*\"shipping\" + 0.012*\"listing\" + 0.010*\"price\" + 0.009*\"sticker\" + 0.009*\"bundle\" + 0.008*\"free\" + 0.008*\"day\" + 0.008*\"2\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7853/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.040122, rho=0.081311\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 26\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 27\n",
      "DEBUG : 7852/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 28\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7856/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #15 (0.050): 0.018*\"kid\" + 0.013*\"brand_new\" + 0.013*\"new\" + 0.011*\"cat\" + 0.009*\"legging\" + 0.009*\"free_shipping\" + 0.008*\"rm\" + 0.008*\"size\" + 0.007*\"2\" + 0.006*\"car_seat\"\n",
      "INFO : topic #9 (0.050): 0.060*\"woman_top\" + 0.060*\"shirt\" + 0.036*\"blouse\" + 0.026*\"blouse_t\" + 0.022*\"size\" + 0.014*\"tank\" + 0.012*\"tee\" + 0.012*\"small\" + 0.012*\"medium\" + 0.011*\"blouse_tank\"\n",
      "INFO : topic #10 (0.050): 0.043*\"item\" + 0.038*\"rm\" + 0.025*\"shipping\" + 0.012*\"listing\" + 0.010*\"price\" + 0.009*\"sticker\" + 0.009*\"bundle\" + 0.008*\"free\" + 0.008*\"day\" + 0.008*\"2\"\n",
      "INFO : topic #6 (0.050): 0.061*\"game\" + 0.039*\"electronic_video\" + 0.037*\"game_console\" + 0.018*\"woman\" + 0.010*\"2\" + 0.009*\"accessory_sunglass\" + 0.008*\"brand_new\" + 0.008*\"sunglass\" + 0.007*\"case\" + 0.007*\"new\"\n",
      "INFO : topic #14 (0.050): 0.054*\"beauty_makeup\" + 0.023*\"lip\" + 0.019*\"face\" + 0.018*\"eye\" + 0.018*\"color\" + 0.018*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.011*\"palette\" + 0.010*\"shade\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.041651, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 28\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 26\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 27\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 28\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.050): 0.016*\"hair\" + 0.015*\"home_kitchen\" + 0.013*\"skin\" + 0.010*\"rae\" + 0.010*\"brand_new\" + 0.010*\"black\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"nail\" + 0.007*\"beauty\"\n",
      "INFO : topic #15 (0.050): 0.018*\"kid\" + 0.013*\"brand_new\" + 0.013*\"new\" + 0.011*\"cat\" + 0.009*\"legging\" + 0.009*\"free_shipping\" + 0.008*\"rm\" + 0.008*\"size\" + 0.007*\"2\" + 0.006*\"car_seat\"\n",
      "INFO : topic #14 (0.050): 0.054*\"beauty_makeup\" + 0.023*\"lip\" + 0.019*\"face\" + 0.018*\"eye\" + 0.018*\"color\" + 0.018*\"brand_new\" + 0.015*\"new\" + 0.012*\"makeup\" + 0.011*\"palette\" + 0.010*\"shade\"\n",
      "INFO : topic #13 (0.050): 0.073*\"pink\" + 0.037*\"victoria_secret\" + 0.037*\"woman\" + 0.022*\"size\" + 0.021*\"man\" + 0.018*\"new\" + 0.012*\"beauty_fragrance\" + 0.012*\"brand_new\" + 0.010*\"small\" + 0.009*\"perfume\"\n",
      "INFO : topic #7 (0.050): 0.054*\"woman\" + 0.049*\"size\" + 0.028*\"man\" + 0.028*\"shoe\" + 0.017*\"black\" + 0.015*\"sweater\" + 0.012*\"new\" + 0.012*\"nike\" + 0.011*\"athletic\" + 0.010*\"boot\"\n",
      "INFO : topic diff=0.042296, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 27\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 25\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 27\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 28\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 28\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.050): 0.060*\"case\" + 0.037*\"phone_accessory\" + 0.037*\"electronic_cell\" + 0.020*\"case_skin\" + 0.019*\"phone\" + 0.015*\"iphone\" + 0.013*\"cell_phone\" + 0.013*\"iphone_6\" + 0.012*\"new\" + 0.011*\"brand_new\"\n",
      "INFO : topic #15 (0.050): 0.019*\"kid\" + 0.013*\"brand_new\" + 0.013*\"new\" + 0.012*\"cat\" + 0.009*\"free_shipping\" + 0.009*\"rm\" + 0.008*\"legging\" + 0.008*\"size\" + 0.007*\"2\" + 0.007*\"car_seat\"\n",
      "INFO : topic #6 (0.050): 0.064*\"game\" + 0.040*\"electronic_video\" + 0.038*\"game_console\" + 0.017*\"woman\" + 0.011*\"2\" + 0.009*\"accessory_sunglass\" + 0.008*\"sunglass\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"case\"\n",
      "INFO : topic #19 (0.050): 0.021*\"shirt\" + 0.018*\"2\" + 0.014*\"oz\" + 0.013*\"1\" + 0.013*\"new\" + 0.012*\"bundle\" + 0.011*\"home\" + 0.011*\"4\" + 0.011*\"kid_toy\" + 0.011*\"3\"\n",
      "INFO : topic #3 (0.050): 0.014*\"new\" + 0.011*\"brand_new\" + 0.009*\"2\" + 0.009*\"1\" + 0.008*\"box\" + 0.008*\"charger\" + 0.008*\"headphone\" + 0.008*\"color\" + 0.008*\"accessory\" + 0.006*\"iphone\"\n",
      "INFO : topic diff=0.042308, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 25\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 26\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 27DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 27\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #0 (0.050): 0.021*\"home\" + 0.019*\"1\" + 0.016*\"home_dcor\" + 0.014*\"x\" + 0.013*\"glass\" + 0.009*\"2\" + 0.008*\"4\" + 0.007*\"color\" + 0.007*\"size\" + 0.007*\"3\"\n",
      "INFO : topic #1 (0.050): 0.058*\"woman\" + 0.044*\"dress\" + 0.041*\"size\" + 0.026*\"jean\" + 0.017*\"pant\" + 0.016*\"dress_knee\" + 0.014*\"length\" + 0.013*\"mini\" + 0.012*\"black\" + 0.010*\"small\"\n",
      "INFO : topic #6 (0.050): 0.065*\"game\" + 0.041*\"electronic_video\" + 0.039*\"game_console\" + 0.017*\"woman\" + 0.011*\"2\" + 0.010*\"accessory_sunglass\" + 0.009*\"sunglass\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"case\"\n",
      "INFO : topic #15 (0.050): 0.020*\"kid\" + 0.013*\"brand_new\" + 0.013*\"new\" + 0.012*\"cat\" + 0.009*\"free_shipping\" + 0.009*\"rm\" + 0.008*\"size\" + 0.008*\"legging\" + 0.007*\"car_seat\" + 0.007*\"2\"\n",
      "INFO : topic #8 (0.050): 0.061*\"case\" + 0.038*\"phone_accessory\" + 0.038*\"electronic_cell\" + 0.020*\"case_skin\" + 0.019*\"phone\" + 0.016*\"iphone\" + 0.013*\"cell_phone\" + 0.013*\"iphone_6\" + 0.012*\"new\" + 0.011*\"brand_new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.042442, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 27\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 27DEBUG : processing chunk #66 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 26\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 27\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.050): 0.067*\"game\" + 0.041*\"electronic_video\" + 0.039*\"game_console\" + 0.017*\"woman\" + 0.011*\"2\" + 0.010*\"accessory_sunglass\" + 0.009*\"sunglass\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.008*\"case\"\n",
      "INFO : topic #2 (0.050): 0.017*\"hair\" + 0.015*\"home_kitchen\" + 0.014*\"skin\" + 0.011*\"rae\" + 0.010*\"brand_new\" + 0.009*\"black\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"nail\" + 0.008*\"beauty\"\n",
      "INFO : topic #17 (0.050): 0.054*\"jacket\" + 0.024*\"book\" + 0.016*\"care_face\" + 0.015*\"beauty_skin\" + 0.015*\"coat_jacket\" + 0.015*\"size\" + 0.012*\"vest\" + 0.011*\"new\" + 0.011*\"apparel\" + 0.011*\"dog\"\n",
      "INFO : topic #5 (0.050): 0.060*\"bra\" + 0.037*\"size\" + 0.028*\"woman_underwear\" + 0.013*\"victoria_secret\" + 0.013*\"lace\" + 0.013*\"black\" + 0.010*\"belt\" + 0.010*\"small\" + 0.010*\"vintage_collectible\" + 0.009*\"cup\"\n",
      "INFO : topic #0 (0.050): 0.021*\"home\" + 0.019*\"1\" + 0.017*\"home_dcor\" + 0.014*\"x\" + 0.013*\"glass\" + 0.009*\"2\" + 0.008*\"4\" + 0.007*\"color\" + 0.007*\"3\" + 0.007*\"size\"\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.042307, rho=0.081311\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 26\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 26\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 27\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7903/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #19 (0.050): 0.018*\"shirt\" + 0.017*\"2\" + 0.014*\"oz\" + 0.014*\"1\" + 0.013*\"new\" + 0.012*\"bundle\" + 0.012*\"kid_toy\" + 0.011*\"home\" + 0.011*\"bottle\" + 0.011*\"3\"\n",
      "INFO : topic #17 (0.050): 0.055*\"jacket\" + 0.025*\"book\" + 0.016*\"care_face\" + 0.016*\"beauty_skin\" + 0.015*\"coat_jacket\" + 0.015*\"size\" + 0.012*\"vest\" + 0.011*\"new\" + 0.011*\"dog\" + 0.011*\"apparel\"\n",
      "INFO : topic #1 (0.050): 0.058*\"woman\" + 0.044*\"dress\" + 0.041*\"size\" + 0.026*\"jean\" + 0.017*\"pant\" + 0.016*\"dress_knee\" + 0.015*\"length\" + 0.013*\"mini\" + 0.012*\"black\" + 0.010*\"small\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #18 (0.050): 0.059*\"lularoe\" + 0.059*\"legging\" + 0.045*\"woman_athletic\" + 0.045*\"tight_legging\" + 0.044*\"apparel_pant\" + 0.022*\"size\" + 0.016*\"woman\" + 0.015*\"home\" + 0.015*\"black\" + 0.014*\"brand_new\"\n",
      "INFO : topic #2 (0.050): 0.017*\"hair\" + 0.016*\"home_kitchen\" + 0.014*\"skin\" + 0.011*\"rae\" + 0.010*\"brand_new\" + 0.009*\"black\" + 0.009*\"color\" + 0.008*\"new\" + 0.008*\"nail\" + 0.008*\"beauty\"\n",
      "INFO : topic diff=0.041906, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7903/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 28\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.050): 0.060*\"bra\" + 0.037*\"size\" + 0.029*\"woman_underwear\" + 0.013*\"victoria_secret\" + 0.013*\"lace\" + 0.013*\"black\" + 0.010*\"belt\" + 0.010*\"small\" + 0.010*\"vintage_collectible\" + 0.009*\"cup\"\n",
      "INFO : topic #6 (0.050): 0.069*\"game\" + 0.042*\"electronic_video\" + 0.040*\"game_console\" + 0.016*\"woman\" + 0.011*\"2\" + 0.010*\"accessory_sunglass\" + 0.009*\"sunglass\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.008*\"case\"\n",
      "INFO : topic #18 (0.050): 0.060*\"lularoe\" + 0.059*\"legging\" + 0.045*\"woman_athletic\" + 0.045*\"tight_legging\" + 0.045*\"apparel_pant\" + 0.022*\"size\" + 0.016*\"woman\" + 0.015*\"black\" + 0.015*\"home\" + 0.014*\"brand_new\"\n",
      "INFO : topic #11 (0.050): 0.061*\"short\" + 0.049*\"woman_athletic\" + 0.041*\"size\" + 0.029*\"apparel\" + 0.023*\"sport_bra\" + 0.022*\"apparel_short\" + 0.019*\"sock\" + 0.018*\"pair\" + 0.013*\"2\" + 0.011*\"new\"\n",
      "INFO : topic #1 (0.050): 0.058*\"woman\" + 0.044*\"dress\" + 0.041*\"size\" + 0.026*\"jean\" + 0.017*\"pant\" + 0.016*\"dress_knee\" + 0.015*\"length\" + 0.013*\"mini\" + 0.012*\"black\" + 0.010*\"small\"\n",
      "INFO : topic diff=0.042013, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 27\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 26\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 26\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #17 (0.050): 0.057*\"jacket\" + 0.025*\"book\" + 0.016*\"coat_jacket\" + 0.016*\"care_face\" + 0.016*\"beauty_skin\" + 0.014*\"size\" + 0.012*\"vest\" + 0.012*\"dog\" + 0.011*\"new\" + 0.011*\"apparel\"\n",
      "INFO : topic #4 (0.050): 0.037*\"woman_jewelry\" + 0.027*\"necklace\" + 0.025*\"ring\" + 0.025*\"bracelet\" + 0.020*\"earring\" + 0.019*\"gold\" + 0.014*\"silver\" + 0.013*\"size\" + 0.013*\"jewelry\" + 0.010*\"rm\"\n",
      "INFO : topic #9 (0.050): 0.068*\"shirt\" + 0.061*\"woman_top\" + 0.036*\"blouse\" + 0.026*\"blouse_t\" + 0.024*\"size\" + 0.016*\"tank\" + 0.013*\"small\" + 0.013*\"tee\" + 0.013*\"medium\" + 0.011*\"blouse_tank\"\n",
      "INFO : topic #8 (0.050): 0.064*\"case\" + 0.040*\"phone_accessory\" + 0.039*\"electronic_cell\" + 0.021*\"phone\" + 0.020*\"case_skin\" + 0.017*\"iphone\" + 0.014*\"cell_phone\" + 0.014*\"iphone_6\" + 0.012*\"new\" + 0.011*\"brand_new\"\n",
      "INFO : topic #12 (0.050): 0.032*\"kid_girl\" + 0.025*\"baby\" + 0.025*\"girl\" + 0.024*\"size\" + 0.024*\"0_24\" + 0.021*\"kid_boy\" + 0.019*\"3\" + 0.018*\"accessory\" + 0.017*\"mo\" + 0.014*\"boy\"\n",
      "INFO : topic diff=0.041700, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 27\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7918/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.050): 0.032*\"kid_girl\" + 0.026*\"girl\" + 0.025*\"baby\" + 0.024*\"size\" + 0.024*\"0_24\" + 0.022*\"kid_boy\" + 0.019*\"3\" + 0.018*\"accessory\" + 0.017*\"mo\" + 0.014*\"boy\"\n",
      "INFO : topic #7 (0.050): 0.056*\"woman\" + 0.049*\"size\" + 0.032*\"man\" + 0.030*\"shoe\" + 0.017*\"black\" + 0.016*\"sweater\" + 0.012*\"nike\" + 0.012*\"new\" + 0.011*\"athletic\" + 0.011*\"boot\"\n",
      "INFO : topic #14 (0.050): 0.055*\"beauty_makeup\" + 0.023*\"lip\" + 0.020*\"face\" + 0.019*\"eye\" + 0.019*\"color\" + 0.018*\"brand_new\" + 0.016*\"new\" + 0.013*\"makeup\" + 0.011*\"palette\" + 0.010*\"shade\"\n",
      "INFO : topic #13 (0.050): 0.080*\"pink\" + 0.041*\"woman\" + 0.040*\"victoria_secret\" + 0.023*\"size\" + 0.021*\"man\" + 0.018*\"new\" + 0.013*\"beauty_fragrance\" + 0.013*\"brand_new\" + 0.011*\"small\" + 0.010*\"perfume\"\n",
      "INFO : topic #6 (0.050): 0.070*\"game\" + 0.043*\"electronic_video\" + 0.041*\"game_console\" + 0.015*\"woman\" + 0.011*\"2\" + 0.010*\"accessory_sunglass\" + 0.009*\"sunglass\" + 0.009*\"new\" + 0.008*\"brand_new\" + 0.008*\"case\"\n",
      "INFO : topic diff=0.041213, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #119 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 27\n",
      "DEBUG : 7913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 27\n",
      "DEBUG : 7907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 28\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7919/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 27\n",
      "DEBUG : 7913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7923/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.050): 0.014*\"new\" + 0.011*\"brand_new\" + 0.009*\"2\" + 0.009*\"charger\" + 0.009*\"headphone\" + 0.009*\"accessory\" + 0.009*\"1\" + 0.008*\"box\" + 0.008*\"color\" + 0.007*\"battery\"\n",
      "INFO : topic #18 (0.050): 0.061*\"lularoe\" + 0.061*\"legging\" + 0.046*\"tight_legging\" + 0.046*\"woman_athletic\" + 0.046*\"apparel_pant\" + 0.023*\"size\" + 0.016*\"woman\" + 0.015*\"black\" + 0.014*\"brand_new\" + 0.014*\"home\"\n",
      "INFO : topic #12 (0.050): 0.033*\"kid_girl\" + 0.026*\"girl\" + 0.025*\"baby\" + 0.025*\"size\" + 0.024*\"0_24\" + 0.022*\"kid_boy\" + 0.019*\"3\" + 0.018*\"accessory\" + 0.017*\"mo\" + 0.014*\"boy\"\n",
      "INFO : topic #0 (0.050): 0.025*\"home\" + 0.021*\"home_dcor\" + 0.018*\"1\" + 0.014*\"x\" + 0.012*\"glass\" + 0.009*\"2\" + 0.008*\"inch\" + 0.008*\"4\" + 0.007*\"accent\" + 0.007*\"color\"\n",
      "INFO : topic #4 (0.050): 0.038*\"woman_jewelry\" + 0.028*\"necklace\" + 0.026*\"ring\" + 0.025*\"bracelet\" + 0.020*\"earring\" + 0.020*\"gold\" + 0.014*\"silver\" + 0.013*\"jewelry\" + 0.013*\"size\" + 0.010*\"rm\"\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.041028, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 26\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7904/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7897/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.050): 0.082*\"pink\" + 0.042*\"woman\" + 0.041*\"victoria_secret\" + 0.023*\"size\" + 0.021*\"man\" + 0.018*\"new\" + 0.013*\"beauty_fragrance\" + 0.013*\"brand_new\" + 0.011*\"small\" + 0.010*\"perfume\"\n",
      "INFO : topic #16 (0.050): 0.051*\"woman\" + 0.048*\"bag\" + 0.027*\"woman_handbag\" + 0.019*\"purse\" + 0.012*\"pocket\" + 0.011*\"kid_toy\" + 0.011*\"wallet\" + 0.010*\"coach\" + 0.010*\"leather\" + 0.010*\"new\"\n",
      "INFO : topic #0 (0.050): 0.025*\"home\" + 0.021*\"home_dcor\" + 0.018*\"1\" + 0.015*\"x\" + 0.012*\"glass\" + 0.009*\"2\" + 0.008*\"accent\" + 0.008*\"inch\" + 0.007*\"4\" + 0.007*\"color\"\n",
      "INFO : topic #1 (0.050): 0.059*\"woman\" + 0.044*\"dress\" + 0.041*\"size\" + 0.027*\"jean\" + 0.017*\"pant\" + 0.016*\"dress_knee\" + 0.015*\"length\" + 0.013*\"mini\" + 0.012*\"black\" + 0.010*\"small\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #11 (0.050): 0.064*\"short\" + 0.053*\"woman_athletic\" + 0.042*\"size\" + 0.034*\"apparel\" + 0.023*\"sport_bra\" + 0.023*\"apparel_short\" + 0.020*\"pair\" + 0.019*\"sock\" + 0.013*\"2\" + 0.011*\"4\"\n",
      "INFO : topic diff=0.040766, rho=0.081311\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7919/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7921/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 2007/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.050): 0.038*\"woman_jewelry\" + 0.029*\"necklace\" + 0.026*\"ring\" + 0.025*\"bracelet\" + 0.020*\"gold\" + 0.020*\"earring\" + 0.014*\"silver\" + 0.013*\"jewelry\" + 0.013*\"size\" + 0.010*\"new\"\n",
      "INFO : topic #14 (0.050): 0.056*\"beauty_makeup\" + 0.024*\"lip\" + 0.020*\"face\" + 0.019*\"eye\" + 0.019*\"color\" + 0.018*\"brand_new\" + 0.016*\"new\" + 0.013*\"makeup\" + 0.011*\"palette\" + 0.010*\"shade\"\n",
      "INFO : topic #15 (0.050): 0.027*\"kid\" + 0.016*\"cat\" + 0.013*\"brand_new\" + 0.012*\"new\" + 0.009*\"rm\" + 0.008*\"free_shipping\" + 0.007*\"car_seat\" + 0.007*\"2\" + 0.006*\"tooth\" + 0.006*\"2017\"\n",
      "INFO : topic #5 (0.050): 0.062*\"bra\" + 0.038*\"size\" + 0.030*\"woman_underwear\" + 0.014*\"victoria_secret\" + 0.014*\"lace\" + 0.013*\"black\" + 0.011*\"belt\" + 0.010*\"small\" + 0.009*\"vintage_collectible\" + 0.009*\"sexy\"\n",
      "INFO : topic #17 (0.050): 0.059*\"jacket\" + 0.027*\"book\" + 0.018*\"coat_jacket\" + 0.016*\"care_face\" + 0.016*\"beauty_skin\" + 0.014*\"size\" + 0.013*\"dog\" + 0.013*\"vest\" + 0.011*\"new\" + 0.010*\"apparel\"\n",
      "INFO : topic diff=0.040125, rho=0.081311\n",
      "DEBUG : 7920/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 26028 documents into a model of 1186028 documents\n",
      "INFO : topic #15 (0.050): 0.028*\"kid\" + 0.017*\"cat\" + 0.012*\"brand_new\" + 0.012*\"new\" + 0.008*\"rm\" + 0.008*\"free_shipping\" + 0.007*\"car_seat\" + 0.007*\"2\" + 0.006*\"tooth\" + 0.006*\"cover\"\n",
      "INFO : topic #3 (0.050): 0.013*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\" + 0.010*\"charger\" + 0.009*\"accessory\" + 0.009*\"headphone\" + 0.009*\"1\" + 0.008*\"box\" + 0.008*\"color\" + 0.007*\"watch\"\n",
      "INFO : topic #2 (0.050): 0.018*\"hair\" + 0.016*\"home_kitchen\" + 0.016*\"skin\" + 0.012*\"rae\" + 0.010*\"brand_new\" + 0.009*\"black\" + 0.008*\"nail\" + 0.008*\"new\" + 0.008*\"color\" + 0.008*\"beauty\"\n",
      "INFO : topic #7 (0.050): 0.057*\"woman\" + 0.049*\"size\" + 0.034*\"man\" + 0.031*\"shoe\" + 0.017*\"black\" + 0.017*\"sweater\" + 0.012*\"nike\" + 0.012*\"new\" + 0.012*\"athletic\" + 0.011*\"boot\"\n",
      "INFO : topic #11 (0.050): 0.065*\"short\" + 0.055*\"woman_athletic\" + 0.042*\"size\" + 0.035*\"apparel\" + 0.024*\"sport_bra\" + 0.023*\"apparel_short\" + 0.020*\"pair\" + 0.019*\"sock\" + 0.013*\"2\" + 0.011*\"nike\"\n",
      "INFO : topic diff=0.035605, rho=0.081311\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.169 per-word bound, 143.9 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=329852, num_topics=20, decay=0.5, chunksize=8000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 585000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 586000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 587000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 588000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 589000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 590000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 591000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 592000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 593000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 594000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 595000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 596000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 597000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 598000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 599000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 600000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 601000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 602000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 603000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 604000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 605000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 606000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 607000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 608000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 609000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 610000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 611000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 612000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 613000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 614000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 615000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 616000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 617000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 618000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 619000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 620000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 621000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 622000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 623000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 624000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 625000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 626000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 627000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 628000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 629000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 630000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 631000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 632000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 633000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 634000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 635000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 636000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 637000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 638000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 639000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 640000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 641000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 642000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 643000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 644000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 645000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 646000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 647000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 648000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 649000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 650000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 651000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 652000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 653000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 654000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 655000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 656000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 657000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 658000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 659000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 660000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 661000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 662000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 663000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 664000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 665000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 666000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 667000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 668000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 669000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 670000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 671000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 672000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 673000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 674000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 675000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 676000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 677000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 678000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 679000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 680000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 681000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 682000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 683000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 684000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 685000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 686000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 687000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 688000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 689000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 690000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 691000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 692000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 693000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 694000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 695000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 696000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 697000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 698000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 699000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 700000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 701000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 702000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 703000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 704000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 705000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 706000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 707000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 708000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 709000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 710000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 711000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 712000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 713000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 714000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 715000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 716000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 717000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 718000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 719000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 720000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 721000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 722000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 723000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 724000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 725000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 726000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 727000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 728000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 729000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 730000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 731000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 732000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 733000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 734000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 735000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 736000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 737000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 738000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 739000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 740000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 741000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 742000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 743000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 744000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 745000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 746000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 747000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 748000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 749000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 750000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 751000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 752000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 753000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 754000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 755000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 756000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 757000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 758000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 759000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 760000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 761000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 762000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 763000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 764000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 765000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 766000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 767000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 768000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 769000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 770000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 771000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 772000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 773000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 774000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 775000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 776000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 777000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 778000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 779000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 780000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 781000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 782000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 783000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 784000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 785000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 786000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 787000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 788000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 789000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 790000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 791000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 792000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 793000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 794000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 795000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 796000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 797000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 798000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 799000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 800000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 801000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 802000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 803000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 804000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 805000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 806000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 807000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 808000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 809000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 810000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 811000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 812000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 813000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 814000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 815000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 816000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 817000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 818000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 819000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 820000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 821000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 822000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 823000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 824000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 825000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 826000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 827000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 828000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 829000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 830000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 831000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 832000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 833000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 834000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 835000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 836000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 837000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 838000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 839000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 840000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 841000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 842000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 843000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 844000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 845000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 846000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 847000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 848000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 849000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 850000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 851000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 852000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 853000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 854000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 855000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 856000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 857000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 858000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 859000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 860000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 861000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 862000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 863000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 864000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 865000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 866000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 867000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 868000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 869000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 870000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 871000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 872000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 873000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 874000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 875000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 876000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 877000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 878000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 879000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 880000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 881000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 882000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 883000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 884000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 885000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 886000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 887000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 888000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 889000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 890000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 891000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 892000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 893000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 894000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 895000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 896000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 897000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 898000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 899000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 900000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 901000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 902000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 903000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 904000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 905000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 906000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 907000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 908000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 909000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 910000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 911000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 912000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 913000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 914000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 915000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 916000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 917000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 918000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 919000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 920000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 921000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 922000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 923000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 924000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 925000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 926000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 927000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 928000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 929000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 930000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 931000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 932000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 933000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 934000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 935000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 936000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 937000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 938000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 939000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 940000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 941000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 942000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 943000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 944000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 945000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 946000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 947000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 948000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 949000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 950000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 951000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 952000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 953000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 954000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 955000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 956000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 957000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 958000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 959000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 960000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 961000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 962000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 963000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 964000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 965000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 966000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 967000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 968000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 969000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 970000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 971000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 972000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 973000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 974000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 975000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 976000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 977000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 978000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 979000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 980000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 981000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 982000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 983000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 984000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 985000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 986000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 987000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 988000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 989000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 990000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 991000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 992000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 993000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 994000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 995000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 996000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 997000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 998000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 999000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1000000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1001000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1002000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1003000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1004000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1005000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1006000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1007000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1008000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1009000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1010000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1011000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1012000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1013000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1014000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1015000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1016000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1017000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1018000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1019000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1020000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1021000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1022000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1023000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1024000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1025000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1026000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1027000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1028000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1029000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1030000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1031000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1032000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1033000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1034000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1035000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1036000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1037000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1038000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1039000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1040000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1041000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1042000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1043000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1044000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1045000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1046000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1047000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1048000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1049000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1050000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1051000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1052000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1053000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1054000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1055000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1056000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1057000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1058000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1059000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1060000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1061000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1062000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1063000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1064000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1065000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1066000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1067000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1068000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1069000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1070000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1071000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1072000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1073000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1074000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1075000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1076000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1077000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1078000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1079000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1080000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1081000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1082000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1083000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1084000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1085000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1086000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1087000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1088000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1089000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1090000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1091000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1092000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1093000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1094000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1095000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1096000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1097000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1098000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1099000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1184000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1186000 documents\n",
      "DEBUG : performing inference on a chunk of 1186028 documents\n",
      "DEBUG : 1172772/1186028 documents converged within 50 iterations\n",
      "\n",
      "\n",
      " 80%|  | 4/5 [33:59<08:29, 509.76s/it]\u001b[A\u001b[AINFO : using symmetric alpha at 0.04\n",
      "INFO : using symmetric eta at 0.04\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 25 topics, 3 passes over the supplied corpus of 1186028 documents, updating every 88000 documents, evaluating every ~880000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 11 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 33\n",
      "DEBUG : 6375/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6439/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 33\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6300/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6360/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6356/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6338/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 32\n",
      "DEBUG : 6435/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6381/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6340/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : 6411/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6340/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6354/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 6352/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6338/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #16 (0.040): 0.028*\"woman\" + 0.016*\"size\" + 0.012*\"2\" + 0.012*\"rm\" + 0.011*\"bag\" + 0.010*\"box\" + 0.010*\"black\" + 0.009*\"small\" + 0.009*\"new\" + 0.007*\"shirt\"\n",
      "INFO : topic #14 (0.040): 0.019*\"beauty_makeup\" + 0.014*\"woman\" + 0.012*\"brand_new\" + 0.010*\"black\" + 0.009*\"lip\" + 0.009*\"size\" + 0.009*\"face\" + 0.009*\"item\" + 0.007*\"bundle\" + 0.007*\"color\"\n",
      "INFO : topic #6 (0.040): 0.022*\"woman\" + 0.020*\"size\" + 0.010*\"game\" + 0.010*\"brand_new\" + 0.008*\"case\" + 0.007*\"2\" + 0.007*\"black\" + 0.007*\"man\" + 0.006*\"dress\" + 0.006*\"item\"\n",
      "INFO : topic #3 (0.040): 0.019*\"new\" + 0.012*\"brand_new\" + 0.009*\"woman\" + 0.008*\"beauty_makeup\" + 0.008*\"1\" + 0.007*\"lip\" + 0.007*\"man\" + 0.007*\"size\" + 0.007*\"rm\" + 0.006*\"free_shipping\"\n",
      "INFO : topic #5 (0.040): 0.032*\"size\" + 0.014*\"woman\" + 0.011*\"small\" + 0.011*\"shirt\" + 0.011*\"man\" + 0.008*\"shoe\" + 0.008*\"dress\" + 0.008*\"bra\" + 0.007*\"color\" + 0.007*\"black\"\n",
      "DEBUG : 6350/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6441/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=24.934904, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6369/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6395/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6397/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6381/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 6390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6358/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 6336/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6323/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6758/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6365/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #7 (0.040): 0.037*\"size\" + 0.030*\"woman\" + 0.016*\"new\" + 0.015*\"black\" + 0.013*\"shirt\" + 0.008*\"2\" + 0.008*\"medium\" + 0.007*\"large\" + 0.007*\"shoe\" + 0.007*\"pink\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #10 (0.040): 0.017*\"item\" + 0.017*\"woman\" + 0.017*\"rm\" + 0.011*\"brand_new\" + 0.011*\"size\" + 0.009*\"new\" + 0.007*\"shipping\" + 0.006*\"woman_athletic\" + 0.006*\"black\" + 0.006*\"bag\"\n",
      "INFO : topic #12 (0.040): 0.021*\"woman\" + 0.015*\"size\" + 0.012*\"3\" + 0.010*\"pink\" + 0.008*\"5\" + 0.008*\"2\" + 0.008*\"box\" + 0.008*\"baby\" + 0.008*\"new\" + 0.007*\"brand_new\"\n",
      "INFO : topic #22 (0.040): 0.037*\"woman\" + 0.010*\"pink\" + 0.009*\"size\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"color\" + 0.008*\"rm\" + 0.007*\"accessory\" + 0.007*\"brand_new\" + 0.007*\"2\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.040): 0.017*\"woman\" + 0.017*\"black\" + 0.013*\"size\" + 0.011*\"brand_new\" + 0.009*\"color\" + 0.009*\"new\" + 0.007*\"bundle\" + 0.007*\"beauty_makeup\" + 0.006*\"3\" + 0.006*\"2\"\n",
      "DEBUG : 6803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.445594, rho=0.277350\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6754/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6669/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 31\n",
      "DEBUG : 6755/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6781/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #23 (0.040): 0.018*\"size\" + 0.014*\"woman\" + 0.011*\"color\" + 0.009*\"brand_new\" + 0.009*\"man\" + 0.009*\"ring\" + 0.008*\"black\" + 0.008*\"new\" + 0.007*\"accessory\" + 0.007*\"rm\"\n",
      "INFO : topic #7 (0.040): 0.039*\"size\" + 0.033*\"woman\" + 0.016*\"black\" + 0.016*\"new\" + 0.013*\"shirt\" + 0.009*\"shoe\" + 0.009*\"medium\" + 0.008*\"large\" + 0.008*\"pink\" + 0.007*\"small\"\n",
      "INFO : topic #2 (0.040): 0.016*\"black\" + 0.016*\"woman\" + 0.011*\"size\" + 0.011*\"brand_new\" + 0.010*\"color\" + 0.009*\"new\" + 0.007*\"bundle\" + 0.007*\"beauty_makeup\" + 0.006*\"3\" + 0.006*\"2\"\n",
      "INFO : topic #13 (0.040): 0.029*\"pink\" + 0.021*\"size\" + 0.021*\"woman\" + 0.015*\"new\" + 0.012*\"man\" + 0.010*\"victoria_secret\" + 0.008*\"color\" + 0.008*\"shirt\" + 0.007*\"white\" + 0.007*\"small\"\n",
      "INFO : topic #22 (0.040): 0.040*\"woman\" + 0.010*\"pink\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"size\" + 0.008*\"color\" + 0.008*\"rm\" + 0.008*\"accessory\" + 0.007*\"brand_new\" + 0.007*\"2\"\n",
      "INFO : topic diff=0.329071, rho=0.204124\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6685/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 6729/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6700/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6755/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6752/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6734/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6701/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7063/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7077/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #7 (0.040): 0.042*\"size\" + 0.036*\"woman\" + 0.017*\"black\" + 0.015*\"new\" + 0.013*\"shirt\" + 0.011*\"shoe\" + 0.009*\"medium\" + 0.009*\"sweater\" + 0.009*\"large\" + 0.008*\"small\"\n",
      "INFO : topic #9 (0.040): 0.022*\"woman_top\" + 0.020*\"blouse\" + 0.018*\"shirt\" + 0.014*\"size\" + 0.011*\"brand_new\" + 0.008*\"kid_toy\" + 0.008*\"blouse_t\" + 0.008*\"case\" + 0.007*\"color\" + 0.007*\"new\"\n",
      "INFO : topic #3 (0.040): 0.020*\"new\" + 0.012*\"brand_new\" + 0.008*\"beauty_makeup\" + 0.008*\"1\" + 0.008*\"lip\" + 0.007*\"woman\" + 0.007*\"bracelet\" + 0.007*\"rm\" + 0.007*\"box\" + 0.007*\"free_shipping\"\n",
      "INFO : topic #22 (0.040): 0.042*\"woman\" + 0.010*\"pink\" + 0.009*\"accessory\" + 0.009*\"new\" + 0.009*\"black\" + 0.008*\"color\" + 0.008*\"rm\" + 0.008*\"size\" + 0.008*\"wallet\" + 0.007*\"brand_new\"\n",
      "INFO : topic #20 (0.040): 0.020*\"size\" + 0.011*\"shirt\" + 0.011*\"1\" + 0.010*\"rm\" + 0.009*\"brand_new\" + 0.009*\"woman_athletic\" + 0.008*\"brush\" + 0.008*\"small\" + 0.008*\"woman\" + 0.007*\"color\"\n",
      "INFO : topic diff=0.250540, rho=0.169031\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7100/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7084/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 31\n",
      "DEBUG : 7044/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 32\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7089/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7123/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : 7062/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7077/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 33\n",
      "DEBUG : 7117/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7042/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7206/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7224/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7251/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.040): 0.025*\"size\" + 0.016*\"1\" + 0.014*\"man\" + 0.009*\"color\" + 0.008*\"home\" + 0.008*\"black\" + 0.008*\"brand_new\" + 0.007*\"4\" + 0.007*\"2\" + 0.007*\"3\"\n",
      "INFO : topic #10 (0.040): 0.024*\"item\" + 0.021*\"rm\" + 0.013*\"woman\" + 0.010*\"brand_new\" + 0.010*\"shipping\" + 0.009*\"size\" + 0.009*\"new\" + 0.006*\"10\" + 0.006*\"listing\" + 0.006*\"free\"\n",
      "INFO : topic #5 (0.040): 0.035*\"size\" + 0.018*\"man\" + 0.014*\"shirt\" + 0.012*\"small\" + 0.012*\"woman\" + 0.010*\"shoe\" + 0.008*\"black\" + 0.008*\"xl\" + 0.008*\"dress\" + 0.007*\"vintage_collectible\"\n",
      "INFO : topic #16 (0.040): 0.032*\"woman\" + 0.024*\"bag\" + 0.016*\"woman_handbag\" + 0.012*\"size\" + 0.010*\"2\" + 0.010*\"box\" + 0.010*\"rm\" + 0.010*\"black\" + 0.009*\"new\" + 0.009*\"small\"\n",
      "INFO : topic #11 (0.040): 0.034*\"size\" + 0.016*\"new\" + 0.015*\"short\" + 0.012*\"woman_athletic\" + 0.012*\"necklace\" + 0.011*\"2\" + 0.010*\"3\" + 0.010*\"woman\" + 0.009*\"shoe\" + 0.009*\"8\"\n",
      "INFO : topic diff=0.214914, rho=0.147442\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7245/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7233/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7210/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7240/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7186/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7263/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7263/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7218/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7340/8000 documents converged within 50 iterations\n",
      "DEBUG : 7387/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7384/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7397/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7384/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #15 (0.040): 0.024*\"legging\" + 0.018*\"shirt\" + 0.018*\"lularoe\" + 0.015*\"brand_new\" + 0.014*\"new\" + 0.014*\"size\" + 0.010*\"black\" + 0.010*\"woman\" + 0.010*\"woman_top\" + 0.009*\"woman_athletic\"\n",
      "INFO : topic #8 (0.040): 0.024*\"game\" + 0.023*\"case\" + 0.015*\"new\" + 0.013*\"electronic_video\" + 0.012*\"game_console\" + 0.012*\"brand_new\" + 0.011*\"size\" + 0.011*\"pink\" + 0.011*\"electronic_cell\" + 0.011*\"phone_accessory\"\n",
      "INFO : topic #14 (0.040): 0.042*\"beauty_makeup\" + 0.018*\"lip\" + 0.016*\"face\" + 0.015*\"brand_new\" + 0.012*\"color\" + 0.012*\"eye\" + 0.010*\"new\" + 0.009*\"bundle\" + 0.008*\"shade\" + 0.008*\"item\"\n",
      "INFO : topic #21 (0.040): 0.038*\"bra\" + 0.018*\"woman_underwear\" + 0.017*\"pink\" + 0.014*\"size\" + 0.013*\"victoria_secret\" + 0.012*\"dress\" + 0.011*\"woman\" + 0.010*\"new\" + 0.008*\"bundle\" + 0.008*\"tank\"\n",
      "INFO : topic #24 (0.040): 0.030*\"woman\" + 0.010*\"jean\" + 0.008*\"shoe\" + 0.008*\"beauty_fragrance\" + 0.008*\"1\" + 0.007*\"new\" + 0.007*\"3\" + 0.007*\"size\" + 0.007*\"iphone\" + 0.007*\"perfume\"\n",
      "INFO : topic diff=0.213074, rho=0.132453\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 21\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7385/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7384/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7351/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 18\n",
      "DEBUG : 7381/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 20\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 29\n",
      "DEBUG : 7403/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 30\n",
      "DEBUG : 7459/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7456/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7462/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7512/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7502/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #24 (0.040): 0.029*\"woman\" + 0.010*\"beauty_fragrance\" + 0.009*\"jean\" + 0.008*\"new\" + 0.008*\"1\" + 0.008*\"iphone\" + 0.008*\"perfume\" + 0.007*\"3\" + 0.007*\"shoe\" + 0.007*\"brand_new\"\n",
      "INFO : topic #13 (0.040): 0.044*\"pink\" + 0.023*\"size\" + 0.020*\"woman\" + 0.017*\"victoria_secret\" + 0.015*\"new\" + 0.014*\"man\" + 0.008*\"color\" + 0.008*\"shirt\" + 0.008*\"small\" + 0.008*\"woman_athletic\"\n",
      "INFO : topic #16 (0.040): 0.033*\"woman\" + 0.031*\"bag\" + 0.020*\"woman_handbag\" + 0.011*\"purse\" + 0.010*\"size\" + 0.009*\"kid_toy\" + 0.009*\"black\" + 0.009*\"box\" + 0.009*\"2\" + 0.009*\"new\"\n",
      "INFO : topic #9 (0.040): 0.034*\"woman_top\" + 0.030*\"blouse\" + 0.025*\"shirt\" + 0.014*\"size\" + 0.014*\"blouse_t\" + 0.011*\"kid_toy\" + 0.010*\"brand_new\" + 0.009*\"electronic\" + 0.007*\"description\" + 0.007*\"new\"\n",
      "INFO : topic #19 (0.040): 0.038*\"shirt\" + 0.015*\"size\" + 0.014*\"2\" + 0.013*\"top\" + 0.012*\"apparel\" + 0.011*\"new\" + 0.010*\"woman_athletic\" + 0.010*\"4\" + 0.009*\"bundle\" + 0.009*\"oz\"\n",
      "INFO : topic diff=0.191260, rho=0.120386\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 30\n",
      "DEBUG : 7523/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7500/8000 documents converged within 50 iterations\n",
      "DEBUG : 7466/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : 7440/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7478/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7489/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : 7541/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7566/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7545/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7575/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7557/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.040): 0.046*\"size\" + 0.044*\"woman\" + 0.018*\"black\" + 0.017*\"shoe\" + 0.013*\"new\" + 0.012*\"sweater\" + 0.012*\"man\" + 0.011*\"shirt\" + 0.009*\"medium\" + 0.009*\"small\"\n",
      "INFO : topic #6 (0.040): 0.023*\"woman\" + 0.019*\"size\" + 0.016*\"game\" + 0.010*\"electronic_video\" + 0.010*\"game_console\" + 0.009*\"brand_new\" + 0.009*\"skinny\" + 0.008*\"piece\" + 0.007*\"2\" + 0.007*\"case\"\n",
      "INFO : topic #9 (0.040): 0.037*\"woman_top\" + 0.032*\"blouse\" + 0.028*\"shirt\" + 0.015*\"blouse_t\" + 0.014*\"size\" + 0.011*\"kid_toy\" + 0.010*\"brand_new\" + 0.010*\"electronic\" + 0.008*\"description\" + 0.007*\"new\"\n",
      "INFO : topic #23 (0.040): 0.018*\"ring\" + 0.017*\"size\" + 0.012*\"woman\" + 0.011*\"man\" + 0.010*\"color\" + 0.009*\"brand_new\" + 0.009*\"accessory\" + 0.009*\"gold\" + 0.008*\"new\" + 0.008*\"black\"\n",
      "INFO : topic #10 (0.040): 0.030*\"item\" + 0.025*\"rm\" + 0.013*\"shipping\" + 0.010*\"woman\" + 0.010*\"brand_new\" + 0.008*\"new\" + 0.007*\"size\" + 0.007*\"listing\" + 0.006*\"sticker\" + 0.006*\"10\"\n",
      "INFO : topic diff=0.182643, rho=0.111803\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 20\n",
      "DEBUG : 7607/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7522/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7551/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7551/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 15\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 16\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7538/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 18\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7531/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 26\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 28\n",
      "DEBUG : 7616/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7580/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7573/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7610/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #8 (0.040): 0.032*\"case\" + 0.030*\"game\" + 0.017*\"electronic_video\" + 0.016*\"game_console\" + 0.015*\"phone_accessory\" + 0.015*\"electronic_cell\" + 0.014*\"new\" + 0.011*\"brand_new\" + 0.011*\"case_skin\" + 0.009*\"pink\"\n",
      "INFO : topic #24 (0.040): 0.027*\"woman\" + 0.012*\"beauty_fragrance\" + 0.009*\"iphone\" + 0.009*\"new\" + 0.009*\"perfume\" + 0.009*\"jean\" + 0.008*\"charger\" + 0.008*\"brand_new\" + 0.008*\"phone_accessory\" + 0.008*\"3\"\n",
      "INFO : topic #23 (0.040): 0.020*\"ring\" + 0.017*\"size\" + 0.012*\"woman\" + 0.011*\"man\" + 0.010*\"color\" + 0.009*\"accessory\" + 0.009*\"gold\" + 0.009*\"brand_new\" + 0.009*\"watch\" + 0.008*\"new\"\n",
      "INFO : topic #7 (0.040): 0.047*\"size\" + 0.046*\"woman\" + 0.018*\"shoe\" + 0.018*\"black\" + 0.013*\"new\" + 0.013*\"sweater\" + 0.013*\"man\" + 0.010*\"shirt\" + 0.009*\"small\" + 0.009*\"medium\"\n",
      "INFO : topic #10 (0.040): 0.031*\"item\" + 0.027*\"rm\" + 0.015*\"shipping\" + 0.009*\"brand_new\" + 0.009*\"woman\" + 0.008*\"new\" + 0.008*\"listing\" + 0.007*\"size\" + 0.007*\"sticker\" + 0.006*\"10\"\n",
      "INFO : topic diff=0.175533, rho=0.104828\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7628/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 27DEBUG : 7615/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7588/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7596/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7574/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7646/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 28\n",
      "DEBUG : 7668/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 29\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7662/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7654/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7647/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7656/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7660/8000 documents converged within 50 iterations\n",
      "INFO : topic #22 (0.040): 0.055*\"woman\" + 0.014*\"wallet\" + 0.014*\"accessory\" + 0.011*\"accessory_wallet\" + 0.010*\"new\" + 0.009*\"color\" + 0.009*\"black\" + 0.009*\"pink\" + 0.009*\"brand_new\" + 0.008*\"rm\"\n",
      "INFO : topic #21 (0.040): 0.048*\"bra\" + 0.025*\"woman_underwear\" + 0.020*\"pink\" + 0.018*\"size\" + 0.018*\"victoria_secret\" + 0.012*\"tank\" + 0.011*\"lace\" + 0.009*\"new\" + 0.009*\"woman_top\" + 0.009*\"woman\"\n",
      "INFO : topic #7 (0.040): 0.047*\"woman\" + 0.047*\"size\" + 0.019*\"shoe\" + 0.018*\"black\" + 0.013*\"man\" + 0.013*\"sweater\" + 0.013*\"new\" + 0.010*\"shirt\" + 0.009*\"small\" + 0.009*\"medium\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #14 (0.040): 0.049*\"beauty_makeup\" + 0.020*\"lip\" + 0.018*\"face\" + 0.016*\"brand_new\" + 0.015*\"color\" + 0.015*\"eye\" + 0.012*\"new\" + 0.009*\"makeup\" + 0.009*\"shade\" + 0.009*\"bundle\"\n",
      "INFO : topic #24 (0.040): 0.027*\"woman\" + 0.013*\"beauty_fragrance\" + 0.009*\"iphone\" + 0.009*\"new\" + 0.009*\"perfume\" + 0.009*\"charger\" + 0.008*\"brand_new\" + 0.008*\"phone_accessory\" + 0.008*\"electronic_cell\" + 0.008*\"jean\"\n",
      "INFO : topic diff=0.173913, rho=0.099015\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7623/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7647/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7634/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 23\n",
      "DEBUG : 7650/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 24\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 26DEBUG : 7655/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7706/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7689/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7698/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7706/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #16 (0.040): 0.040*\"bag\" + 0.035*\"woman\" + 0.025*\"woman_handbag\" + 0.015*\"purse\" + 0.011*\"pocket\" + 0.010*\"kid_toy\" + 0.009*\"black\" + 0.009*\"new\" + 0.008*\"box\" + 0.008*\"2\"\n",
      "INFO : topic #17 (0.040): 0.031*\"jacket\" + 0.017*\"size\" + 0.017*\"woman\" + 0.016*\"book\" + 0.011*\"new\" + 0.011*\"small\" + 0.009*\"vest\" + 0.009*\"2\" + 0.008*\"apparel\" + 0.008*\"brand_new\"\n",
      "INFO : topic #20 (0.040): 0.016*\"brush\" + 0.015*\"beauty_skin\" + 0.014*\"size\" + 0.014*\"1\" + 0.012*\"skin\" + 0.012*\"rm\" + 0.010*\"brand_new\" + 0.010*\"care_face\" + 0.008*\"2\" + 0.008*\"new\"\n",
      "INFO : topic #10 (0.040): 0.034*\"item\" + 0.030*\"rm\" + 0.017*\"shipping\" + 0.009*\"brand_new\" + 0.008*\"listing\" + 0.008*\"new\" + 0.008*\"woman\" + 0.008*\"sticker\" + 0.007*\"size\" + 0.006*\"free\"\n",
      "INFO : topic #3 (0.040): 0.019*\"new\" + 0.011*\"brand_new\" + 0.008*\"1\" + 0.008*\"box\" + 0.007*\"rm\" + 0.007*\"bracelet\" + 0.007*\"free_shipping\" + 0.007*\"lip\" + 0.007*\"2\" + 0.006*\"item\"\n",
      "INFO : topic diff=0.163268, rho=0.094072\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 25\n",
      "DEBUG : 7663/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7662/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 20\n",
      "DEBUG : 7668/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7674/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7729/8000 documents converged within 50 iterations\n",
      "DEBUG : 7723/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7717/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7702/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7707/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.040): 0.019*\"new\" + 0.011*\"brand_new\" + 0.008*\"1\" + 0.008*\"box\" + 0.007*\"rm\" + 0.007*\"free_shipping\" + 0.007*\"bracelet\" + 0.007*\"2\" + 0.006*\"lip\" + 0.006*\"price\"\n",
      "INFO : topic #12 (0.040): 0.023*\"0_24\" + 0.022*\"baby\" + 0.021*\"kid_girl\" + 0.019*\"size\" + 0.017*\"3\" + 0.016*\"mo\" + 0.015*\"girl\" + 0.014*\"kid_boy\" + 0.013*\"accessory\" + 0.013*\"shoe\"\n",
      "INFO : topic #0 (0.040): 0.020*\"1\" + 0.017*\"size\" + 0.014*\"home\" + 0.012*\"man\" + 0.011*\"glass\" + 0.010*\"home_dcor\" + 0.009*\"2\" + 0.008*\"3\" + 0.008*\"x\" + 0.008*\"brand_new\"\n",
      "INFO : topic #7 (0.040): 0.049*\"woman\" + 0.048*\"size\" + 0.021*\"shoe\" + 0.018*\"black\" + 0.015*\"man\" + 0.014*\"sweater\" + 0.013*\"new\" + 0.009*\"nike\" + 0.009*\"small\" + 0.009*\"boot\"\n",
      "INFO : topic #15 (0.040): 0.024*\"legging\" + 0.021*\"shirt\" + 0.020*\"lularoe\" + 0.016*\"brand_new\" + 0.014*\"new\" + 0.013*\"size\" + 0.013*\"woman_top\" + 0.011*\"tee\" + 0.011*\"blouse_t\" + 0.010*\"black\"\n",
      "INFO : topic diff=0.162789, rho=0.089803\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7723/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7702/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7724/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7716/8000 documents converged within 50 iterations\n",
      "DEBUG : 1972/2028 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7740/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : 7720/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7737/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.040): 0.019*\"new\" + 0.011*\"brand_new\" + 0.008*\"box\" + 0.008*\"1\" + 0.007*\"rm\" + 0.007*\"free_shipping\" + 0.007*\"2\" + 0.007*\"bracelet\" + 0.006*\"price\" + 0.006*\"item\"\n",
      "INFO : topic #17 (0.040): 0.037*\"jacket\" + 0.019*\"book\" + 0.017*\"size\" + 0.017*\"woman\" + 0.011*\"small\" + 0.011*\"new\" + 0.010*\"vest\" + 0.009*\"apparel\" + 0.009*\"coat_jacket\" + 0.008*\"2\"\n",
      "INFO : topic #9 (0.040): 0.049*\"woman_top\" + 0.038*\"shirt\" + 0.038*\"blouse\" + 0.021*\"blouse_t\" + 0.015*\"size\" + 0.012*\"kid_toy\" + 0.011*\"electronic\" + 0.009*\"brand_new\" + 0.008*\"description\" + 0.008*\"doll_accessory\"\n",
      "INFO : topic #0 (0.040): 0.020*\"1\" + 0.016*\"size\" + 0.016*\"home\" + 0.011*\"man\" + 0.011*\"glass\" + 0.011*\"home_dcor\" + 0.009*\"2\" + 0.008*\"3\" + 0.008*\"x\" + 0.008*\"bath_body\"\n",
      "INFO : topic #4 (0.040): 0.022*\"bracelet\" + 0.020*\"woman_jewelry\" + 0.017*\"earring\" + 0.016*\"rm\" + 0.013*\"size\" + 0.013*\"2\" + 0.013*\"brand_new\" + 0.011*\"new\" + 0.010*\"gold\" + 0.009*\"1\"\n",
      "INFO : topic diff=0.162086, rho=0.086066\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 24000 documents into a model of 1186028 documents\n",
      "INFO : topic #4 (0.040): 0.023*\"bracelet\" + 0.021*\"woman_jewelry\" + 0.018*\"earring\" + 0.016*\"rm\" + 0.013*\"size\" + 0.013*\"brand_new\" + 0.012*\"2\" + 0.011*\"new\" + 0.011*\"gold\" + 0.010*\"jersey\"\n",
      "INFO : topic #7 (0.040): 0.051*\"woman\" + 0.048*\"size\" + 0.023*\"shoe\" + 0.018*\"black\" + 0.016*\"man\" + 0.014*\"sweater\" + 0.012*\"new\" + 0.010*\"nike\" + 0.009*\"boot\" + 0.009*\"small\"\n",
      "INFO : topic #8 (0.040): 0.043*\"case\" + 0.036*\"game\" + 0.021*\"electronic_video\" + 0.020*\"game_console\" + 0.020*\"phone_accessory\" + 0.020*\"electronic_cell\" + 0.015*\"case_skin\" + 0.014*\"new\" + 0.011*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic #20 (0.040): 0.018*\"brush\" + 0.017*\"beauty_skin\" + 0.014*\"skin\" + 0.014*\"1\" + 0.012*\"size\" + 0.012*\"rm\" + 0.011*\"care_face\" + 0.011*\"brand_new\" + 0.009*\"2\" + 0.008*\"mask\"\n",
      "INFO : topic #14 (0.040): 0.052*\"beauty_makeup\" + 0.022*\"lip\" + 0.018*\"face\" + 0.017*\"brand_new\" + 0.016*\"color\" + 0.016*\"eye\" + 0.014*\"new\" + 0.010*\"makeup\" + 0.010*\"palette\" + 0.009*\"shade\"\n",
      "INFO : topic diff=0.078975, rho=0.082689\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.404 per-word bound, 169.3 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25DEBUG : processing chunk #2 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7762/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7770/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : 7770/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7739/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7786/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7777/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.040): 0.016*\"hair\" + 0.011*\"black\" + 0.011*\"color\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.008*\"home_kitchen\" + 0.007*\"2\" + 0.007*\"woman\" + 0.007*\"body\" + 0.006*\"beauty\"\n",
      "INFO : topic #15 (0.040): 0.022*\"legging\" + 0.022*\"shirt\" + 0.021*\"lularoe\" + 0.016*\"brand_new\" + 0.013*\"new\" + 0.013*\"woman_top\" + 0.013*\"tee\" + 0.013*\"size\" + 0.012*\"blouse_t\" + 0.011*\"black\"\n",
      "INFO : topic #13 (0.040): 0.066*\"pink\" + 0.028*\"victoria_secret\" + 0.025*\"size\" + 0.021*\"woman\" + 0.016*\"new\" + 0.014*\"man\" + 0.010*\"small\" + 0.010*\"sweat\" + 0.009*\"woman_athletic\" + 0.009*\"color\"\n",
      "INFO : topic #24 (0.040): 0.024*\"woman\" + 0.016*\"beauty_fragrance\" + 0.012*\"iphone\" + 0.011*\"perfume\" + 0.011*\"charger\" + 0.011*\"phone_accessory\" + 0.010*\"electronic_cell\" + 0.010*\"new\" + 0.010*\"apple\" + 0.010*\"brand_new\"\n",
      "INFO : topic #17 (0.040): 0.044*\"jacket\" + 0.021*\"book\" + 0.017*\"size\" + 0.016*\"woman\" + 0.012*\"small\" + 0.011*\"vest\" + 0.011*\"new\" + 0.011*\"apparel\" + 0.011*\"coat_jacket\" + 0.009*\"woman_athletic\"\n",
      "INFO : topic diff=0.039667, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 27\n",
      "DEBUG : 7781/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 22\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 23\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 24\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 25\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 28\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7794/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7754/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "INFO : topic #21 (0.040): 0.055*\"bra\" + 0.029*\"woman_underwear\" + 0.024*\"size\" + 0.021*\"victoria_secret\" + 0.021*\"pink\" + 0.016*\"tank\" + 0.015*\"lace\" + 0.012*\"blouse_tank\" + 0.011*\"woman_top\" + 0.010*\"pantie\"\n",
      "INFO : topic #14 (0.040): 0.053*\"beauty_makeup\" + 0.022*\"lip\" + 0.019*\"face\" + 0.017*\"brand_new\" + 0.017*\"color\" + 0.016*\"eye\" + 0.014*\"new\" + 0.011*\"makeup\" + 0.010*\"palette\" + 0.010*\"shade\"\n",
      "INFO : topic #10 (0.040): 0.039*\"item\" + 0.035*\"rm\" + 0.021*\"shipping\" + 0.010*\"listing\" + 0.009*\"sticker\" + 0.008*\"brand_new\" + 0.008*\"new\" + 0.007*\"bundle\" + 0.007*\"price\" + 0.007*\"free\"\n",
      "INFO : topic #22 (0.040): 0.062*\"woman\" + 0.017*\"accessory\" + 0.017*\"wallet\" + 0.013*\"accessory_wallet\" + 0.010*\"card\" + 0.010*\"new\" + 0.010*\"color\" + 0.009*\"brand_new\" + 0.009*\"black\" + 0.008*\"pink\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #1 (0.040): 0.050*\"woman\" + 0.041*\"size\" + 0.040*\"dress\" + 0.021*\"jean\" + 0.019*\"pant\" + 0.015*\"dress_knee\" + 0.012*\"length\" + 0.012*\"black\" + 0.011*\"mini\" + 0.011*\"small\"\n",
      "INFO : topic diff=0.042095, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 27\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 27DEBUG : processing chunk #34 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 28\n",
      "DEBUG : 7769/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 27\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #23 (0.040): 0.029*\"ring\" + 0.017*\"size\" + 0.013*\"watch\" + 0.013*\"gold\" + 0.012*\"man\" + 0.011*\"accessory\" + 0.011*\"woman\" + 0.011*\"jewelry\" + 0.011*\"woman_jewelry\" + 0.009*\"brand_new\"\n",
      "INFO : topic #5 (0.040): 0.041*\"man\" + 0.035*\"size\" + 0.019*\"shirt\" + 0.016*\"vintage_collectible\" + 0.012*\"small\" + 0.011*\"belt\" + 0.011*\"xl\" + 0.010*\"top_t\" + 0.009*\"t_shirt\" + 0.009*\"black\"\n",
      "INFO : topic #4 (0.040): 0.026*\"bracelet\" + 0.025*\"woman_jewelry\" + 0.020*\"earring\" + 0.015*\"rm\" + 0.012*\"2\" + 0.012*\"brand_new\" + 0.012*\"size\" + 0.012*\"gold\" + 0.011*\"new\" + 0.011*\"jersey\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #16 (0.040): 0.047*\"bag\" + 0.036*\"woman\" + 0.029*\"woman_handbag\" + 0.018*\"purse\" + 0.012*\"pocket\" + 0.011*\"kid_toy\" + 0.009*\"new\" + 0.009*\"black\" + 0.008*\"action_figure\" + 0.008*\"statue\"\n",
      "INFO : topic #6 (0.040): 0.023*\"woman\" + 0.019*\"size\" + 0.017*\"skinny\" + 0.013*\"game\" + 0.013*\"jean_slim\" + 0.009*\"skinny_jean\" + 0.009*\"piece\" + 0.009*\"brand_new\" + 0.008*\"electronic_video\" + 0.008*\"game_console\"\n",
      "INFO : topic diff=0.044783, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 26\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 27\n",
      "DEBUG : 7808/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 27\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 27\n",
      "DEBUG : 7802/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7821/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.040): 0.051*\"woman\" + 0.041*\"dress\" + 0.041*\"size\" + 0.022*\"jean\" + 0.019*\"pant\" + 0.015*\"dress_knee\" + 0.013*\"length\" + 0.012*\"black\" + 0.011*\"mini\" + 0.011*\"small\"\n",
      "INFO : topic #5 (0.040): 0.042*\"man\" + 0.034*\"size\" + 0.019*\"shirt\" + 0.017*\"vintage_collectible\" + 0.012*\"small\" + 0.011*\"belt\" + 0.011*\"xl\" + 0.011*\"top_t\" + 0.009*\"t_shirt\" + 0.009*\"black\"\n",
      "INFO : topic #21 (0.040): 0.057*\"bra\" + 0.030*\"woman_underwear\" + 0.026*\"size\" + 0.021*\"victoria_secret\" + 0.021*\"pink\" + 0.018*\"tank\" + 0.016*\"lace\" + 0.013*\"blouse_tank\" + 0.012*\"woman_top\" + 0.011*\"pantie\"\n",
      "INFO : topic #18 (0.040): 0.051*\"lularoe\" + 0.050*\"legging\" + 0.042*\"woman_athletic\" + 0.041*\"tight_legging\" + 0.041*\"apparel_pant\" + 0.023*\"size\" + 0.016*\"woman\" + 0.014*\"skirt\" + 0.014*\"home\" + 0.013*\"home_dcor\"\n",
      "INFO : topic #17 (0.040): 0.052*\"jacket\" + 0.024*\"book\" + 0.017*\"size\" + 0.016*\"woman\" + 0.013*\"coat_jacket\" + 0.012*\"vest\" + 0.012*\"apparel\" + 0.012*\"small\" + 0.010*\"new\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic diff=0.045613, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 27\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7820/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 25\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7844/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #16 (0.040): 0.049*\"bag\" + 0.036*\"woman\" + 0.029*\"woman_handbag\" + 0.019*\"purse\" + 0.013*\"pocket\" + 0.012*\"kid_toy\" + 0.009*\"new\" + 0.009*\"black\" + 0.008*\"action_figure\" + 0.008*\"leather\"\n",
      "INFO : topic #4 (0.040): 0.028*\"bracelet\" + 0.027*\"woman_jewelry\" + 0.022*\"earring\" + 0.015*\"rm\" + 0.012*\"gold\" + 0.012*\"necklace\" + 0.012*\"2\" + 0.012*\"brand_new\" + 0.011*\"size\" + 0.011*\"jersey\"\n",
      "INFO : topic #6 (0.040): 0.024*\"woman\" + 0.020*\"size\" + 0.019*\"skinny\" + 0.014*\"jean_slim\" + 0.012*\"game\" + 0.010*\"skinny_jean\" + 0.009*\"piece\" + 0.008*\"brand_new\" + 0.008*\"electronic_video\" + 0.008*\"game_console\"\n",
      "INFO : topic #9 (0.040): 0.056*\"woman_top\" + 0.047*\"shirt\" + 0.042*\"blouse\" + 0.026*\"blouse_t\" + 0.017*\"size\" + 0.013*\"kid_toy\" + 0.012*\"electronic\" + 0.009*\"medium\" + 0.009*\"small\" + 0.009*\"doll_accessory\"\n",
      "INFO : topic #24 (0.040): 0.023*\"woman\" + 0.018*\"beauty_fragrance\" + 0.013*\"iphone\" + 0.013*\"charger\" + 0.012*\"perfume\" + 0.012*\"phone_accessory\" + 0.012*\"electronic_cell\" + 0.011*\"new\" + 0.011*\"apple\" + 0.010*\"brand_new\"\n",
      "INFO : topic diff=0.045912, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7814/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7828/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 27\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 27\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.040): 0.052*\"short\" + 0.038*\"size\" + 0.034*\"woman_athletic\" + 0.029*\"necklace\" + 0.020*\"apparel_short\" + 0.020*\"sport_bra\" + 0.016*\"sock\" + 0.015*\"apparel\" + 0.014*\"new\" + 0.013*\"pair\"\n",
      "INFO : topic #16 (0.040): 0.050*\"bag\" + 0.036*\"woman\" + 0.030*\"woman_handbag\" + 0.019*\"purse\" + 0.013*\"pocket\" + 0.012*\"kid_toy\" + 0.009*\"new\" + 0.009*\"black\" + 0.009*\"action_figure\" + 0.008*\"leather\"\n",
      "INFO : topic #10 (0.040): 0.042*\"item\" + 0.038*\"rm\" + 0.024*\"shipping\" + 0.011*\"listing\" + 0.009*\"sticker\" + 0.008*\"price\" + 0.008*\"bundle\" + 0.008*\"new\" + 0.008*\"brand_new\" + 0.007*\"2\"\n",
      "INFO : topic #12 (0.040): 0.029*\"kid_girl\" + 0.027*\"baby\" + 0.027*\"0_24\" + 0.022*\"size\" + 0.022*\"girl\" + 0.019*\"mo\" + 0.019*\"3\" + 0.018*\"kid_boy\" + 0.015*\"shoe\" + 0.013*\"toddler\"\n",
      "INFO : topic #3 (0.040): 0.018*\"new\" + 0.011*\"brand_new\" + 0.009*\"box\" + 0.008*\"1\" + 0.007*\"2\" + 0.006*\"rm\" + 0.006*\"free_shipping\" + 0.006*\"color\" + 0.006*\"price\" + 0.006*\"item\"\n",
      "INFO : topic diff=0.047308, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 26\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 23\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7822/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 28\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.040): 0.054*\"short\" + 0.038*\"size\" + 0.035*\"woman_athletic\" + 0.029*\"necklace\" + 0.020*\"sport_bra\" + 0.020*\"apparel_short\" + 0.017*\"sock\" + 0.016*\"apparel\" + 0.014*\"new\" + 0.013*\"pair\"\n",
      "INFO : topic #9 (0.040): 0.058*\"woman_top\" + 0.049*\"shirt\" + 0.043*\"blouse\" + 0.027*\"blouse_t\" + 0.017*\"size\" + 0.013*\"kid_toy\" + 0.012*\"electronic\" + 0.010*\"medium\" + 0.010*\"small\" + 0.009*\"doll_accessory\"\n",
      "INFO : topic #0 (0.040): 0.025*\"home\" + 0.020*\"1\" + 0.019*\"home_dcor\" + 0.013*\"glass\" + 0.011*\"size\" + 0.010*\"bath_body\" + 0.010*\"2\" + 0.009*\"x\" + 0.009*\"3\" + 0.008*\"bath\"\n",
      "INFO : topic #7 (0.040): 0.056*\"woman\" + 0.049*\"size\" + 0.027*\"shoe\" + 0.018*\"man\" + 0.018*\"black\" + 0.016*\"sweater\" + 0.012*\"new\" + 0.011*\"boot\" + 0.011*\"nike\" + 0.010*\"athletic\"\n",
      "INFO : topic #20 (0.040): 0.021*\"beauty_skin\" + 0.020*\"brush\" + 0.017*\"skin\" + 0.015*\"1\" + 0.013*\"care_face\" + 0.011*\"brand_new\" + 0.011*\"rm\" + 0.010*\"care\" + 0.010*\"2\" + 0.009*\"size\"\n",
      "INFO : topic diff=0.046953, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 27\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7853/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7849/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.040): 0.052*\"case\" + 0.042*\"game\" + 0.025*\"electronic_video\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.024*\"game_console\" + 0.017*\"case_skin\" + 0.013*\"new\" + 0.012*\"phone\" + 0.011*\"brand_new\"\n",
      "INFO : topic #21 (0.040): 0.059*\"bra\" + 0.032*\"woman_underwear\" + 0.028*\"size\" + 0.023*\"victoria_secret\" + 0.021*\"pink\" + 0.020*\"tank\" + 0.017*\"lace\" + 0.014*\"blouse_tank\" + 0.012*\"woman_top\" + 0.012*\"pantie\"\n",
      "INFO : topic #4 (0.040): 0.030*\"bracelet\" + 0.030*\"woman_jewelry\" + 0.024*\"earring\" + 0.016*\"necklace\" + 0.014*\"rm\" + 0.013*\"gold\" + 0.012*\"jersey\" + 0.012*\"brand_new\" + 0.012*\"2\" + 0.011*\"new\"\n",
      "INFO : topic #6 (0.040): 0.024*\"woman\" + 0.022*\"skinny\" + 0.020*\"size\" + 0.017*\"jean_slim\" + 0.011*\"game\" + 0.011*\"skinny_jean\" + 0.009*\"piece\" + 0.008*\"brand_new\" + 0.008*\"sport_fan\" + 0.007*\"electronic_video\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #1 (0.040): 0.052*\"woman\" + 0.044*\"dress\" + 0.042*\"size\" + 0.024*\"jean\" + 0.019*\"pant\" + 0.016*\"dress_knee\" + 0.014*\"length\" + 0.012*\"mini\" + 0.012*\"black\" + 0.011*\"small\"\n",
      "INFO : topic diff=0.046992, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 28\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 26\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 28\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7856/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.040): 0.085*\"pink\" + 0.036*\"victoria_secret\" + 0.026*\"size\" + 0.023*\"woman\" + 0.016*\"new\" + 0.014*\"man\" + 0.012*\"small\" + 0.012*\"sweat\" + 0.010*\"woman_athletic\" + 0.010*\"medium\"\n",
      "INFO : topic #23 (0.040): 0.035*\"ring\" + 0.017*\"size\" + 0.016*\"watch\" + 0.015*\"gold\" + 0.013*\"jewelry\" + 0.013*\"man\" + 0.012*\"accessory\" + 0.012*\"woman_jewelry\" + 0.010*\"woman\" + 0.009*\"brand_new\"\n",
      "INFO : topic #5 (0.040): 0.051*\"man\" + 0.034*\"size\" + 0.020*\"shirt\" + 0.019*\"vintage_collectible\" + 0.012*\"top_t\" + 0.012*\"belt\" + 0.012*\"small\" + 0.011*\"xl\" + 0.010*\"t_shirt\" + 0.009*\"costume\"\n",
      "INFO : topic #21 (0.040): 0.060*\"bra\" + 0.032*\"woman_underwear\" + 0.029*\"size\" + 0.023*\"victoria_secret\" + 0.021*\"pink\" + 0.020*\"tank\" + 0.017*\"lace\" + 0.014*\"blouse_tank\" + 0.012*\"woman_top\" + 0.012*\"pantie\"\n",
      "INFO : topic #9 (0.040): 0.060*\"woman_top\" + 0.051*\"shirt\" + 0.044*\"blouse\" + 0.028*\"blouse_t\" + 0.017*\"size\" + 0.013*\"kid_toy\" + 0.012*\"electronic\" + 0.010*\"medium\" + 0.010*\"small\" + 0.010*\"doll_accessory\"\n",
      "INFO : topic diff=0.047239, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 28\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #20 (0.040): 0.023*\"beauty_skin\" + 0.021*\"brush\" + 0.018*\"skin\" + 0.015*\"1\" + 0.014*\"care_face\" + 0.012*\"brand_new\" + 0.011*\"care\" + 0.011*\"rm\" + 0.010*\"2\" + 0.010*\"nail\"\n",
      "INFO : topic #12 (0.040): 0.031*\"kid_girl\" + 0.029*\"baby\" + 0.027*\"0_24\" + 0.024*\"size\" + 0.024*\"girl\" + 0.020*\"kid_boy\" + 0.020*\"mo\" + 0.019*\"3\" + 0.015*\"shoe\" + 0.014*\"toddler\"\n",
      "INFO : topic #13 (0.040): 0.086*\"pink\" + 0.037*\"victoria_secret\" + 0.026*\"size\" + 0.023*\"woman\" + 0.016*\"new\" + 0.014*\"man\" + 0.012*\"small\" + 0.012*\"sweat\" + 0.010*\"medium\" + 0.010*\"woman_athletic\"\n",
      "INFO : topic #15 (0.040): 0.022*\"shirt\" + 0.021*\"lularoe\" + 0.018*\"classic\" + 0.017*\"tee\" + 0.016*\"legging\" + 0.015*\"brand_new\" + 0.014*\"woman_top\" + 0.013*\"t\" + 0.013*\"new\" + 0.012*\"blouse_t\"\n",
      "INFO : topic #22 (0.040): 0.069*\"woman\" + 0.021*\"accessory\" + 0.019*\"wallet\" + 0.015*\"accessory_wallet\" + 0.014*\"card\" + 0.011*\"new\" + 0.010*\"brand_new\" + 0.010*\"color\" + 0.010*\"black\" + 0.009*\"vintage_collectible\"\n",
      "INFO : topic diff=0.047636, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 28\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 27\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 28DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.040): 0.017*\"new\" + 0.010*\"brand_new\" + 0.010*\"box\" + 0.008*\"1\" + 0.007*\"2\" + 0.006*\"battery\" + 0.006*\"color\" + 0.006*\"rm\" + 0.006*\"free_shipping\" + 0.005*\"great\"\n",
      "INFO : topic #17 (0.040): 0.062*\"jacket\" + 0.029*\"book\" + 0.020*\"coat_jacket\" + 0.018*\"size\" + 0.017*\"woman\" + 0.014*\"apparel\" + 0.014*\"vest\" + 0.013*\"small\" + 0.011*\"dog\" + 0.011*\"woman_athletic\"\n",
      "INFO : topic #24 (0.040): 0.021*\"woman\" + 0.020*\"beauty_fragrance\" + 0.014*\"charger\" + 0.014*\"iphone\" + 0.014*\"phone_accessory\" + 0.014*\"electronic_cell\" + 0.013*\"perfume\" + 0.012*\"new\" + 0.012*\"apple\" + 0.011*\"brand_new\"\n",
      "INFO : topic #4 (0.040): 0.033*\"woman_jewelry\" + 0.033*\"bracelet\" + 0.026*\"earring\" + 0.020*\"necklace\" + 0.014*\"gold\" + 0.013*\"rm\" + 0.012*\"jersey\" + 0.011*\"brand_new\" + 0.011*\"2\" + 0.011*\"new\"\n",
      "INFO : topic #19 (0.040): 0.059*\"shirt\" + 0.024*\"top\" + 0.018*\"2\" + 0.017*\"size\" + 0.016*\"apparel\" + 0.013*\"4\" + 0.013*\"slime\" + 0.012*\"bundle\" + 0.012*\"t\" + 0.011*\"woman_athletic\"\n",
      "INFO : topic diff=0.047171, rho=0.081581\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7852/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #131 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.040): 0.023*\"hair\" + 0.011*\"color\" + 0.010*\"home_kitchen\" + 0.009*\"black\" + 0.009*\"beauty\" + 0.009*\"brand_new\" + 0.009*\"new\" + 0.007*\"2\" + 0.007*\"body\" + 0.007*\"hair_care\"\n",
      "INFO : topic #7 (0.040): 0.059*\"woman\" + 0.049*\"size\" + 0.030*\"shoe\" + 0.019*\"man\" + 0.018*\"black\" + 0.017*\"sweater\" + 0.012*\"new\" + 0.012*\"boot\" + 0.011*\"athletic\" + 0.011*\"nike\"\n",
      "INFO : topic #23 (0.040): 0.037*\"ring\" + 0.018*\"size\" + 0.017*\"watch\" + 0.016*\"gold\" + 0.014*\"jewelry\" + 0.013*\"man\" + 0.013*\"woman_jewelry\" + 0.013*\"accessory\" + 0.010*\"band\" + 0.009*\"brand_new\"\n",
      "INFO : topic #15 (0.040): 0.022*\"shirt\" + 0.021*\"lularoe\" + 0.020*\"classic\" + 0.018*\"tee\" + 0.015*\"brand_new\" + 0.015*\"legging\" + 0.014*\"t\" + 0.013*\"woman_top\" + 0.012*\"new\" + 0.012*\"blouse_t\"\n",
      "INFO : topic #8 (0.040): 0.055*\"case\" + 0.044*\"game\" + 0.027*\"electronic_video\" + 0.025*\"phone_accessory\" + 0.025*\"game_console\" + 0.025*\"electronic_cell\" + 0.018*\"case_skin\" + 0.013*\"phone\" + 0.013*\"new\" + 0.011*\"2\"\n",
      "INFO : topic diff=0.047167, rho=0.081581\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1995/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.040): 0.056*\"case\" + 0.045*\"game\" + 0.027*\"electronic_video\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.025*\"game_console\" + 0.018*\"case_skin\" + 0.013*\"phone\" + 0.013*\"new\" + 0.011*\"2\"\n",
      "INFO : topic #18 (0.040): 0.058*\"legging\" + 0.057*\"lularoe\" + 0.046*\"woman_athletic\" + 0.045*\"tight_legging\" + 0.045*\"apparel_pant\" + 0.024*\"size\" + 0.017*\"woman\" + 0.015*\"black\" + 0.014*\"brand_new\" + 0.014*\"skirt\"\n",
      "INFO : topic #5 (0.040): 0.059*\"man\" + 0.033*\"size\" + 0.021*\"shirt\" + 0.020*\"vintage_collectible\" + 0.013*\"top_t\" + 0.012*\"belt\" + 0.012*\"xl\" + 0.011*\"small\" + 0.011*\"t_shirt\" + 0.009*\"figure\"\n",
      "INFO : topic #7 (0.040): 0.059*\"woman\" + 0.049*\"size\" + 0.031*\"shoe\" + 0.020*\"man\" + 0.018*\"black\" + 0.017*\"sweater\" + 0.012*\"new\" + 0.012*\"athletic\" + 0.012*\"boot\" + 0.011*\"nike\"\n",
      "INFO : topic #2 (0.040): 0.023*\"hair\" + 0.011*\"color\" + 0.010*\"home_kitchen\" + 0.009*\"beauty\" + 0.009*\"black\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"2\" + 0.007*\"body\" + 0.007*\"hair_care\"\n",
      "INFO : topic diff=0.047446, rho=0.081581\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 42028 documents into a model of 1186028 documents\n",
      "INFO : topic #4 (0.040): 0.036*\"woman_jewelry\" + 0.034*\"bracelet\" + 0.027*\"earring\" + 0.025*\"necklace\" + 0.015*\"gold\" + 0.013*\"jersey\" + 0.012*\"rm\" + 0.011*\"new\" + 0.011*\"brand_new\" + 0.011*\"2\"\n",
      "INFO : topic #9 (0.040): 0.064*\"woman_top\" + 0.056*\"shirt\" + 0.046*\"blouse\" + 0.030*\"blouse_t\" + 0.019*\"size\" + 0.014*\"kid_toy\" + 0.012*\"electronic\" + 0.011*\"medium\" + 0.011*\"small\" + 0.010*\"doll_accessory\"\n",
      "INFO : topic #21 (0.040): 0.062*\"bra\" + 0.034*\"woman_underwear\" + 0.032*\"size\" + 0.024*\"victoria_secret\" + 0.023*\"tank\" + 0.021*\"pink\" + 0.019*\"lace\" + 0.015*\"blouse_tank\" + 0.013*\"pantie\" + 0.013*\"woman_top\"\n",
      "INFO : topic #13 (0.040): 0.094*\"pink\" + 0.040*\"victoria_secret\" + 0.027*\"size\" + 0.024*\"woman\" + 0.016*\"new\" + 0.013*\"man\" + 0.013*\"small\" + 0.013*\"sweat\" + 0.010*\"medium\" + 0.010*\"large\"\n",
      "INFO : topic #10 (0.040): 0.045*\"item\" + 0.044*\"rm\" + 0.027*\"shipping\" + 0.013*\"listing\" + 0.011*\"price\" + 0.010*\"bundle\" + 0.009*\"sticker\" + 0.009*\"2\" + 0.008*\"free\" + 0.008*\"day\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.042894, rho=0.081581\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.227 per-word bound, 149.8 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 27\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 28DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #22 (0.040): 0.072*\"woman\" + 0.024*\"accessory\" + 0.020*\"wallet\" + 0.016*\"card\" + 0.016*\"accessory_wallet\" + 0.011*\"new\" + 0.011*\"brand_new\" + 0.011*\"color\" + 0.010*\"black\" + 0.010*\"vintage_collectible\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.040): 0.060*\"woman\" + 0.050*\"size\" + 0.031*\"shoe\" + 0.020*\"man\" + 0.018*\"black\" + 0.017*\"sweater\" + 0.012*\"new\" + 0.012*\"boot\" + 0.012*\"athletic\" + 0.011*\"nike\"\n",
      "INFO : topic #24 (0.040): 0.021*\"woman\" + 0.021*\"beauty_fragrance\" + 0.015*\"charger\" + 0.015*\"iphone\" + 0.014*\"phone_accessory\" + 0.014*\"electronic_cell\" + 0.014*\"perfume\" + 0.013*\"new\" + 0.013*\"apple\" + 0.012*\"brand_new\"\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #23 (0.040): 0.039*\"ring\" + 0.018*\"watch\" + 0.018*\"size\" + 0.017*\"gold\" + 0.014*\"jewelry\" + 0.013*\"woman_jewelry\" + 0.013*\"accessory\" + 0.013*\"man\" + 0.011*\"band\" + 0.009*\"brand_new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #6 (0.040): 0.030*\"skinny\" + 0.025*\"woman\" + 0.022*\"jean_slim\" + 0.021*\"size\" + 0.014*\"skinny_jean\" + 0.011*\"sport_fan\" + 0.009*\"american_eagle\" + 0.008*\"piece\" + 0.008*\"game\" + 0.008*\"brand_new\"\n",
      "INFO : topic diff=0.040768, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 26\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 27\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "INFO : merging changes from 112000 documents into a model of 1186028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #15 (0.040): 0.024*\"classic\" + 0.021*\"shirt\" + 0.020*\"lularoe\" + 0.019*\"tee\" + 0.016*\"t\" + 0.015*\"brand_new\" + 0.013*\"woman_top\" + 0.013*\"legging\" + 0.012*\"new\" + 0.011*\"blouse_t\"\n",
      "INFO : topic #1 (0.040): 0.055*\"woman\" + 0.047*\"dress\" + 0.042*\"size\" + 0.027*\"jean\" + 0.019*\"pant\" + 0.017*\"dress_knee\" + 0.016*\"length\" + 0.014*\"mini\" + 0.012*\"black\" + 0.011*\"small\"\n",
      "INFO : topic #6 (0.040): 0.031*\"skinny\" + 0.025*\"woman\" + 0.023*\"jean_slim\" + 0.021*\"size\" + 0.014*\"skinny_jean\" + 0.012*\"sport_fan\" + 0.009*\"american_eagle\" + 0.008*\"piece\" + 0.008*\"game\" + 0.008*\"brand_new\"\n",
      "INFO : topic #13 (0.040): 0.097*\"pink\" + 0.041*\"victoria_secret\" + 0.027*\"size\" + 0.025*\"woman\" + 0.016*\"new\" + 0.013*\"small\" + 0.013*\"man\" + 0.013*\"sweat\" + 0.011*\"medium\" + 0.010*\"hat\"\n",
      "INFO : topic #18 (0.040): 0.060*\"legging\" + 0.059*\"lularoe\" + 0.047*\"woman_athletic\" + 0.046*\"tight_legging\" + 0.046*\"apparel_pant\" + 0.024*\"size\" + 0.017*\"woman\" + 0.015*\"black\" + 0.014*\"brand_new\" + 0.013*\"skirt\"\n",
      "INFO : topic diff=0.042715, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 23\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 18\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 19\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 26\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 28\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 29\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 28\n",
      "DEBUG : 7907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7915/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #15 (0.040): 0.025*\"classic\" + 0.021*\"shirt\" + 0.020*\"lularoe\" + 0.019*\"tee\" + 0.017*\"t\" + 0.015*\"brand_new\" + 0.013*\"woman_top\" + 0.012*\"legging\" + 0.012*\"new\" + 0.011*\"blouse_t\"\n",
      "INFO : topic #20 (0.040): 0.024*\"beauty_skin\" + 0.022*\"brush\" + 0.020*\"skin\" + 0.015*\"1\" + 0.014*\"care_face\" + 0.012*\"care\" + 0.012*\"brand_new\" + 0.011*\"nail\" + 0.010*\"new\" + 0.010*\"2\"\n",
      "INFO : topic #7 (0.040): 0.061*\"woman\" + 0.050*\"size\" + 0.032*\"shoe\" + 0.020*\"man\" + 0.018*\"black\" + 0.018*\"sweater\" + 0.012*\"athletic\" + 0.012*\"boot\" + 0.012*\"new\" + 0.012*\"nike\"\n",
      "INFO : topic #2 (0.040): 0.025*\"hair\" + 0.011*\"color\" + 0.011*\"home_kitchen\" + 0.010*\"beauty\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"black\" + 0.008*\"2\" + 0.007*\"hair_care\" + 0.007*\"body\"\n",
      "INFO : topic #18 (0.040): 0.061*\"legging\" + 0.059*\"lularoe\" + 0.047*\"woman_athletic\" + 0.047*\"tight_legging\" + 0.047*\"apparel_pant\" + 0.024*\"size\" + 0.017*\"woman\" + 0.016*\"black\" + 0.014*\"brand_new\" + 0.013*\"skirt\"\n",
      "INFO : topic diff=0.042617, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 28\n",
      "DEBUG : 7909/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 26\n",
      "DEBUG : 7904/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 28\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7904/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7912/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #11 (0.040): 0.071*\"short\" + 0.045*\"woman_athletic\" + 0.042*\"size\" + 0.025*\"sport_bra\" + 0.025*\"apparel_short\" + 0.022*\"apparel\" + 0.021*\"necklace\" + 0.020*\"sock\" + 0.019*\"pair\" + 0.013*\"black\"\n",
      "INFO : topic #12 (0.040): 0.036*\"kid_girl\" + 0.032*\"baby\" + 0.029*\"0_24\" + 0.028*\"girl\" + 0.027*\"size\" + 0.023*\"kid_boy\" + 0.021*\"mo\" + 0.020*\"3\" + 0.016*\"shoe\" + 0.015*\"toddler\"\n",
      "INFO : topic #21 (0.040): 0.063*\"bra\" + 0.034*\"woman_underwear\" + 0.034*\"size\" + 0.024*\"victoria_secret\" + 0.024*\"tank\" + 0.021*\"pink\" + 0.020*\"lace\" + 0.016*\"blouse_tank\" + 0.014*\"pantie\" + 0.013*\"woman_top\"\n",
      "INFO : topic #23 (0.040): 0.041*\"ring\" + 0.019*\"watch\" + 0.018*\"size\" + 0.017*\"gold\" + 0.015*\"jewelry\" + 0.014*\"woman_jewelry\" + 0.013*\"accessory\" + 0.013*\"man\" + 0.012*\"band\" + 0.009*\"silver\"\n",
      "INFO : topic #5 (0.040): 0.068*\"man\" + 0.032*\"size\" + 0.022*\"vintage_collectible\" + 0.021*\"shirt\" + 0.014*\"top_t\" + 0.013*\"belt\" + 0.012*\"xl\" + 0.011*\"small\" + 0.011*\"t_shirt\" + 0.010*\"figure\"\n",
      "INFO : topic diff=0.042589, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 28\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 27\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7913/8000 documents converged within 50 iterations\n",
      "DEBUG : updating topics\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7903/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7919/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.040): 0.055*\"woman\" + 0.048*\"dress\" + 0.043*\"size\" + 0.027*\"jean\" + 0.019*\"pant\" + 0.018*\"dress_knee\" + 0.016*\"length\" + 0.014*\"mini\" + 0.013*\"black\" + 0.011*\"small\"\n",
      "INFO : topic #9 (0.040): 0.067*\"woman_top\" + 0.060*\"shirt\" + 0.047*\"blouse\" + 0.032*\"blouse_t\" + 0.020*\"size\" + 0.014*\"kid_toy\" + 0.012*\"electronic\" + 0.012*\"medium\" + 0.011*\"small\" + 0.010*\"doll_accessory\"\n",
      "INFO : topic #11 (0.040): 0.073*\"short\" + 0.046*\"woman_athletic\" + 0.042*\"size\" + 0.026*\"sport_bra\" + 0.025*\"apparel_short\" + 0.022*\"apparel\" + 0.021*\"sock\" + 0.020*\"necklace\" + 0.019*\"pair\" + 0.013*\"black\"\n",
      "INFO : topic #5 (0.040): 0.070*\"man\" + 0.032*\"size\" + 0.022*\"vintage_collectible\" + 0.021*\"shirt\" + 0.014*\"top_t\" + 0.013*\"belt\" + 0.012*\"xl\" + 0.011*\"small\" + 0.011*\"t_shirt\" + 0.010*\"figure\"\n",
      "INFO : topic #0 (0.040): 0.036*\"home\" + 0.029*\"home_dcor\" + 0.017*\"1\" + 0.013*\"glass\" + 0.011*\"accent\" + 0.011*\"bath_body\" + 0.010*\"2\" + 0.010*\"x\" + 0.009*\"3\" + 0.009*\"candle\"\n",
      "INFO : topic diff=0.042792, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 27\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 22\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 26\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 27\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7909/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7910/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 28\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7903/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #20 (0.040): 0.024*\"beauty_skin\" + 0.022*\"brush\" + 0.021*\"skin\" + 0.015*\"1\" + 0.014*\"care_face\" + 0.013*\"care\" + 0.012*\"brand_new\" + 0.011*\"nail\" + 0.011*\"new\" + 0.010*\"2\"\n",
      "INFO : topic #2 (0.040): 0.026*\"hair\" + 0.011*\"home_kitchen\" + 0.011*\"color\" + 0.010*\"beauty\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"black\" + 0.008*\"hair_care\" + 0.008*\"2\" + 0.007*\"natural\"\n",
      "INFO : topic #0 (0.040): 0.036*\"home\" + 0.030*\"home_dcor\" + 0.017*\"1\" + 0.013*\"glass\" + 0.011*\"accent\" + 0.011*\"bath_body\" + 0.010*\"2\" + 0.010*\"x\" + 0.009*\"3\" + 0.009*\"candle\"\n",
      "INFO : topic #17 (0.040): 0.068*\"jacket\" + 0.032*\"book\" + 0.027*\"coat_jacket\" + 0.020*\"woman\" + 0.019*\"size\" + 0.015*\"dog\" + 0.014*\"apparel\" + 0.014*\"vest\" + 0.014*\"small\" + 0.011*\"woman_athletic\"\n",
      "INFO : topic #19 (0.040): 0.067*\"shirt\" + 0.028*\"top\" + 0.018*\"2\" + 0.017*\"size\" + 0.017*\"apparel\" + 0.015*\"slime\" + 0.014*\"4\" + 0.013*\"bundle\" + 0.011*\"woman_athletic\" + 0.011*\"t\"\n",
      "INFO : topic diff=0.043191, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 28\n",
      "DEBUG : 7898/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7903/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7909/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #3 (0.040): 0.015*\"new\" + 0.010*\"box\" + 0.010*\"brand_new\" + 0.008*\"battery\" + 0.008*\"1\" + 0.008*\"audio_surveillance\" + 0.008*\"electronic_tv\" + 0.008*\"headphone\" + 0.007*\"2\" + 0.006*\"bluetooth\"\n",
      "INFO : topic #4 (0.040): 0.042*\"woman_jewelry\" + 0.037*\"bracelet\" + 0.035*\"necklace\" + 0.029*\"earring\" + 0.017*\"gold\" + 0.013*\"jersey\" + 0.013*\"silver\" + 0.012*\"charm\" + 0.011*\"new\" + 0.010*\"rm\"\n",
      "INFO : topic #2 (0.040): 0.026*\"hair\" + 0.011*\"home_kitchen\" + 0.011*\"color\" + 0.010*\"beauty\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"black\" + 0.008*\"hair_care\" + 0.008*\"2\" + 0.007*\"natural\"\n",
      "INFO : topic #13 (0.040): 0.106*\"pink\" + 0.043*\"victoria_secret\" + 0.027*\"size\" + 0.027*\"woman\" + 0.016*\"new\" + 0.014*\"small\" + 0.013*\"sweat\" + 0.012*\"man\" + 0.011*\"hat\" + 0.011*\"medium\"\n",
      "INFO : topic #15 (0.040): 0.029*\"classic\" + 0.020*\"shirt\" + 0.019*\"tee\" + 0.019*\"t\" + 0.019*\"lularoe\" + 0.015*\"brand_new\" + 0.012*\"woman_top\" + 0.012*\"kid\" + 0.012*\"new\" + 0.011*\"blouse_t\"\n",
      "INFO : topic diff=0.042503, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 18\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 26\n",
      "DEBUG : 7912/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 27\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 27\n",
      "DEBUG : 7924/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7921/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7916/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7918/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.040): 0.107*\"pink\" + 0.044*\"victoria_secret\" + 0.027*\"size\" + 0.027*\"woman\" + 0.016*\"new\" + 0.014*\"small\" + 0.013*\"sweat\" + 0.012*\"hat\" + 0.011*\"man\" + 0.011*\"medium\"\n",
      "INFO : topic #0 (0.040): 0.037*\"home\" + 0.031*\"home_dcor\" + 0.017*\"1\" + 0.013*\"glass\" + 0.012*\"accent\" + 0.011*\"bath_body\" + 0.010*\"2\" + 0.010*\"x\" + 0.009*\"3\" + 0.009*\"candle\"\n",
      "INFO : topic #15 (0.040): 0.030*\"classic\" + 0.020*\"t\" + 0.020*\"shirt\" + 0.019*\"tee\" + 0.019*\"lularoe\" + 0.015*\"brand_new\" + 0.012*\"kid\" + 0.012*\"woman_top\" + 0.012*\"new\" + 0.010*\"blouse_t\"\n",
      "INFO : topic #3 (0.040): 0.015*\"new\" + 0.010*\"box\" + 0.010*\"brand_new\" + 0.008*\"battery\" + 0.008*\"headphone\" + 0.008*\"audio_surveillance\" + 0.008*\"electronic_tv\" + 0.008*\"1\" + 0.007*\"2\" + 0.006*\"bluetooth\"\n",
      "INFO : topic #22 (0.040): 0.075*\"woman\" + 0.026*\"accessory\" + 0.021*\"wallet\" + 0.018*\"card\" + 0.017*\"accessory_wallet\" + 0.012*\"new\" + 0.011*\"brand_new\" + 0.011*\"color\" + 0.011*\"mug\" + 0.011*\"vintage_collectible\"\n",
      "DEBUG : 7919/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.042168, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 26\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7921/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : 7915/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 27DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 28\n",
      "DEBUG : 7917/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 29\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7918/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7917/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #20 (0.040): 0.025*\"beauty_skin\" + 0.022*\"brush\" + 0.021*\"skin\" + 0.015*\"1\" + 0.014*\"care_face\" + 0.013*\"care\" + 0.012*\"brand_new\" + 0.012*\"nail\" + 0.011*\"new\" + 0.010*\"2\"\n",
      "INFO : topic #8 (0.040): 0.060*\"case\" + 0.047*\"game\" + 0.028*\"electronic_video\" + 0.027*\"phone_accessory\" + 0.027*\"game_console\" + 0.027*\"electronic_cell\" + 0.019*\"case_skin\" + 0.015*\"phone\" + 0.012*\"new\" + 0.011*\"2\"\n",
      "INFO : topic #14 (0.040): 0.059*\"beauty_makeup\" + 0.025*\"lip\" + 0.020*\"color\" + 0.020*\"face\" + 0.019*\"eye\" + 0.018*\"brand_new\" + 0.016*\"new\" + 0.013*\"makeup\" + 0.012*\"palette\" + 0.011*\"shade\"\n",
      "DEBUG : 7915/8000 documents converged within 50 iterations\n",
      "INFO : topic #11 (0.040): 0.077*\"short\" + 0.049*\"woman_athletic\" + 0.043*\"size\" + 0.027*\"sport_bra\" + 0.026*\"apparel_short\" + 0.025*\"apparel\" + 0.022*\"sock\" + 0.021*\"pair\" + 0.016*\"necklace\" + 0.014*\"black\"\n",
      "INFO : topic #7 (0.040): 0.063*\"woman\" + 0.050*\"size\" + 0.035*\"shoe\" + 0.021*\"man\" + 0.018*\"sweater\" + 0.018*\"black\" + 0.013*\"athletic\" + 0.013*\"boot\" + 0.012*\"new\" + 0.012*\"nike\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.042505, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7904/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 26\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 27\n",
      "DEBUG : 7931/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 27\n",
      "DEBUG : 7917/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7912/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7905/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7923/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7909/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7903/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.040): 0.015*\"new\" + 0.010*\"box\" + 0.010*\"brand_new\" + 0.009*\"battery\" + 0.008*\"headphone\" + 0.008*\"audio_surveillance\" + 0.008*\"electronic_tv\" + 0.008*\"1\" + 0.007*\"2\" + 0.006*\"accessory\"\n",
      "INFO : topic #22 (0.040): 0.076*\"woman\" + 0.027*\"accessory\" + 0.021*\"wallet\" + 0.019*\"card\" + 0.017*\"accessory_wallet\" + 0.012*\"new\" + 0.011*\"brand_new\" + 0.011*\"color\" + 0.011*\"vintage_collectible\" + 0.011*\"mug\"\n",
      "INFO : topic #4 (0.040): 0.044*\"woman_jewelry\" + 0.039*\"necklace\" + 0.038*\"bracelet\" + 0.030*\"earring\" + 0.017*\"gold\" + 0.013*\"jersey\" + 0.013*\"silver\" + 0.013*\"charm\" + 0.011*\"new\" + 0.010*\"brand_new\"\n",
      "INFO : topic #9 (0.040): 0.069*\"woman_top\" + 0.063*\"shirt\" + 0.048*\"blouse\" + 0.033*\"blouse_t\" + 0.020*\"size\" + 0.014*\"kid_toy\" + 0.013*\"medium\" + 0.012*\"electronic\" + 0.012*\"small\" + 0.011*\"tee\"\n",
      "INFO : topic #0 (0.040): 0.039*\"home\" + 0.032*\"home_dcor\" + 0.017*\"1\" + 0.012*\"glass\" + 0.012*\"accent\" + 0.011*\"bath_body\" + 0.010*\"2\" + 0.010*\"x\" + 0.009*\"3\" + 0.009*\"candle\"\n",
      "INFO : topic diff=0.041867, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7919/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 27\n",
      "DEBUG : 7917/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 28\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7920/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : 7933/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7924/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7921/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #21 (0.040): 0.064*\"bra\" + 0.036*\"size\" + 0.035*\"woman_underwear\" + 0.026*\"tank\" + 0.025*\"victoria_secret\" + 0.021*\"lace\" + 0.021*\"pink\" + 0.017*\"blouse_tank\" + 0.014*\"pantie\" + 0.014*\"black\"\n",
      "INFO : topic #8 (0.040): 0.061*\"case\" + 0.047*\"game\" + 0.029*\"electronic_video\" + 0.027*\"phone_accessory\" + 0.027*\"electronic_cell\" + 0.027*\"game_console\" + 0.019*\"case_skin\" + 0.015*\"phone\" + 0.012*\"new\" + 0.011*\"2\"\n",
      "INFO : topic #16 (0.040): 0.059*\"bag\" + 0.038*\"woman\" + 0.033*\"woman_handbag\" + 0.023*\"purse\" + 0.015*\"pocket\" + 0.013*\"kid_toy\" + 0.011*\"leather\" + 0.010*\"coach\" + 0.010*\"action_figure\" + 0.009*\"statue\"\n",
      "INFO : topic #24 (0.040): 0.022*\"beauty_fragrance\" + 0.020*\"woman\" + 0.017*\"charger\" + 0.016*\"phone_accessory\" + 0.016*\"electronic_cell\" + 0.016*\"iphone\" + 0.015*\"perfume\" + 0.014*\"new\" + 0.014*\"apple\" + 0.013*\"brand_new\"\n",
      "INFO : topic #18 (0.040): 0.065*\"legging\" + 0.062*\"lularoe\" + 0.049*\"tight_legging\" + 0.049*\"apparel_pant\" + 0.048*\"woman_athletic\" + 0.025*\"size\" + 0.017*\"woman\" + 0.017*\"black\" + 0.015*\"brand_new\" + 0.012*\"print\"\n",
      "INFO : topic diff=0.041385, rho=0.081311\n",
      "DEBUG : 7917/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7916/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7916/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7922/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7927/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7924/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7911/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7922/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7935/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7916/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7904/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.040): 0.027*\"hair\" + 0.012*\"home_kitchen\" + 0.011*\"beauty\" + 0.011*\"color\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.008*\"hair_care\" + 0.008*\"2\" + 0.008*\"black\" + 0.007*\"natural\"\n",
      "INFO : topic #23 (0.040): 0.045*\"ring\" + 0.021*\"watch\" + 0.019*\"gold\" + 0.019*\"size\" + 0.017*\"jewelry\" + 0.015*\"woman_jewelry\" + 0.014*\"accessory\" + 0.014*\"band\" + 0.013*\"man\" + 0.010*\"silver\"\n",
      "DEBUG : 7921/8000 documents converged within 50 iterations\n",
      "INFO : topic #7 (0.040): 0.064*\"woman\" + 0.050*\"size\" + 0.036*\"shoe\" + 0.021*\"man\" + 0.019*\"sweater\" + 0.018*\"black\" + 0.014*\"athletic\" + 0.013*\"boot\" + 0.012*\"new\" + 0.012*\"nike\"\n",
      "INFO : topic #18 (0.040): 0.065*\"legging\" + 0.063*\"lularoe\" + 0.049*\"tight_legging\" + 0.049*\"apparel_pant\" + 0.049*\"woman_athletic\" + 0.025*\"size\" + 0.017*\"woman\" + 0.017*\"black\" + 0.015*\"brand_new\" + 0.012*\"print\"\n",
      "INFO : topic #12 (0.040): 0.039*\"kid_girl\" + 0.033*\"baby\" + 0.031*\"girl\" + 0.030*\"size\" + 0.029*\"0_24\" + 0.026*\"kid_boy\" + 0.021*\"3\" + 0.021*\"mo\" + 0.017*\"boy\" + 0.016*\"toddler\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.040867, rho=0.081311\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7918/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7923/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7907/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7912/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2013/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7922/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : 7908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7937/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.040): 0.060*\"beauty_makeup\" + 0.026*\"lip\" + 0.021*\"color\" + 0.020*\"face\" + 0.019*\"eye\" + 0.018*\"brand_new\" + 0.016*\"new\" + 0.013*\"makeup\" + 0.012*\"palette\" + 0.011*\"shade\"\n",
      "INFO : topic #2 (0.040): 0.027*\"hair\" + 0.012*\"home_kitchen\" + 0.011*\"beauty\" + 0.011*\"color\" + 0.009*\"new\" + 0.009*\"brand_new\" + 0.009*\"hair_care\" + 0.008*\"2\" + 0.007*\"black\" + 0.007*\"natural\"\n",
      "INFO : topic #5 (0.040): 0.084*\"man\" + 0.030*\"size\" + 0.024*\"vintage_collectible\" + 0.021*\"shirt\" + 0.015*\"top_t\" + 0.013*\"belt\" + 0.012*\"xl\" + 0.011*\"figure\" + 0.011*\"t_shirt\" + 0.010*\"small\"\n",
      "INFO : topic #9 (0.040): 0.070*\"woman_top\" + 0.065*\"shirt\" + 0.048*\"blouse\" + 0.034*\"blouse_t\" + 0.021*\"size\" + 0.014*\"kid_toy\" + 0.013*\"medium\" + 0.012*\"small\" + 0.012*\"electronic\" + 0.012*\"tee\"\n",
      "INFO : topic #4 (0.040): 0.046*\"woman_jewelry\" + 0.042*\"necklace\" + 0.039*\"bracelet\" + 0.030*\"earring\" + 0.018*\"gold\" + 0.014*\"silver\" + 0.013*\"charm\" + 0.013*\"jersey\" + 0.011*\"new\" + 0.010*\"chain\"\n",
      "INFO : topic diff=0.039670, rho=0.081311\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 16000 documents into a model of 1186028 documents\n",
      "INFO : topic #23 (0.040): 0.046*\"ring\" + 0.022*\"watch\" + 0.020*\"gold\" + 0.019*\"size\" + 0.017*\"jewelry\" + 0.015*\"woman_jewelry\" + 0.014*\"accessory\" + 0.014*\"band\" + 0.012*\"man\" + 0.011*\"silver\"\n",
      "INFO : topic #5 (0.040): 0.085*\"man\" + 0.030*\"size\" + 0.024*\"vintage_collectible\" + 0.021*\"shirt\" + 0.015*\"top_t\" + 0.013*\"belt\" + 0.012*\"xl\" + 0.011*\"figure\" + 0.011*\"t_shirt\" + 0.010*\"small\"\n",
      "INFO : topic #3 (0.040): 0.014*\"new\" + 0.010*\"box\" + 0.010*\"brand_new\" + 0.009*\"battery\" + 0.009*\"headphone\" + 0.009*\"audio_surveillance\" + 0.009*\"electronic_tv\" + 0.008*\"1\" + 0.007*\"2\" + 0.007*\"accessory\"\n",
      "INFO : topic #10 (0.040): 0.049*\"rm\" + 0.048*\"item\" + 0.031*\"shipping\" + 0.015*\"price\" + 0.015*\"listing\" + 0.013*\"bundle\" + 0.011*\"day\" + 0.011*\"2\" + 0.010*\"free\" + 0.010*\"free_shipping\"\n",
      "INFO : topic #24 (0.040): 0.023*\"beauty_fragrance\" + 0.020*\"woman\" + 0.017*\"charger\" + 0.016*\"phone_accessory\" + 0.016*\"electronic_cell\" + 0.016*\"iphone\" + 0.016*\"perfume\" + 0.014*\"apple\" + 0.014*\"new\" + 0.013*\"brand_new\"\n",
      "INFO : topic diff=0.037359, rho=0.081311\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.177 per-word bound, 144.7 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=329852, num_topics=25, decay=0.5, chunksize=8000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 585000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 586000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 587000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 588000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 589000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 590000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 591000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 592000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 593000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 594000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 595000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 596000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 597000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 598000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 599000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 600000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 601000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 602000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 603000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 604000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 605000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 606000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 607000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 608000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 609000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 610000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 611000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 612000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 613000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 614000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 615000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 616000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 617000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 618000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 619000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 620000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 621000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 622000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 623000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 624000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 625000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 626000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 627000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 628000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 629000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 630000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 631000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 632000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 633000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 634000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 635000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 636000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 637000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 638000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 639000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 640000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 641000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 642000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 643000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 644000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 645000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 646000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 647000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 648000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 649000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 650000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 651000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 652000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 653000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 654000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 655000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 656000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 657000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 658000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 659000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 660000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 661000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 662000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 663000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 664000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 665000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 666000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 667000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 668000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 669000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 670000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 671000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 672000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 673000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 674000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 675000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 676000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 677000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 678000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 679000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 680000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 681000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 682000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 683000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 684000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 685000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 686000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 687000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 688000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 689000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 690000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 691000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 692000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 693000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 694000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 695000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 696000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 697000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 698000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 699000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 700000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 701000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 702000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 703000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 704000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 705000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 706000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 707000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 708000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 709000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 710000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 711000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 712000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 713000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 714000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 715000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 716000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 717000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 718000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 719000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 720000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 721000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 722000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 723000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 724000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 725000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 726000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 727000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 728000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 729000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 730000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 731000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 732000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 733000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 734000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 735000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 736000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 737000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 738000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 739000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 740000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 741000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 742000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 743000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 744000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 745000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 746000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 747000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 748000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 749000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 750000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 751000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 752000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 753000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 754000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 755000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 756000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 757000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 758000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 759000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 760000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 761000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 762000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 763000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 764000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 765000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 766000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 767000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 768000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 769000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 770000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 771000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 772000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 773000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 774000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 775000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 776000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 777000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 778000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 779000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 780000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 781000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 782000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 783000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 784000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 785000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 786000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 787000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 788000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 789000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 790000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 791000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 792000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 793000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 794000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 795000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 796000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 797000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 798000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 799000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 800000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 801000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 802000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 803000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 804000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 805000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 806000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 807000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 808000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 809000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 810000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 811000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 812000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 813000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 814000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 815000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 816000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 817000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 818000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 819000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 820000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 821000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 822000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 823000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 824000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 825000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 826000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 827000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 828000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 829000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 830000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 831000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 832000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 833000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 834000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 835000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 836000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 837000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 838000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 839000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 840000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 841000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 842000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 843000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 844000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 845000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 846000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 847000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 848000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 849000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 850000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 851000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 852000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 853000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 854000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 855000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 856000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 857000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 858000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 859000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 860000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 861000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 862000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 863000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 864000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 865000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 866000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 867000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 868000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 869000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 870000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 871000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 872000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 873000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 874000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 875000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 876000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 877000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 878000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 879000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 880000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 881000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 882000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 883000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 884000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 885000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 886000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 887000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 888000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 889000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 890000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 891000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 892000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 893000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 894000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 895000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 896000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 897000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 898000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 899000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 900000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 901000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 902000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 903000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 904000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 905000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 906000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 907000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 908000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 909000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 910000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 911000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 912000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 913000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 914000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 915000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 916000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 917000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 918000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 919000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 920000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 921000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 922000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 923000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 924000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 925000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 926000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 927000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 928000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 929000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 930000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 931000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 932000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 933000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 934000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 935000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 936000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 937000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 938000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 939000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 940000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 941000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 942000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 943000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 944000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 945000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 946000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 947000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 948000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 949000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 950000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 951000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 952000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 953000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 954000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 955000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 956000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 957000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 958000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 959000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 960000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 961000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 962000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 963000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 964000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 965000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 966000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 967000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 968000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 969000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 970000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 971000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 972000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 973000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 974000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 975000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 976000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 977000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 978000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 979000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 980000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 981000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 982000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 983000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 984000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 985000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 986000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 987000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 988000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 989000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 990000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 991000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 992000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 993000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 994000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 995000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 996000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 997000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 998000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 999000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1000000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1001000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1002000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1003000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1004000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1005000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1006000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1007000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1008000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1009000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1010000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1011000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1012000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1013000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1014000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1015000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1016000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1017000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1018000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1019000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1020000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1021000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1022000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1023000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1024000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1025000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1026000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1027000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1028000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1029000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1030000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1031000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1032000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1033000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1034000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1035000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1036000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1037000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1038000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1039000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1040000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1041000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1042000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1043000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1044000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1045000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1046000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1047000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1048000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1049000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1050000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1051000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1052000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1053000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1054000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1055000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1056000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1057000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1058000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1059000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1060000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1061000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1062000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1063000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1064000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1065000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1066000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1067000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1068000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1069000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1070000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1071000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1072000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1073000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1074000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1075000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1076000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1077000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1078000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1079000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1080000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1081000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1082000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1083000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1084000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1085000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1086000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1087000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1088000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1089000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1090000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1091000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1092000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1093000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1094000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1095000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1096000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1097000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1098000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1099000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1186000 documents\n",
      "DEBUG : performing inference on a chunk of 1186028 documents\n",
      "DEBUG : 1174505/1186028 documents converged within 50 iterations\n",
      "\n",
      "\n",
      "100%|| 5/5 [43:20<00:00, 520.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "Lda = models.LdaMulticore\n",
    "coherenceList_umass = []\n",
    "num_topics_list = np.arange(5,30,5)\n",
    "for num_topics in tqdm(num_topics_list):\n",
    "    lda= Lda(doc_term_matrix, num_topics=num_topics,id2word = dictionary, \n",
    "             passes=3,chunksize=8000,random_state=43)\n",
    "    cm = CoherenceModel(model=lda, corpus=doc_term_matrix, \n",
    "                        dictionary=dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())\n",
    "    vis = pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary)\n",
    "    pyLDAvis.save_html(vis,f'pyLDAvis_{num_topics}.html')\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotData = pd.DataFrame({'Number of topics':num_topics_list,\n",
    "                         'CoherenceScore':coherenceList_umass})\n",
    "f,ax = plt.subplots(figsize=(10,6))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.pointplot(x='Number of topics',y= 'CoherenceScore',data=plotData)\n",
    "plt.axhline(y=-3.5)\n",
    "plt.title('Topic coherence')\n",
    "plt.savefig('Topic coherence plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.06666666666666667\n",
      "INFO : using symmetric eta at 0.06666666666666667\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 15 topics, 5 passes over the supplied corpus of 1186028 documents, updating every 88000 documents, evaluating every ~880000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 11 processes\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "DEBUG : worker process entering E-step loop\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6081/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 33\n",
      "DEBUG : 6126/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 33\n",
      "DEBUG : 6089/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 33\n",
      "DEBUG : 6120/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6086/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 33\n",
      "DEBUG : 6126/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6082/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 6077/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6024/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 33\n",
      "DEBUG : 6127/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 33\n",
      "DEBUG : 6146/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6064/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6077/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.030*\"woman\" + 0.027*\"size\" + 0.016*\"shirt\" + 0.015*\"small\" + 0.010*\"woman_top\" + 0.010*\"pink\" + 0.010*\"black\" + 0.008*\"bag\" + 0.008*\"brand_new\" + 0.007*\"shoe\"\n",
      "INFO : topic #0 (0.067): 0.025*\"size\" + 0.014*\"woman\" + 0.011*\"white\" + 0.011*\"rm\" + 0.009*\"dress\" + 0.009*\"woman_top\" + 0.009*\"black\" + 0.009*\"shirt\" + 0.008*\"ring\" + 0.007*\"blue\"\n",
      "INFO : topic #6 (0.067): 0.019*\"woman\" + 0.016*\"size\" + 0.010*\"brand_new\" + 0.010*\"new\" + 0.010*\"color\" + 0.008*\"gold\" + 0.008*\"shirt\" + 0.007*\"beauty_makeup\" + 0.006*\"bag\" + 0.006*\"item\"\n",
      "INFO : topic #13 (0.067): 0.017*\"man\" + 0.015*\"size\" + 0.015*\"shirt\" + 0.012*\"blouse\" + 0.010*\"pant\" + 0.010*\"game\" + 0.010*\"brand_new\" + 0.009*\"new\" + 0.008*\"woman_top\" + 0.008*\"medium\"\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #12 (0.067): 0.018*\"rm\" + 0.014*\"pink\" + 0.013*\"new\" + 0.012*\"size\" + 0.012*\"woman\" + 0.008*\"2\" + 0.007*\"brand_new\" + 0.007*\"color\" + 0.007*\"dress\" + 0.007*\"bundle\"\n",
      "INFO : topic diff=15.616856, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 33\n",
      "DEBUG : 6085/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6037/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6076/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6124/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 33\n",
      "DEBUG : 6137/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 33\n",
      "DEBUG : 6194/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6169/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 33\n",
      "DEBUG : 6148/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : 6099/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6087/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6427/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6094/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6433/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.025*\"size\" + 0.014*\"woman\" + 0.011*\"white\" + 0.010*\"rm\" + 0.009*\"black\" + 0.009*\"woman_top\" + 0.009*\"shirt\" + 0.009*\"dress\" + 0.008*\"ring\" + 0.007*\"blue\"\n",
      "INFO : topic #13 (0.067): 0.017*\"man\" + 0.015*\"size\" + 0.015*\"shirt\" + 0.012*\"blouse\" + 0.010*\"pant\" + 0.010*\"game\" + 0.010*\"brand_new\" + 0.009*\"new\" + 0.008*\"woman_top\" + 0.008*\"medium\"\n",
      "INFO : topic #14 (0.067): 0.037*\"size\" + 0.031*\"woman\" + 0.013*\"brand_new\" + 0.011*\"shoe\" + 0.010*\"2\" + 0.010*\"new\" + 0.009*\"home\" + 0.008*\"short\" + 0.007*\"woman_athletic\" + 0.007*\"rm\"\n",
      "INFO : topic #5 (0.067): 0.024*\"woman\" + 0.023*\"size\" + 0.013*\"shirt\" + 0.011*\"small\" + 0.010*\"pink\" + 0.009*\"brand_new\" + 0.008*\"large\" + 0.008*\"woman_athletic\" + 0.007*\"black\" + 0.007*\"3\"\n",
      "INFO : topic #7 (0.067): 0.030*\"woman\" + 0.027*\"size\" + 0.017*\"shirt\" + 0.015*\"small\" + 0.010*\"woman_top\" + 0.010*\"pink\" + 0.010*\"black\" + 0.008*\"bag\" + 0.008*\"brand_new\" + 0.007*\"new\"\n",
      "INFO : topic diff=0.390637, rho=0.288675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 32DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 33\n",
      "DEBUG : 6432/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6473/8000 documents converged within 50 iterations\n",
      "DEBUG : 6471/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 33\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6479/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6520/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 33\n",
      "DEBUG : 6493/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6497/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 33\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6490/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6500/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #2 (0.067): 0.014*\"new\" + 0.014*\"color\" + 0.014*\"size\" + 0.013*\"1\" + 0.011*\"2\" + 0.010*\"woman\" + 0.008*\"beauty_makeup\" + 0.008*\"rm\" + 0.007*\"pink\" + 0.007*\"3\"\n",
      "INFO : topic #1 (0.067): 0.021*\"woman\" + 0.018*\"size\" + 0.014*\"black\" + 0.010*\"bracelet\" + 0.009*\"pink\" + 0.008*\"brand_new\" + 0.007*\"jean\" + 0.007*\"4\" + 0.007*\"woman_athletic\" + 0.007*\"home\"\n",
      "INFO : topic #3 (0.067): 0.015*\"item\" + 0.013*\"new\" + 0.012*\"size\" + 0.011*\"1\" + 0.010*\"woman\" + 0.009*\"rm\" + 0.008*\"3\" + 0.007*\"free_shipping\" + 0.006*\"2\" + 0.006*\"brand_new\"\n",
      "INFO : topic #9 (0.067): 0.027*\"woman\" + 0.018*\"bundle\" + 0.015*\"man\" + 0.009*\"size\" + 0.009*\"new\" + 0.008*\"condition\" + 0.008*\"brand_new\" + 0.008*\"accessory\" + 0.007*\"black\" + 0.007*\"rm\"\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.067): 0.017*\"size\" + 0.013*\"brand_new\" + 0.012*\"3\" + 0.011*\"beauty_makeup\" + 0.009*\"new\" + 0.009*\"2\" + 0.008*\"1\" + 0.007*\"woman\" + 0.007*\"pink\" + 0.007*\"small\"\n",
      "INFO : topic diff=0.275562, rho=0.208514\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 33\n",
      "DEBUG : 6421/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 33\n",
      "DEBUG : 6424/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 33\n",
      "DEBUG : 6392/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 33\n",
      "DEBUG : 6395/8000 documents converged within 50 iterations\n",
      "DEBUG : 6411/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 33\n",
      "DEBUG : 6414/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6375/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 33DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 6406/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6445/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #54 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6435/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6401/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.023*\"rm\" + 0.014*\"pink\" + 0.012*\"new\" + 0.010*\"size\" + 0.009*\"woman\" + 0.009*\"bra\" + 0.008*\"2\" + 0.007*\"brand_new\" + 0.007*\"color\" + 0.007*\"bundle\"\n",
      "INFO : topic #8 (0.067): 0.025*\"new\" + 0.014*\"case\" + 0.014*\"black\" + 0.012*\"brand_new\" + 0.012*\"man\" + 0.011*\"accessory\" + 0.009*\"2\" + 0.008*\"phone_accessory\" + 0.008*\"electronic_cell\" + 0.007*\"color\"\n",
      "INFO : topic #13 (0.067): 0.022*\"man\" + 0.019*\"shirt\" + 0.016*\"game\" + 0.016*\"size\" + 0.015*\"blouse\" + 0.013*\"pant\" + 0.010*\"electronic_video\" + 0.010*\"game_console\" + 0.009*\"woman_top\" + 0.009*\"brand_new\"\n",
      "INFO : topic #9 (0.067): 0.028*\"woman\" + 0.018*\"bundle\" + 0.016*\"man\" + 0.009*\"size\" + 0.009*\"new\" + 0.009*\"condition\" + 0.008*\"accessory\" + 0.008*\"brand_new\" + 0.007*\"black\" + 0.007*\"rm\"\n",
      "INFO : topic #10 (0.067): 0.028*\"woman\" + 0.016*\"size\" + 0.015*\"brand_new\" + 0.014*\"black\" + 0.012*\"new\" + 0.008*\"color\" + 0.008*\"woman_athletic\" + 0.007*\"beauty_makeup\" + 0.007*\"tag\" + 0.006*\"2\"\n",
      "DEBUG : 6714/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.216875, rho=0.171499\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 33\n",
      "DEBUG : 6742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6693/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 33\n",
      "DEBUG : 6681/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6687/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6736/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 32\n",
      "DEBUG : 6776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6788/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : 6944/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.067): 0.025*\"man\" + 0.021*\"shirt\" + 0.019*\"game\" + 0.017*\"blouse\" + 0.016*\"size\" + 0.014*\"pant\" + 0.012*\"electronic_video\" + 0.011*\"game_console\" + 0.010*\"woman_top\" + 0.009*\"new\"\n",
      "INFO : topic #0 (0.067): 0.028*\"size\" + 0.014*\"dress\" + 0.013*\"woman\" + 0.013*\"white\" + 0.012*\"ring\" + 0.011*\"woman_top\" + 0.011*\"black\" + 0.010*\"rm\" + 0.010*\"shirt\" + 0.008*\"blue\"\n",
      "INFO : topic #11 (0.067): 0.013*\"woman\" + 0.010*\"legging\" + 0.010*\"beauty_makeup\" + 0.009*\"new\" + 0.009*\"lularoe\" + 0.009*\"rm\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.007*\"nail\" + 0.007*\"size\"\n",
      "INFO : topic #14 (0.067): 0.040*\"size\" + 0.036*\"woman\" + 0.017*\"shoe\" + 0.012*\"brand_new\" + 0.011*\"short\" + 0.010*\"new\" + 0.010*\"home\" + 0.009*\"2\" + 0.007*\"woman_athletic\" + 0.007*\"black\"\n",
      "INFO : topic #6 (0.067): 0.020*\"woman\" + 0.014*\"size\" + 0.011*\"necklace\" + 0.011*\"gold\" + 0.010*\"brand_new\" + 0.010*\"new\" + 0.010*\"color\" + 0.009*\"bag\" + 0.007*\"beauty_makeup\" + 0.007*\"inch\"\n",
      "DEBUG : 6936/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.187938, rho=0.149071\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 33\n",
      "DEBUG : 6842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 33\n",
      "DEBUG : 6920/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 33\n",
      "DEBUG : 6890/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 6866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 33\n",
      "DEBUG : 6908/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 6889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7080/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 6994/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.034*\"woman\" + 0.030*\"size\" + 0.024*\"shirt\" + 0.019*\"small\" + 0.018*\"woman_top\" + 0.017*\"pink\" + 0.012*\"black\" + 0.011*\"victoria_secret\" + 0.011*\"bag\" + 0.011*\"blouse_t\"\n",
      "INFO : topic #12 (0.067): 0.025*\"rm\" + 0.014*\"pink\" + 0.012*\"new\" + 0.011*\"bra\" + 0.010*\"size\" + 0.008*\"2\" + 0.008*\"home_dcor\" + 0.008*\"card\" + 0.008*\"woman\" + 0.008*\"brand_new\"\n",
      "INFO : topic #4 (0.067): 0.016*\"beauty_makeup\" + 0.015*\"size\" + 0.014*\"brand_new\" + 0.013*\"3\" + 0.011*\"new\" + 0.010*\"baby\" + 0.009*\"2\" + 0.008*\"beauty_skin\" + 0.008*\"1\" + 0.007*\"face\"\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.067): 0.028*\"size\" + 0.026*\"woman\" + 0.017*\"woman_athletic\" + 0.013*\"pink\" + 0.012*\"small\" + 0.012*\"shirt\" + 0.012*\"legging\" + 0.012*\"jacket\" + 0.010*\"dress\" + 0.010*\"large\"\n",
      "INFO : topic #10 (0.067): 0.031*\"woman\" + 0.015*\"size\" + 0.015*\"brand_new\" + 0.013*\"black\" + 0.012*\"new\" + 0.008*\"color\" + 0.007*\"woman_athletic\" + 0.007*\"beauty_fragrance\" + 0.007*\"beauty_makeup\" + 0.006*\"tag\"\n",
      "INFO : topic diff=0.177405, rho=0.133631\n",
      "DEBUG : 7054/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 32\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7057/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7122/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7094/8000 documents converged within 50 iterations\n",
      "DEBUG : 7096/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : 7051/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7089/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7071/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7048/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.026*\"rm\" + 0.014*\"pink\" + 0.012*\"bra\" + 0.012*\"new\" + 0.009*\"size\" + 0.009*\"home_dcor\" + 0.009*\"2\" + 0.008*\"card\" + 0.008*\"brand_new\" + 0.007*\"bundle\"\n",
      "INFO : topic #10 (0.067): 0.032*\"woman\" + 0.015*\"size\" + 0.015*\"brand_new\" + 0.013*\"black\" + 0.012*\"new\" + 0.008*\"color\" + 0.008*\"beauty_fragrance\" + 0.007*\"woman_athletic\" + 0.006*\"perfume\" + 0.006*\"tag\"\n",
      "INFO : topic #13 (0.067): 0.029*\"man\" + 0.024*\"game\" + 0.023*\"shirt\" + 0.018*\"blouse\" + 0.016*\"size\" + 0.015*\"pant\" + 0.014*\"electronic_video\" + 0.014*\"game_console\" + 0.010*\"woman_top\" + 0.009*\"new\"\n",
      "INFO : topic #2 (0.067): 0.016*\"color\" + 0.014*\"1\" + 0.014*\"beauty_makeup\" + 0.014*\"new\" + 0.011*\"2\" + 0.010*\"size\" + 0.009*\"skin\" + 0.009*\"rm\" + 0.008*\"lip\" + 0.007*\"brand_new\"\n",
      "INFO : topic #8 (0.067): 0.022*\"new\" + 0.021*\"case\" + 0.013*\"accessory\" + 0.013*\"phone_accessory\" + 0.013*\"electronic_cell\" + 0.012*\"brand_new\" + 0.012*\"black\" + 0.010*\"2\" + 0.009*\"man\" + 0.007*\"case_skin\"\n",
      "DEBUG : 7057/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.169262, rho=0.122169\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #88 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 31\n",
      "DEBUG : 7178/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 32\n",
      "DEBUG : 7206/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 33\n",
      "DEBUG : 7229/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7230/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7167/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7213/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7154/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 33\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7216/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7186/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7260/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7311/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7302/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #2 (0.067): 0.016*\"color\" + 0.015*\"beauty_makeup\" + 0.014*\"1\" + 0.013*\"new\" + 0.011*\"2\" + 0.010*\"size\" + 0.009*\"skin\" + 0.009*\"rm\" + 0.008*\"lip\" + 0.008*\"brand_new\"\n",
      "INFO : topic #1 (0.067): 0.021*\"woman\" + 0.018*\"bracelet\" + 0.018*\"size\" + 0.014*\"jean\" + 0.012*\"black\" + 0.009*\"pink\" + 0.009*\"pair\" + 0.009*\"home\" + 0.008*\"woman_jewelry\" + 0.007*\"brand_new\"\n",
      "INFO : topic #0 (0.067): 0.028*\"size\" + 0.019*\"dress\" + 0.015*\"ring\" + 0.013*\"white\" + 0.013*\"woman\" + 0.011*\"black\" + 0.011*\"woman_top\" + 0.009*\"rm\" + 0.009*\"woman_jewelry\" + 0.009*\"blue\"\n",
      "INFO : topic #14 (0.067): 0.042*\"size\" + 0.039*\"woman\" + 0.021*\"shoe\" + 0.012*\"short\" + 0.011*\"new\" + 0.011*\"brand_new\" + 0.009*\"home\" + 0.008*\"black\" + 0.008*\"2\" + 0.007*\"man\"\n",
      "INFO : topic #3 (0.067): 0.020*\"item\" + 0.012*\"1\" + 0.012*\"new\" + 0.009*\"rm\" + 0.008*\"3\" + 0.008*\"size\" + 0.008*\"2\" + 0.007*\"shipping\" + 0.007*\"vintage_collectible\" + 0.007*\"free_shipping\"\n",
      "DEBUG : result put\n",
      "INFO : topic diff=0.160744, rho=0.113228\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 33\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7265/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7327/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 33\n",
      "DEBUG : 7297/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 33\n",
      "DEBUG : 7255/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7236/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 33DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7269/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 32\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 0, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 33\n",
      "DEBUG : 7316/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 33DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7271/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #4 (0.067): 0.018*\"beauty_makeup\" + 0.014*\"brand_new\" + 0.014*\"size\" + 0.014*\"3\" + 0.012*\"baby\" + 0.012*\"new\" + 0.010*\"2\" + 0.009*\"beauty_skin\" + 0.009*\"face\" + 0.008*\"1\"\n",
      "INFO : topic #11 (0.067): 0.010*\"legging\" + 0.010*\"beauty_makeup\" + 0.010*\"new\" + 0.009*\"woman\" + 0.009*\"nail\" + 0.009*\"lularoe\" + 0.009*\"brand_new\" + 0.009*\"2\" + 0.008*\"book\" + 0.008*\"rm\"\n",
      "INFO : topic #2 (0.067): 0.016*\"color\" + 0.016*\"beauty_makeup\" + 0.014*\"1\" + 0.013*\"new\" + 0.011*\"2\" + 0.009*\"skin\" + 0.009*\"size\" + 0.009*\"rm\" + 0.009*\"lip\" + 0.008*\"brand_new\"\n",
      "INFO : topic #9 (0.067): 0.029*\"woman\" + 0.018*\"man\" + 0.016*\"bundle\" + 0.011*\"accessory\" + 0.010*\"hair\" + 0.010*\"new\" + 0.009*\"condition\" + 0.008*\"beauty\" + 0.008*\"brand_new\" + 0.007*\"size\"\n",
      "INFO : topic #14 (0.067): 0.043*\"size\" + 0.040*\"woman\" + 0.022*\"shoe\" + 0.013*\"short\" + 0.011*\"new\" + 0.010*\"brand_new\" + 0.009*\"home\" + 0.009*\"black\" + 0.008*\"2\" + 0.007*\"man\"\n",
      "INFO : topic diff=0.155203, rho=0.106000\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 33\n",
      "DEBUG : 7289/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7409/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7331/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 32\n",
      "DEBUG : 7285/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 33\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 33\n",
      "DEBUG : 7353/8000 documents converged within 50 iterations\n",
      "DEBUG : 7344/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 33\n",
      "DEBUG : 7383/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7369/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7343/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 32\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 32\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 33\n",
      "DEBUG : 7340/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 33\n",
      "DEBUG : 7331/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7413/8000 documents converged within 50 iterations\n",
      "DEBUG : 7387/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #3 (0.067): 0.022*\"item\" + 0.012*\"1\" + 0.012*\"new\" + 0.009*\"rm\" + 0.008*\"3\" + 0.008*\"2\" + 0.008*\"shipping\" + 0.008*\"vintage_collectible\" + 0.007*\"size\" + 0.007*\"rae\"\n",
      "INFO : topic #2 (0.067): 0.017*\"beauty_makeup\" + 0.016*\"color\" + 0.014*\"1\" + 0.013*\"new\" + 0.011*\"2\" + 0.010*\"skin\" + 0.009*\"lip\" + 0.009*\"rm\" + 0.009*\"size\" + 0.008*\"brand_new\"\n",
      "INFO : topic #10 (0.067): 0.034*\"woman\" + 0.015*\"brand_new\" + 0.014*\"size\" + 0.013*\"new\" + 0.012*\"black\" + 0.010*\"beauty_fragrance\" + 0.008*\"perfume\" + 0.008*\"color\" + 0.007*\"hat\" + 0.007*\"man\"\n",
      "INFO : topic #14 (0.067): 0.043*\"size\" + 0.041*\"woman\" + 0.023*\"shoe\" + 0.013*\"short\" + 0.011*\"new\" + 0.010*\"brand_new\" + 0.009*\"black\" + 0.009*\"home\" + 0.008*\"man\" + 0.008*\"2\"\n",
      "INFO : topic #13 (0.067): 0.035*\"man\" + 0.029*\"game\" + 0.026*\"shirt\" + 0.019*\"blouse\" + 0.017*\"electronic_video\" + 0.017*\"game_console\" + 0.016*\"size\" + 0.016*\"pant\" + 0.009*\"woman_top\" + 0.009*\"new\"\n",
      "DEBUG : 7399/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.151814, rho=0.100000\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7398/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : bound: at document #0\n",
      "DEBUG : 7390/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7432/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7434/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7454/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7411/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7433/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7448/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7435/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7489/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7447/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7498/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7448/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7470/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7459/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7501/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7522/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7492/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7486/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7435/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7462/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7453/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7401/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7467/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7477/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7430/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7488/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7454/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7490/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : -7.729 per-word bound, 212.1 perplexity estimate based on a held-out corpus of 8000 documents with 141210 words\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 33\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.067): 0.034*\"woman\" + 0.033*\"size\" + 0.028*\"shirt\" + 0.024*\"woman_top\" + 0.021*\"pink\" + 0.020*\"small\" + 0.014*\"victoria_secret\" + 0.014*\"blouse_t\" + 0.013*\"black\" + 0.011*\"medium\"\n",
      "INFO : topic #6 (0.067): 0.024*\"woman\" + 0.019*\"necklace\" + 0.018*\"bag\" + 0.014*\"gold\" + 0.011*\"woman_handbag\" + 0.010*\"new\" + 0.010*\"size\" + 0.009*\"brand_new\" + 0.009*\"inch\" + 0.009*\"woman_jewelry\"\n",
      "INFO : topic #10 (0.067): 0.035*\"woman\" + 0.015*\"brand_new\" + 0.014*\"size\" + 0.013*\"new\" + 0.012*\"black\" + 0.011*\"beauty_fragrance\" + 0.008*\"perfume\" + 0.008*\"hat\" + 0.008*\"man\" + 0.007*\"color\"\n",
      "INFO : topic #13 (0.067): 0.036*\"man\" + 0.030*\"game\" + 0.027*\"shirt\" + 0.020*\"blouse\" + 0.018*\"electronic_video\" + 0.017*\"game_console\" + 0.016*\"pant\" + 0.016*\"size\" + 0.009*\"new\" + 0.009*\"woman_top\"\n",
      "INFO : topic #12 (0.067): 0.030*\"rm\" + 0.015*\"bra\" + 0.015*\"pink\" + 0.012*\"home_dcor\" + 0.011*\"new\" + 0.009*\"card\" + 0.009*\"2\" + 0.009*\"home\" + 0.009*\"size\" + 0.009*\"woman_underwear\"\n",
      "INFO : topic diff=0.154475, rho=0.094916\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 22\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : 7477/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.067): 0.019*\"beauty_makeup\" + 0.015*\"3\" + 0.015*\"brand_new\" + 0.014*\"size\" + 0.013*\"baby\" + 0.012*\"new\" + 0.011*\"2\" + 0.010*\"beauty_skin\" + 0.009*\"face\" + 0.009*\"0_24\"\n",
      "INFO : topic #0 (0.067): 0.028*\"size\" + 0.024*\"dress\" + 0.018*\"ring\" + 0.013*\"white\" + 0.013*\"woman\" + 0.012*\"black\" + 0.011*\"woman_jewelry\" + 0.010*\"woman_top\" + 0.009*\"blue\" + 0.009*\"rm\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #11 (0.067): 0.010*\"book\" + 0.010*\"nail\" + 0.010*\"new\" + 0.009*\"beauty_makeup\" + 0.009*\"brand_new\" + 0.009*\"legging\" + 0.009*\"2\" + 0.008*\"slime\" + 0.008*\"kid_toy\" + 0.008*\"lularoe\"\n",
      "INFO : topic #5 (0.067): 0.031*\"size\" + 0.026*\"woman_athletic\" + 0.026*\"woman\" + 0.020*\"legging\" + 0.015*\"tight_legging\" + 0.015*\"apparel_pant\" + 0.015*\"lularoe\" + 0.014*\"jacket\" + 0.013*\"pink\" + 0.013*\"small\"\n",
      "INFO : topic #8 (0.067): 0.027*\"case\" + 0.019*\"new\" + 0.018*\"phone_accessory\" + 0.018*\"electronic_cell\" + 0.015*\"accessory\" + 0.013*\"brand_new\" + 0.010*\"2\" + 0.009*\"black\" + 0.009*\"iphone\" + 0.009*\"phone\"\n",
      "INFO : topic diff=0.159845, rho=0.090167\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 12\n",
      "DEBUG : 7458/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 5\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7512/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7502/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 1926/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.028*\"size\" + 0.025*\"dress\" + 0.018*\"ring\" + 0.013*\"white\" + 0.013*\"woman\" + 0.012*\"black\" + 0.011*\"woman_jewelry\" + 0.010*\"woman_top\" + 0.009*\"blue\" + 0.009*\"rm\"\n",
      "INFO : topic #11 (0.067): 0.011*\"book\" + 0.011*\"nail\" + 0.010*\"new\" + 0.009*\"beauty_makeup\" + 0.009*\"brand_new\" + 0.009*\"slime\" + 0.009*\"2\" + 0.009*\"kid_toy\" + 0.008*\"legging\" + 0.008*\"lularoe\"\n",
      "INFO : topic #10 (0.067): 0.037*\"woman\" + 0.015*\"brand_new\" + 0.014*\"size\" + 0.013*\"new\" + 0.012*\"beauty_fragrance\" + 0.012*\"black\" + 0.009*\"perfume\" + 0.009*\"hat\" + 0.008*\"man\" + 0.007*\"color\"\n",
      "INFO : topic #6 (0.067): 0.025*\"woman\" + 0.022*\"bag\" + 0.020*\"necklace\" + 0.014*\"gold\" + 0.013*\"woman_handbag\" + 0.010*\"new\" + 0.009*\"woman_jewelry\" + 0.009*\"inch\" + 0.009*\"brand_new\" + 0.008*\"size\"\n",
      "INFO : topic #13 (0.067): 0.039*\"man\" + 0.032*\"game\" + 0.028*\"shirt\" + 0.020*\"electronic_video\" + 0.019*\"blouse\" + 0.019*\"game_console\" + 0.016*\"pant\" + 0.016*\"size\" + 0.009*\"new\" + 0.009*\"top_t\"\n",
      "INFO : topic diff=0.141536, rho=0.085749\n",
      "DEBUG : 7548/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7570/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 18028 documents into a model of 1186028 documents\n",
      "INFO : topic #11 (0.067): 0.011*\"book\" + 0.011*\"nail\" + 0.010*\"new\" + 0.009*\"slime\" + 0.009*\"kid_toy\" + 0.009*\"brand_new\" + 0.009*\"beauty_makeup\" + 0.009*\"2\" + 0.008*\"legging\" + 0.007*\"headphone\"\n",
      "INFO : topic #5 (0.067): 0.032*\"size\" + 0.028*\"woman_athletic\" + 0.026*\"woman\" + 0.021*\"legging\" + 0.017*\"lularoe\" + 0.016*\"tight_legging\" + 0.016*\"apparel_pant\" + 0.015*\"jacket\" + 0.013*\"pink\" + 0.013*\"small\"\n",
      "INFO : topic #3 (0.067): 0.025*\"item\" + 0.012*\"1\" + 0.011*\"new\" + 0.010*\"rm\" + 0.009*\"2\" + 0.009*\"shipping\" + 0.009*\"home_kitchen\" + 0.008*\"3\" + 0.008*\"rae\" + 0.008*\"vintage_collectible\"\n",
      "INFO : topic #8 (0.067): 0.029*\"case\" + 0.019*\"phone_accessory\" + 0.019*\"electronic_cell\" + 0.018*\"new\" + 0.015*\"accessory\" + 0.013*\"brand_new\" + 0.010*\"iphone\" + 0.010*\"2\" + 0.010*\"phone\" + 0.009*\"case_skin\"\n",
      "INFO : topic #0 (0.067): 0.028*\"size\" + 0.027*\"dress\" + 0.018*\"ring\" + 0.013*\"woman\" + 0.013*\"white\" + 0.012*\"black\" + 0.011*\"woman_jewelry\" + 0.010*\"woman_top\" + 0.009*\"blue\" + 0.009*\"rm\"\n",
      "INFO : topic diff=0.067254, rho=0.082479\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.404 per-word bound, 169.3 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7635/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7619/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 30\n",
      "DEBUG : 7636/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 30\n",
      "DEBUG : 7595/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 30DEBUG : processing chunk #11 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7580/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 31\n",
      "DEBUG : 7601/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7626/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 31\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7614/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7639/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 31\n",
      "DEBUG : 7633/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 31\n",
      "DEBUG : 7630/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7636/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7647/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7609/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.067): 0.012*\"book\" + 0.011*\"nail\" + 0.010*\"new\" + 0.010*\"kid_toy\" + 0.010*\"slime\" + 0.009*\"brand_new\" + 0.009*\"2\" + 0.009*\"beauty_makeup\" + 0.008*\"headphone\" + 0.007*\"legging\"\n",
      "INFO : topic #0 (0.067): 0.028*\"size\" + 0.028*\"dress\" + 0.019*\"ring\" + 0.013*\"woman\" + 0.013*\"white\" + 0.012*\"black\" + 0.012*\"woman_jewelry\" + 0.010*\"woman_top\" + 0.009*\"blue\" + 0.009*\"rm\"\n",
      "INFO : topic #9 (0.067): 0.028*\"woman\" + 0.017*\"man\" + 0.014*\"bundle\" + 0.014*\"accessory\" + 0.013*\"hair\" + 0.012*\"kid_toy\" + 0.010*\"beauty\" + 0.010*\"new\" + 0.008*\"brand_new\" + 0.008*\"condition\"\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7641/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #7 (0.067): 0.034*\"size\" + 0.032*\"woman\" + 0.031*\"shirt\" + 0.028*\"woman_top\" + 0.023*\"pink\" + 0.020*\"small\" + 0.015*\"victoria_secret\" + 0.015*\"blouse_t\" + 0.013*\"black\" + 0.012*\"medium\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7627/8000 documents converged within 50 iterations\n",
      "INFO : topic #1 (0.067): 0.023*\"bracelet\" + 0.023*\"jean\" + 0.023*\"woman\" + 0.019*\"size\" + 0.012*\"home\" + 0.011*\"pair\" + 0.010*\"black\" + 0.009*\"skinny\" + 0.009*\"woman_jewelry\" + 0.008*\"sock\"\n",
      "INFO : topic diff=0.040566, rho=0.081581\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7587/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 30\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 31\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 32\n",
      "DEBUG : 7630/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7661/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7598/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7598/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 27DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 29\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 30\n",
      "DEBUG : 7637/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7615/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.067): 0.032*\"size\" + 0.029*\"woman_athletic\" + 0.026*\"woman\" + 0.022*\"legging\" + 0.018*\"lularoe\" + 0.017*\"tight_legging\" + 0.017*\"apparel_pant\" + 0.015*\"jacket\" + 0.013*\"pink\" + 0.013*\"small\"\n",
      "INFO : topic #2 (0.067): 0.023*\"beauty_makeup\" + 0.017*\"color\" + 0.013*\"1\" + 0.013*\"new\" + 0.012*\"lip\" + 0.010*\"2\" + 0.010*\"skin\" + 0.010*\"brand_new\" + 0.009*\"eye\" + 0.009*\"rm\"\n",
      "INFO : topic #3 (0.067): 0.026*\"item\" + 0.012*\"1\" + 0.011*\"new\" + 0.009*\"rm\" + 0.009*\"shipping\" + 0.009*\"home_kitchen\" + 0.009*\"2\" + 0.009*\"rae\" + 0.008*\"3\" + 0.008*\"vintage_collectible\"\n",
      "INFO : topic #9 (0.067): 0.027*\"woman\" + 0.017*\"man\" + 0.015*\"accessory\" + 0.014*\"bundle\" + 0.014*\"hair\" + 0.013*\"kid_toy\" + 0.011*\"beauty\" + 0.010*\"new\" + 0.009*\"brand_new\" + 0.008*\"box\"\n",
      "DEBUG : 7622/8000 documents converged within 50 iterations\n",
      "INFO : topic #13 (0.067): 0.043*\"man\" + 0.035*\"game\" + 0.030*\"shirt\" + 0.021*\"electronic_video\" + 0.020*\"game_console\" + 0.018*\"blouse\" + 0.017*\"pant\" + 0.015*\"size\" + 0.010*\"top_t\" + 0.010*\"new\"DEBUG : 7680/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7645/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.045239, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7678/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7656/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7670/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7629/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 29\n",
      "DEBUG : 7669/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7671/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7674/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.028*\"woman\" + 0.028*\"bag\" + 0.021*\"necklace\" + 0.017*\"woman_handbag\" + 0.014*\"gold\" + 0.010*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"inch\" + 0.009*\"purse\" + 0.009*\"brand_new\"\n",
      "INFO : topic #2 (0.067): 0.024*\"beauty_makeup\" + 0.017*\"color\" + 0.013*\"1\" + 0.013*\"new\" + 0.012*\"lip\" + 0.010*\"skin\" + 0.010*\"2\" + 0.010*\"brand_new\" + 0.009*\"eye\" + 0.009*\"rm\"\n",
      "INFO : topic #3 (0.067): 0.027*\"item\" + 0.012*\"1\" + 0.011*\"new\" + 0.010*\"shipping\" + 0.010*\"home_kitchen\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"rae\" + 0.008*\"3\" + 0.008*\"vintage_collectible\"\n",
      "INFO : topic #10 (0.067): 0.039*\"woman\" + 0.014*\"brand_new\" + 0.014*\"new\" + 0.014*\"beauty_fragrance\" + 0.013*\"size\" + 0.011*\"black\" + 0.010*\"perfume\" + 0.010*\"hat\" + 0.010*\"man\" + 0.008*\"\"\n",
      "INFO : topic #7 (0.067): 0.035*\"size\" + 0.032*\"woman\" + 0.032*\"shirt\" + 0.029*\"woman_top\" + 0.024*\"pink\" + 0.021*\"small\" + 0.016*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.013*\"black\" + 0.012*\"medium\"\n",
      "INFO : topic diff=0.046884, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 29\n",
      "DEBUG : 7675/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7665/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7676/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7679/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7687/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 29\n",
      "DEBUG : 7632/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7666/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7710/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7671/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7684/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.028*\"item\" + 0.012*\"1\" + 0.011*\"new\" + 0.010*\"home_kitchen\" + 0.010*\"shipping\" + 0.009*\"rm\" + 0.009*\"2\" + 0.009*\"rae\" + 0.008*\"3\" + 0.008*\"vintage_collectible\"\n",
      "INFO : topic #4 (0.067): 0.017*\"beauty_makeup\" + 0.016*\"baby\" + 0.016*\"3\" + 0.014*\"brand_new\" + 0.013*\"size\" + 0.013*\"new\" + 0.012*\"0_24\" + 0.012*\"2\" + 0.011*\"beauty_skin\" + 0.009*\"face\"\n",
      "INFO : topic #11 (0.067): 0.014*\"book\" + 0.012*\"nail\" + 0.011*\"kid_toy\" + 0.010*\"slime\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.008*\"beauty_makeup\" + 0.008*\"headphone\" + 0.007*\"rm\"\n",
      "INFO : topic #0 (0.067): 0.032*\"dress\" + 0.028*\"size\" + 0.020*\"ring\" + 0.014*\"woman\" + 0.013*\"woman_jewelry\" + 0.013*\"white\" + 0.012*\"black\" + 0.009*\"blue\" + 0.009*\"earring\" + 0.009*\"silver\"\n",
      "INFO : topic #12 (0.067): 0.034*\"rm\" + 0.018*\"bra\" + 0.015*\"home_dcor\" + 0.014*\"pink\" + 0.012*\"home\" + 0.010*\"card\" + 0.010*\"2\" + 0.010*\"new\" + 0.010*\"woman_underwear\" + 0.009*\"shipping\"\n",
      "DEBUG : 7695/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.048184, rho=0.081581\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7693/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7680/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7705/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7692/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 28\n",
      "DEBUG : 7702/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7731/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 30\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 30\n",
      "DEBUG : 7697/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7743/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7715/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7718/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7731/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.036*\"size\" + 0.032*\"shirt\" + 0.031*\"woman\" + 0.031*\"woman_top\" + 0.025*\"pink\" + 0.021*\"small\" + 0.016*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.013*\"black\" + 0.013*\"medium\"\n",
      "INFO : topic #13 (0.067): 0.047*\"man\" + 0.038*\"game\" + 0.032*\"shirt\" + 0.023*\"electronic_video\" + 0.021*\"game_console\" + 0.017*\"pant\" + 0.016*\"blouse\" + 0.015*\"size\" + 0.011*\"top_t\" + 0.010*\"new\"\n",
      "INFO : topic #10 (0.067): 0.040*\"woman\" + 0.015*\"beauty_fragrance\" + 0.014*\"brand_new\" + 0.014*\"new\" + 0.012*\"size\" + 0.011*\"perfume\" + 0.011*\"hat\" + 0.011*\"black\" + 0.010*\"man\" + 0.008*\"\"\n",
      "INFO : topic #0 (0.067): 0.033*\"dress\" + 0.028*\"size\" + 0.020*\"ring\" + 0.014*\"woman\" + 0.013*\"woman_jewelry\" + 0.012*\"white\" + 0.012*\"black\" + 0.010*\"earring\" + 0.009*\"blue\" + 0.009*\"silver\"\n",
      "INFO : topic #12 (0.067): 0.035*\"rm\" + 0.018*\"bra\" + 0.015*\"home_dcor\" + 0.014*\"pink\" + 0.012*\"home\" + 0.010*\"card\" + 0.010*\"2\" + 0.010*\"new\" + 0.010*\"woman_underwear\" + 0.009*\"shipping\"\n",
      "DEBUG : 7680/8000 documents converged within 50 iterations\n",
      "INFO : topic diff=0.048363, rho=0.081581DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 29\n",
      "DEBUG : 7726/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7696/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7725/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7688/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 28\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 29\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7735/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7734/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7721/8000 documents converged within 50 iterations\n",
      "DEBUG : 7704/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7721/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.035*\"rm\" + 0.019*\"bra\" + 0.015*\"home_dcor\" + 0.014*\"pink\" + 0.012*\"home\" + 0.010*\"card\" + 0.010*\"2\" + 0.010*\"woman_underwear\" + 0.010*\"new\" + 0.009*\"shipping\"\n",
      "INFO : topic #1 (0.067): 0.028*\"jean\" + 0.026*\"bracelet\" + 0.024*\"woman\" + 0.020*\"size\" + 0.013*\"home\" + 0.012*\"skinny\" + 0.011*\"pair\" + 0.010*\"woman_jewelry\" + 0.009*\"sock\" + 0.009*\"jean_slim\"\n",
      "INFO : topic #13 (0.067): 0.048*\"man\" + 0.038*\"game\" + 0.032*\"shirt\" + 0.023*\"electronic_video\" + 0.022*\"game_console\" + 0.017*\"pant\" + 0.016*\"blouse\" + 0.015*\"size\" + 0.011*\"top_t\" + 0.010*\"new\"\n",
      "INFO : topic #7 (0.067): 0.036*\"size\" + 0.033*\"shirt\" + 0.031*\"woman_top\" + 0.030*\"woman\" + 0.025*\"pink\" + 0.021*\"small\" + 0.016*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.013*\"black\" + 0.013*\"medium\"\n",
      "INFO : topic #2 (0.067): 0.027*\"beauty_makeup\" + 0.017*\"color\" + 0.013*\"lip\" + 0.013*\"new\" + 0.013*\"1\" + 0.011*\"brand_new\" + 0.010*\"skin\" + 0.010*\"eye\" + 0.010*\"2\" + 0.010*\"face\"\n",
      "DEBUG : 7711/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.048835, rho=0.081581\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 28\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7713/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7756/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7725/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 28\n",
      "DEBUG : 7737/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7759/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7744/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7729/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7750/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.067): 0.048*\"size\" + 0.045*\"woman\" + 0.030*\"shoe\" + 0.016*\"short\" + 0.012*\"black\" + 0.012*\"new\" + 0.011*\"man\" + 0.010*\"athletic\" + 0.009*\"brand_new\" + 0.009*\"boot\"\n",
      "INFO : topic #3 (0.067): 0.030*\"item\" + 0.012*\"1\" + 0.011*\"new\" + 0.011*\"home_kitchen\" + 0.010*\"shipping\" + 0.010*\"2\" + 0.010*\"rae\" + 0.009*\"rm\" + 0.008*\"vintage_collectible\" + 0.008*\"3\"\n",
      "INFO : topic #6 (0.067): 0.032*\"bag\" + 0.032*\"woman\" + 0.022*\"necklace\" + 0.019*\"woman_handbag\" + 0.014*\"gold\" + 0.012*\"purse\" + 0.010*\"woman_jewelry\" + 0.010*\"new\" + 0.009*\"inch\" + 0.009*\"pocket\"\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #8 (0.067): 0.033*\"case\" + 0.022*\"phone_accessory\" + 0.021*\"electronic_cell\" + 0.016*\"new\" + 0.016*\"accessory\" + 0.012*\"brand_new\" + 0.012*\"phone\" + 0.012*\"iphone\" + 0.010*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #0 (0.067): 0.036*\"dress\" + 0.028*\"size\" + 0.021*\"ring\" + 0.015*\"woman\" + 0.014*\"woman_jewelry\" + 0.012*\"white\" + 0.012*\"black\" + 0.011*\"earring\" + 0.009*\"blue\" + 0.009*\"silver\"\n",
      "INFO : topic diff=0.048945, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 28\n",
      "DEBUG : 7743/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7729/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7764/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7712/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7705/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 30\n",
      "DEBUG : 7742/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 31\n",
      "DEBUG : 7762/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7731/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7724/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7761/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7744/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7719/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #9 (0.067): 0.025*\"woman\" + 0.018*\"kid_toy\" + 0.016*\"accessory\" + 0.016*\"hair\" + 0.015*\"man\" + 0.013*\"bundle\" + 0.012*\"beauty\" + 0.010*\"new\" + 0.009*\"box\" + 0.009*\"vintage_collectible\"\n",
      "INFO : topic #7 (0.067): 0.036*\"size\" + 0.033*\"shirt\" + 0.032*\"woman_top\" + 0.030*\"woman\" + 0.026*\"pink\" + 0.021*\"small\" + 0.017*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.013*\"black\" + 0.013*\"blouse\"\n",
      "INFO : topic #14 (0.067): 0.048*\"size\" + 0.045*\"woman\" + 0.031*\"shoe\" + 0.017*\"short\" + 0.012*\"black\" + 0.012*\"new\" + 0.011*\"man\" + 0.010*\"athletic\" + 0.009*\"boot\" + 0.009*\"brand_new\"\n",
      "INFO : topic #6 (0.067): 0.033*\"bag\" + 0.033*\"woman\" + 0.022*\"necklace\" + 0.020*\"woman_handbag\" + 0.014*\"gold\" + 0.012*\"purse\" + 0.010*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"inch\" + 0.009*\"pocket\"\n",
      "INFO : topic #10 (0.067): 0.041*\"woman\" + 0.016*\"beauty_fragrance\" + 0.014*\"new\" + 0.014*\"brand_new\" + 0.012*\"size\" + 0.012*\"hat\" + 0.011*\"man\" + 0.011*\"perfume\" + 0.010*\"black\" + 0.009*\"\"\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7712/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.049026, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7733/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7775/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7738/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7783/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7724/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7759/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7745/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7753/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.036*\"rm\" + 0.019*\"bra\" + 0.016*\"home_dcor\" + 0.014*\"pink\" + 0.013*\"home\" + 0.011*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.010*\"shipping\" + 0.009*\"new\"\n",
      "INFO : topic #0 (0.067): 0.038*\"dress\" + 0.028*\"size\" + 0.021*\"ring\" + 0.015*\"woman\" + 0.014*\"woman_jewelry\" + 0.012*\"black\" + 0.012*\"white\" + 0.012*\"earring\" + 0.010*\"dress_knee\" + 0.009*\"blue\"\n",
      "INFO : topic #2 (0.067): 0.029*\"beauty_makeup\" + 0.018*\"color\" + 0.014*\"lip\" + 0.013*\"new\" + 0.012*\"1\" + 0.011*\"brand_new\" + 0.011*\"eye\" + 0.011*\"skin\" + 0.010*\"face\" + 0.010*\"2\"\n",
      "INFO : topic #14 (0.067): 0.049*\"size\" + 0.046*\"woman\" + 0.031*\"shoe\" + 0.017*\"short\" + 0.012*\"black\" + 0.012*\"new\" + 0.012*\"man\" + 0.010*\"athletic\" + 0.010*\"boot\" + 0.009*\"brand_new\"\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #1 (0.067): 0.031*\"jean\" + 0.027*\"bracelet\" + 0.024*\"woman\" + 0.020*\"size\" + 0.014*\"home\" + 0.013*\"skinny\" + 0.012*\"pair\" + 0.010*\"sock\" + 0.010*\"woman_jewelry\" + 0.010*\"jean_slim\"\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.049506, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 19\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7755/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7762/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7751/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 14\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 30\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7731/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7753/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7759/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7781/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7746/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.067): 0.018*\"baby\" + 0.017*\"3\" + 0.015*\"0_24\" + 0.014*\"beauty_makeup\" + 0.014*\"brand_new\" + 0.014*\"new\" + 0.013*\"size\" + 0.013*\"2\" + 0.012*\"beauty_skin\" + 0.010*\"kid_girl\"\n",
      "INFO : topic #7 (0.067): 0.037*\"size\" + 0.034*\"shirt\" + 0.033*\"woman_top\" + 0.029*\"woman\" + 0.026*\"pink\" + 0.021*\"small\" + 0.017*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.014*\"blouse\" + 0.014*\"black\"\n",
      "INFO : topic #9 (0.067): 0.024*\"woman\" + 0.019*\"kid_toy\" + 0.017*\"accessory\" + 0.017*\"hair\" + 0.014*\"man\" + 0.013*\"bundle\" + 0.012*\"beauty\" + 0.010*\"new\" + 0.010*\"box\" + 0.010*\"vintage_collectible\"\n",
      "INFO : topic #3 (0.067): 0.032*\"item\" + 0.012*\"1\" + 0.011*\"home_kitchen\" + 0.011*\"shipping\" + 0.011*\"new\" + 0.010*\"rae\" + 0.010*\"2\" + 0.009*\"rm\" + 0.008*\"vintage_collectible\" + 0.008*\"3\"\n",
      "INFO : topic #11 (0.067): 0.016*\"book\" + 0.014*\"kid_toy\" + 0.012*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.008*\"headphone\" + 0.007*\"kid\" + 0.007*\"beauty_makeup\"\n",
      "INFO : topic diff=0.050702, rho=0.081581\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 26\n",
      "DEBUG : 7767/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7746/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7772/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 27\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 28\n",
      "DEBUG : 7739/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7777/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7752/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7757/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7776/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.067): 0.018*\"baby\" + 0.018*\"3\" + 0.015*\"0_24\" + 0.014*\"beauty_makeup\" + 0.014*\"new\" + 0.014*\"brand_new\" + 0.013*\"size\" + 0.013*\"2\" + 0.012*\"beauty_skin\" + 0.011*\"kid_girl\"\n",
      "INFO : topic #14 (0.067): 0.049*\"size\" + 0.046*\"woman\" + 0.032*\"shoe\" + 0.017*\"short\" + 0.012*\"black\" + 0.012*\"man\" + 0.012*\"new\" + 0.010*\"athletic\" + 0.010*\"boot\" + 0.009*\"8\"\n",
      "INFO : topic #6 (0.067): 0.035*\"woman\" + 0.035*\"bag\" + 0.022*\"necklace\" + 0.021*\"woman_handbag\" + 0.014*\"gold\" + 0.013*\"purse\" + 0.010*\"new\" + 0.010*\"woman_jewelry\" + 0.010*\"pocket\" + 0.009*\"inch\"\n",
      "INFO : topic #11 (0.067): 0.016*\"book\" + 0.014*\"kid_toy\" + 0.013*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.008*\"headphone\" + 0.008*\"kid\" + 0.006*\"beauty_makeup\"\n",
      "INFO : topic #0 (0.067): 0.041*\"dress\" + 0.028*\"size\" + 0.021*\"ring\" + 0.016*\"woman\" + 0.014*\"woman_jewelry\" + 0.012*\"earring\" + 0.012*\"black\" + 0.012*\"white\" + 0.011*\"dress_knee\" + 0.009*\"mini\"\n",
      "INFO : topic diff=0.050558, rho=0.081581\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #133 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7767/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7768/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7808/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7780/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7794/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7774/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.042*\"dress\" + 0.028*\"size\" + 0.021*\"ring\" + 0.016*\"woman\" + 0.015*\"woman_jewelry\" + 0.013*\"earring\" + 0.012*\"black\" + 0.011*\"white\" + 0.011*\"dress_knee\" + 0.010*\"mini\"\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #1 (0.067): 0.033*\"jean\" + 0.028*\"bracelet\" + 0.025*\"woman\" + 0.021*\"size\" + 0.015*\"home\" + 0.014*\"skinny\" + 0.012*\"pair\" + 0.011*\"sock\" + 0.010*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #10 (0.067): 0.043*\"woman\" + 0.017*\"beauty_fragrance\" + 0.015*\"new\" + 0.014*\"brand_new\" + 0.013*\"hat\" + 0.012*\"man\" + 0.012*\"perfume\" + 0.011*\"size\" + 0.010*\"black\" + 0.009*\"jersey\"\n",
      "INFO : topic #11 (0.067): 0.016*\"book\" + 0.014*\"kid_toy\" + 0.012*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.008*\"kid\" + 0.008*\"headphone\" + 0.006*\"beauty_makeup\"\n",
      "INFO : topic #5 (0.067): 0.034*\"woman_athletic\" + 0.034*\"size\" + 0.026*\"woman\" + 0.026*\"legging\" + 0.022*\"lularoe\" + 0.019*\"tight_legging\" + 0.019*\"apparel_pant\" + 0.016*\"jacket\" + 0.014*\"pink\" + 0.013*\"black\"\n",
      "INFO : topic diff=0.051010, rho=0.081581\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7773/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7815/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : 7778/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7784/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1979/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7797/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : 7798/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.067): 0.035*\"case\" + 0.023*\"phone_accessory\" + 0.023*\"electronic_cell\" + 0.016*\"accessory\" + 0.015*\"new\" + 0.013*\"phone\" + 0.012*\"iphone\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #1 (0.067): 0.034*\"jean\" + 0.028*\"bracelet\" + 0.025*\"woman\" + 0.021*\"size\" + 0.015*\"home\" + 0.014*\"skinny\" + 0.012*\"pair\" + 0.011*\"sock\" + 0.010*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #5 (0.067): 0.035*\"woman_athletic\" + 0.034*\"size\" + 0.026*\"legging\" + 0.026*\"woman\" + 0.023*\"lularoe\" + 0.020*\"tight_legging\" + 0.019*\"apparel_pant\" + 0.016*\"jacket\" + 0.014*\"pink\" + 0.013*\"black\"\n",
      "INFO : topic #3 (0.067): 0.033*\"item\" + 0.012*\"1\" + 0.012*\"shipping\" + 0.012*\"home_kitchen\" + 0.011*\"new\" + 0.011*\"rae\" + 0.010*\"2\" + 0.009*\"rm\" + 0.008*\"vintage_collectible\" + 0.008*\"3\"\n",
      "INFO : topic #0 (0.067): 0.043*\"dress\" + 0.028*\"size\" + 0.022*\"ring\" + 0.017*\"woman\" + 0.015*\"woman_jewelry\" + 0.013*\"earring\" + 0.012*\"black\" + 0.012*\"dress_knee\" + 0.011*\"white\" + 0.010*\"mini\"\n",
      "INFO : topic diff=0.049801, rho=0.081581\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 16000 documents into a model of 1186028 documents\n",
      "INFO : topic #8 (0.067): 0.035*\"case\" + 0.023*\"phone_accessory\" + 0.023*\"electronic_cell\" + 0.016*\"accessory\" + 0.014*\"new\" + 0.013*\"phone\" + 0.013*\"iphone\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #0 (0.067): 0.044*\"dress\" + 0.028*\"size\" + 0.022*\"ring\" + 0.017*\"woman\" + 0.015*\"woman_jewelry\" + 0.014*\"earring\" + 0.012*\"black\" + 0.012*\"dress_knee\" + 0.011*\"white\" + 0.011*\"mini\"\n",
      "INFO : topic #1 (0.067): 0.035*\"jean\" + 0.028*\"bracelet\" + 0.025*\"woman\" + 0.021*\"size\" + 0.015*\"home\" + 0.015*\"skinny\" + 0.012*\"pair\" + 0.011*\"sock\" + 0.011*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #10 (0.067): 0.043*\"woman\" + 0.017*\"beauty_fragrance\" + 0.015*\"new\" + 0.014*\"brand_new\" + 0.013*\"man\" + 0.013*\"hat\" + 0.012*\"perfume\" + 0.011*\"size\" + 0.010*\"jersey\" + 0.010*\"\"\n",
      "INFO : topic #7 (0.067): 0.038*\"size\" + 0.035*\"shirt\" + 0.035*\"woman_top\" + 0.028*\"woman\" + 0.027*\"pink\" + 0.021*\"small\" + 0.017*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.017*\"blouse\" + 0.014*\"black\"\n",
      "INFO : topic diff=0.040354, rho=0.081581\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.296 per-word bound, 157.1 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : 7787/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7805/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 29\n",
      "DEBUG : 7765/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 29DEBUG : 7769/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 29DEBUG : processing chunk #11 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 30\n",
      "DEBUG : 7794/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7771/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 30\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7787/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 31\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7797/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7790/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7782/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7801/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.045*\"dress\" + 0.028*\"size\" + 0.022*\"ring\" + 0.017*\"woman\" + 0.015*\"woman_jewelry\" + 0.014*\"earring\" + 0.013*\"dress_knee\" + 0.012*\"black\" + 0.011*\"white\" + 0.011*\"mini\"\n",
      "INFO : topic #4 (0.067): 0.020*\"baby\" + 0.018*\"3\" + 0.017*\"0_24\" + 0.014*\"2\" + 0.014*\"new\" + 0.013*\"size\" + 0.013*\"brand_new\" + 0.012*\"kid_girl\" + 0.012*\"beauty_skin\" + 0.011*\"kid_boy\"\n",
      "INFO : topic #14 (0.067): 0.051*\"size\" + 0.047*\"woman\" + 0.034*\"shoe\" + 0.018*\"short\" + 0.013*\"black\" + 0.013*\"man\" + 0.012*\"new\" + 0.011*\"athletic\" + 0.011*\"boot\" + 0.009*\"8\"\n",
      "INFO : topic #11 (0.067): 0.017*\"book\" + 0.016*\"kid_toy\" + 0.013*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"kid\" + 0.009*\"brand_new\" + 0.007*\"headphone\" + 0.006*\"art_craft\"\n",
      "INFO : topic #5 (0.067): 0.035*\"woman_athletic\" + 0.034*\"size\" + 0.027*\"legging\" + 0.026*\"woman\" + 0.023*\"lularoe\" + 0.020*\"tight_legging\" + 0.020*\"apparel_pant\" + 0.016*\"jacket\" + 0.014*\"pink\" + 0.013*\"black\"\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "INFO : topic diff=0.041086, rho=0.081311\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 29\n",
      "DEBUG : 7779/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7772/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7793/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 27\n",
      "DEBUG : 7803/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 28\n",
      "DEBUG : 7787/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 29\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7815/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #10 (0.067): 0.044*\"woman\" + 0.018*\"beauty_fragrance\" + 0.015*\"new\" + 0.014*\"brand_new\" + 0.013*\"man\" + 0.013*\"hat\" + 0.013*\"perfume\" + 0.011*\"size\" + 0.011*\"jersey\" + 0.010*\"\"\n",
      "INFO : topic #14 (0.067): 0.051*\"size\" + 0.047*\"woman\" + 0.035*\"shoe\" + 0.018*\"short\" + 0.013*\"black\" + 0.013*\"man\" + 0.012*\"new\" + 0.011*\"athletic\" + 0.011*\"boot\" + 0.010*\"8\"\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7809/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #5 (0.067): 0.036*\"woman_athletic\" + 0.034*\"size\" + 0.027*\"legging\" + 0.026*\"woman\" + 0.024*\"lularoe\" + 0.020*\"tight_legging\" + 0.020*\"apparel_pant\" + 0.016*\"jacket\" + 0.014*\"pink\" + 0.014*\"black\"\n",
      "INFO : topic #1 (0.067): 0.036*\"jean\" + 0.029*\"bracelet\" + 0.025*\"woman\" + 0.021*\"size\" + 0.015*\"home\" + 0.015*\"skinny\" + 0.012*\"pair\" + 0.011*\"sock\" + 0.011*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #6 (0.067): 0.039*\"woman\" + 0.038*\"bag\" + 0.022*\"woman_handbag\" + 0.022*\"necklace\" + 0.015*\"purse\" + 0.013*\"gold\" + 0.010*\"pocket\" + 0.010*\"new\" + 0.010*\"woman_jewelry\" + 0.009*\"inch\"\n",
      "INFO : topic diff=0.043248, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 27\n",
      "DEBUG : 7807/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7808/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7793/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7792/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7791/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7796/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7799/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7816/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7808/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.039*\"rm\" + 0.021*\"bra\" + 0.017*\"home_dcor\" + 0.015*\"home\" + 0.014*\"pink\" + 0.012*\"shipping\" + 0.011*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.009*\"1\"\n",
      "INFO : topic #11 (0.067): 0.017*\"book\" + 0.016*\"kid_toy\" + 0.013*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.009*\"kid\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.007*\"headphone\" + 0.007*\"art_craft\"\n",
      "INFO : topic #6 (0.067): 0.040*\"woman\" + 0.038*\"bag\" + 0.022*\"woman_handbag\" + 0.022*\"necklace\" + 0.015*\"purse\" + 0.013*\"gold\" + 0.011*\"pocket\" + 0.010*\"new\" + 0.009*\"woman_jewelry\" + 0.009*\"inch\"\n",
      "INFO : topic #2 (0.067): 0.034*\"beauty_makeup\" + 0.017*\"color\" + 0.016*\"lip\" + 0.013*\"new\" + 0.013*\"brand_new\" + 0.013*\"face\" + 0.013*\"eye\" + 0.011*\"1\" + 0.011*\"skin\" + 0.010*\"makeup\"\n",
      "INFO : topic #9 (0.067): 0.023*\"kid_toy\" + 0.021*\"woman\" + 0.019*\"hair\" + 0.019*\"accessory\" + 0.013*\"beauty\" + 0.012*\"bundle\" + 0.011*\"vintage_collectible\" + 0.011*\"man\" + 0.011*\"box\" + 0.010*\"new\"\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.043157, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 29\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7811/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 29\n",
      "DEBUG : 7800/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7795/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.067): 0.061*\"man\" + 0.045*\"game\" + 0.036*\"shirt\" + 0.027*\"electronic_video\" + 0.026*\"game_console\" + 0.017*\"pant\" + 0.014*\"size\" + 0.013*\"top_t\" + 0.010*\"new\" + 0.008*\"2\"\n",
      "INFO : topic #1 (0.067): 0.037*\"jean\" + 0.029*\"bracelet\" + 0.026*\"woman\" + 0.022*\"size\" + 0.016*\"home\" + 0.016*\"skinny\" + 0.012*\"pair\" + 0.011*\"sock\" + 0.011*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #0 (0.067): 0.047*\"dress\" + 0.028*\"size\" + 0.022*\"ring\" + 0.018*\"woman\" + 0.016*\"woman_jewelry\" + 0.015*\"earring\" + 0.014*\"dress_knee\" + 0.012*\"black\" + 0.012*\"mini\" + 0.011*\"white\"\n",
      "INFO : topic #2 (0.067): 0.034*\"beauty_makeup\" + 0.018*\"color\" + 0.016*\"lip\" + 0.013*\"new\" + 0.013*\"brand_new\" + 0.013*\"face\" + 0.013*\"eye\" + 0.011*\"1\" + 0.011*\"skin\" + 0.010*\"makeup\"\n",
      "INFO : topic #8 (0.067): 0.036*\"case\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"new\" + 0.014*\"phone\" + 0.013*\"iphone\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic diff=0.043291, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 29DEBUG : processing chunk #56 of 8000 documents\n",
      "\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7804/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7814/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7812/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 28\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7806/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7820/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.067): 0.037*\"case\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.016*\"accessory\" + 0.014*\"new\" + 0.014*\"phone\" + 0.013*\"iphone\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #3 (0.067): 0.037*\"item\" + 0.013*\"shipping\" + 0.012*\"home_kitchen\" + 0.012*\"1\" + 0.011*\"new\" + 0.011*\"rae\" + 0.011*\"2\" + 0.009*\"rm\" + 0.009*\"day\" + 0.008*\"brand_new\"\n",
      "INFO : topic #11 (0.067): 0.018*\"book\" + 0.017*\"kid_toy\" + 0.013*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.010*\"kid\" + 0.009*\"2\" + 0.009*\"brand_new\" + 0.007*\"headphone\" + 0.007*\"art_craft\"\n",
      "INFO : topic #1 (0.067): 0.038*\"jean\" + 0.029*\"bracelet\" + 0.026*\"woman\" + 0.022*\"size\" + 0.016*\"home\" + 0.016*\"skinny\" + 0.012*\"pair\" + 0.012*\"sock\" + 0.011*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #6 (0.067): 0.041*\"woman\" + 0.039*\"bag\" + 0.023*\"woman_handbag\" + 0.022*\"necklace\" + 0.015*\"purse\" + 0.013*\"gold\" + 0.011*\"pocket\" + 0.010*\"new\" + 0.009*\"woman_jewelry\" + 0.009*\"inch\"\n",
      "INFO : topic diff=0.042783, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7820/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #71 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 29\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7824/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.067): 0.052*\"size\" + 0.047*\"woman\" + 0.036*\"shoe\" + 0.019*\"short\" + 0.014*\"man\" + 0.014*\"black\" + 0.012*\"new\" + 0.012*\"boot\" + 0.011*\"athletic\" + 0.010*\"8\"\n",
      "INFO : topic #7 (0.067): 0.039*\"size\" + 0.036*\"woman_top\" + 0.036*\"shirt\" + 0.027*\"pink\" + 0.026*\"woman\" + 0.021*\"small\" + 0.019*\"blouse\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.014*\"medium\"\n",
      "INFO : topic #13 (0.067): 0.063*\"man\" + 0.046*\"game\" + 0.037*\"shirt\" + 0.028*\"electronic_video\" + 0.026*\"game_console\" + 0.017*\"pant\" + 0.014*\"size\" + 0.013*\"top_t\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "INFO : topic #9 (0.067): 0.024*\"kid_toy\" + 0.020*\"hair\" + 0.019*\"accessory\" + 0.019*\"woman\" + 0.013*\"beauty\" + 0.012*\"vintage_collectible\" + 0.011*\"bundle\" + 0.011*\"box\" + 0.011*\"doll_accessory\" + 0.010*\"new\"\n",
      "INFO : topic #12 (0.067): 0.040*\"rm\" + 0.021*\"bra\" + 0.018*\"home_dcor\" + 0.015*\"home\" + 0.014*\"pink\" + 0.012*\"shipping\" + 0.011*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.009*\"1\"\n",
      "INFO : topic diff=0.043011, rho=0.081311\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 29\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 22\n",
      "DEBUG : 7830/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 29\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "DEBUG : 7818/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7819/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7789/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #8 (0.067): 0.037*\"case\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"phone\" + 0.013*\"new\" + 0.013*\"iphone\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #12 (0.067): 0.040*\"rm\" + 0.021*\"bra\" + 0.018*\"home_dcor\" + 0.016*\"home\" + 0.014*\"pink\" + 0.012*\"shipping\" + 0.011*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.009*\"1\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.038*\"item\" + 0.014*\"shipping\" + 0.013*\"home_kitchen\" + 0.012*\"1\" + 0.011*\"rae\" + 0.011*\"new\" + 0.011*\"2\" + 0.010*\"day\" + 0.009*\"rm\" + 0.008*\"brand_new\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #10 (0.067): 0.045*\"woman\" + 0.018*\"beauty_fragrance\" + 0.016*\"new\" + 0.014*\"man\" + 0.014*\"hat\" + 0.014*\"brand_new\" + 0.013*\"perfume\" + 0.011*\"jersey\" + 0.011*\"\" + 0.010*\"size\"\n",
      "INFO : topic #5 (0.067): 0.037*\"woman_athletic\" + 0.034*\"size\" + 0.028*\"legging\" + 0.026*\"woman\" + 0.025*\"lularoe\" + 0.021*\"tight_legging\" + 0.021*\"apparel_pant\" + 0.017*\"jacket\" + 0.014*\"pink\" + 0.014*\"black\"\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.042939, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 27\n",
      "DEBUG : 7814/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7827/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 23DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7813/8000 documents converged within 50 iterations\n",
      "\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 26\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 28\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 29\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #11 (0.067): 0.018*\"book\" + 0.018*\"kid_toy\" + 0.013*\"nail\" + 0.011*\"slime\" + 0.010*\"new\" + 0.010*\"kid\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.007*\"headphone\" + 0.007*\"art_craft\"\n",
      "INFO : topic #8 (0.067): 0.037*\"case\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"phone\" + 0.013*\"iphone\" + 0.013*\"new\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #3 (0.067): 0.039*\"item\" + 0.014*\"shipping\" + 0.013*\"home_kitchen\" + 0.012*\"1\" + 0.011*\"rae\" + 0.011*\"new\" + 0.011*\"2\" + 0.010*\"day\" + 0.009*\"rm\" + 0.008*\"brand_new\"\n",
      "DEBUG : 7834/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #9 (0.067): 0.024*\"kid_toy\" + 0.021*\"hair\" + 0.020*\"accessory\" + 0.018*\"woman\" + 0.013*\"beauty\" + 0.012*\"vintage_collectible\" + 0.011*\"box\" + 0.011*\"bundle\" + 0.011*\"doll_accessory\" + 0.011*\"action_figure\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #2 (0.067): 0.036*\"beauty_makeup\" + 0.018*\"color\" + 0.016*\"lip\" + 0.014*\"face\" + 0.014*\"brand_new\" + 0.013*\"new\" + 0.013*\"eye\" + 0.011*\"skin\" + 0.011*\"1\" + 0.010*\"makeup\"\n",
      "INFO : topic diff=0.042023, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7826/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7829/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 27\n",
      "DEBUG : 7823/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 2, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 28\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7825/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #12 (0.067): 0.041*\"rm\" + 0.021*\"bra\" + 0.018*\"home_dcor\" + 0.016*\"home\" + 0.013*\"pink\" + 0.012*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.009*\"1\"\n",
      "INFO : topic #1 (0.067): 0.040*\"jean\" + 0.030*\"bracelet\" + 0.026*\"woman\" + 0.023*\"size\" + 0.016*\"skinny\" + 0.016*\"home\" + 0.012*\"pair\" + 0.012*\"sock\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #0 (0.067): 0.050*\"dress\" + 0.028*\"size\" + 0.022*\"ring\" + 0.020*\"woman\" + 0.016*\"woman_jewelry\" + 0.016*\"earring\" + 0.015*\"dress_knee\" + 0.014*\"mini\" + 0.012*\"black\" + 0.012*\"length\"\n",
      "INFO : topic #4 (0.067): 0.022*\"baby\" + 0.020*\"3\" + 0.019*\"0_24\" + 0.015*\"kid_girl\" + 0.015*\"2\" + 0.014*\"size\" + 0.013*\"new\" + 0.013*\"mo\" + 0.013*\"beauty_skin\" + 0.012*\"kid_boy\"\n",
      "DEBUG : 7817/8000 documents converged within 50 iterations\n",
      "INFO : topic #8 (0.067): 0.037*\"case\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"phone\" + 0.013*\"iphone\" + 0.013*\"new\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.042304, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 28\n",
      "DEBUG : 7820/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 29\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7829/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7840/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.044*\"woman\" + 0.041*\"bag\" + 0.023*\"woman_handbag\" + 0.021*\"necklace\" + 0.016*\"purse\" + 0.013*\"gold\" + 0.011*\"pocket\" + 0.010*\"new\" + 0.009*\"leather\" + 0.009*\"black\"\n",
      "INFO : topic #9 (0.067): 0.025*\"kid_toy\" + 0.021*\"hair\" + 0.020*\"accessory\" + 0.018*\"woman\" + 0.013*\"beauty\" + 0.013*\"vintage_collectible\" + 0.011*\"box\" + 0.011*\"doll_accessory\" + 0.011*\"action_figure\" + 0.011*\"bundle\"\n",
      "INFO : topic #12 (0.067): 0.041*\"rm\" + 0.022*\"bra\" + 0.018*\"home_dcor\" + 0.016*\"home\" + 0.013*\"pink\" + 0.012*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.009*\"1\"\n",
      "INFO : topic #7 (0.067): 0.040*\"size\" + 0.037*\"woman_top\" + 0.036*\"shirt\" + 0.028*\"pink\" + 0.026*\"woman\" + 0.021*\"small\" + 0.020*\"blouse\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.014*\"medium\"\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #3 (0.067): 0.040*\"item\" + 0.015*\"shipping\" + 0.013*\"home_kitchen\" + 0.012*\"1\" + 0.011*\"rae\" + 0.011*\"new\" + 0.011*\"2\" + 0.010*\"day\" + 0.009*\"rm\" + 0.008*\"brand_new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.041930, rho=0.081311\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7824/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 25\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7845/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "INFO : topic #8 (0.067): 0.038*\"case\" + 0.024*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"phone\" + 0.013*\"iphone\" + 0.013*\"new\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #12 (0.067): 0.041*\"rm\" + 0.022*\"bra\" + 0.018*\"home_dcor\" + 0.016*\"home\" + 0.013*\"pink\" + 0.012*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.010*\"woman_underwear\" + 0.009*\"1\"\n",
      "INFO : topic #2 (0.067): 0.037*\"beauty_makeup\" + 0.017*\"color\" + 0.017*\"lip\" + 0.014*\"face\" + 0.014*\"brand_new\" + 0.014*\"eye\" + 0.014*\"new\" + 0.011*\"skin\" + 0.010*\"1\" + 0.010*\"makeup\"\n",
      "INFO : topic #1 (0.067): 0.041*\"jean\" + 0.030*\"bracelet\" + 0.027*\"woman\" + 0.023*\"size\" + 0.017*\"skinny\" + 0.016*\"home\" + 0.012*\"sock\" + 0.012*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #11 (0.067): 0.018*\"book\" + 0.018*\"kid_toy\" + 0.013*\"nail\" + 0.011*\"slime\" + 0.011*\"kid\" + 0.011*\"new\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.007*\"headphone\" + 0.007*\"disney\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.040189, rho=0.081311\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7829/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7829/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #4 (0.067): 0.022*\"baby\" + 0.020*\"3\" + 0.019*\"0_24\" + 0.016*\"kid_girl\" + 0.016*\"2\" + 0.014*\"size\" + 0.013*\"new\" + 0.013*\"mo\" + 0.013*\"kid_boy\" + 0.013*\"beauty_skin\"\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "INFO : topic #6 (0.067): 0.044*\"woman\" + 0.041*\"bag\" + 0.024*\"woman_handbag\" + 0.021*\"necklace\" + 0.016*\"purse\" + 0.012*\"gold\" + 0.011*\"pocket\" + 0.010*\"new\" + 0.010*\"leather\" + 0.009*\"black\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #8 (0.067): 0.038*\"case\" + 0.025*\"phone_accessory\" + 0.024*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"phone\" + 0.014*\"iphone\" + 0.013*\"new\" + 0.012*\"brand_new\" + 0.011*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #7 (0.067): 0.040*\"size\" + 0.037*\"woman_top\" + 0.036*\"shirt\" + 0.028*\"pink\" + 0.025*\"woman\" + 0.021*\"small\" + 0.020*\"blouse\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.014*\"medium\"\n",
      "INFO : topic #9 (0.067): 0.025*\"kid_toy\" + 0.022*\"hair\" + 0.020*\"accessory\" + 0.017*\"woman\" + 0.014*\"beauty\" + 0.013*\"vintage_collectible\" + 0.011*\"box\" + 0.011*\"doll_accessory\" + 0.011*\"action_figure\" + 0.011*\"statue\"\n",
      "INFO : topic diff=0.040499, rho=0.081311\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7831/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 1991/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "INFO : topic #8 (0.067): 0.038*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"accessory\" + 0.014*\"phone\" + 0.014*\"iphone\" + 0.013*\"new\" + 0.012*\"brand_new\" + 0.012*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #9 (0.067): 0.025*\"kid_toy\" + 0.022*\"hair\" + 0.021*\"accessory\" + 0.017*\"woman\" + 0.014*\"beauty\" + 0.014*\"vintage_collectible\" + 0.011*\"box\" + 0.011*\"doll_accessory\" + 0.011*\"action_figure\" + 0.011*\"statue\"\n",
      "INFO : topic #4 (0.067): 0.022*\"baby\" + 0.020*\"3\" + 0.020*\"0_24\" + 0.017*\"kid_girl\" + 0.016*\"2\" + 0.014*\"size\" + 0.013*\"mo\" + 0.013*\"new\" + 0.013*\"kid_boy\" + 0.013*\"beauty_skin\"\n",
      "INFO : topic #1 (0.067): 0.042*\"jean\" + 0.030*\"bracelet\" + 0.027*\"woman\" + 0.024*\"size\" + 0.017*\"skinny\" + 0.016*\"home\" + 0.013*\"sock\" + 0.012*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #14 (0.067): 0.053*\"size\" + 0.048*\"woman\" + 0.038*\"shoe\" + 0.020*\"short\" + 0.015*\"man\" + 0.014*\"black\" + 0.012*\"boot\" + 0.012*\"new\" + 0.012*\"athletic\" + 0.010*\"nike\"\n",
      "INFO : topic diff=0.038887, rho=0.081311\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.252 per-word bound, 152.4 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 28\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7843/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 29\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7837/8000 documents converged within 50 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 27\n",
      "DEBUG : 7852/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 28\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 28\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7849/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.042*\"item\" + 0.016*\"shipping\" + 0.013*\"home_kitchen\" + 0.012*\"1\" + 0.011*\"rae\" + 0.011*\"2\" + 0.011*\"new\" + 0.011*\"day\" + 0.010*\"rm\" + 0.009*\"brand_new\"\n",
      "INFO : topic #0 (0.067): 0.052*\"dress\" + 0.028*\"size\" + 0.022*\"ring\" + 0.021*\"woman\" + 0.017*\"woman_jewelry\" + 0.016*\"dress_knee\" + 0.016*\"earring\" + 0.015*\"mini\" + 0.013*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #12 (0.067): 0.042*\"rm\" + 0.022*\"bra\" + 0.018*\"home_dcor\" + 0.016*\"home\" + 0.013*\"pink\" + 0.012*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.009*\"woman_underwear\" + 0.009*\"1\"\n",
      "INFO : topic #7 (0.067): 0.040*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.025*\"woman\" + 0.021*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.014*\"medium\"\n",
      "INFO : topic #1 (0.067): 0.042*\"jean\" + 0.030*\"bracelet\" + 0.027*\"woman\" + 0.024*\"size\" + 0.017*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic diff=0.037123, rho=0.081043\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 28\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 28DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7849/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7839/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7852/8000 documents converged within 50 iterations\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7856/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.067): 0.039*\"woman_athletic\" + 0.034*\"size\" + 0.029*\"legging\" + 0.026*\"lularoe\" + 0.026*\"woman\" + 0.021*\"tight_legging\" + 0.021*\"apparel_pant\" + 0.017*\"jacket\" + 0.014*\"black\" + 0.014*\"pink\"\n",
      "INFO : topic #1 (0.067): 0.043*\"jean\" + 0.031*\"bracelet\" + 0.027*\"woman\" + 0.024*\"size\" + 0.017*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #14 (0.067): 0.053*\"size\" + 0.048*\"woman\" + 0.039*\"shoe\" + 0.020*\"short\" + 0.015*\"man\" + 0.014*\"black\" + 0.013*\"boot\" + 0.012*\"athletic\" + 0.012*\"new\" + 0.011*\"nike\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.021*\"3\" + 0.020*\"0_24\" + 0.017*\"kid_girl\" + 0.016*\"2\" + 0.014*\"size\" + 0.014*\"mo\" + 0.013*\"kid_boy\" + 0.013*\"new\" + 0.013*\"beauty_skin\"\n",
      "INFO : topic #12 (0.067): 0.043*\"rm\" + 0.022*\"bra\" + 0.018*\"home_dcor\" + 0.016*\"home\" + 0.013*\"pink\" + 0.013*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.009*\"woman_underwear\" + 0.009*\"1\"\n",
      "DEBUG : 7853/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.037866, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 26\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7853/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 20\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7849/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.040*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.025*\"woman\" + 0.021*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #5 (0.067): 0.039*\"woman_athletic\" + 0.034*\"size\" + 0.029*\"legging\" + 0.027*\"lularoe\" + 0.026*\"woman\" + 0.021*\"tight_legging\" + 0.021*\"apparel_pant\" + 0.017*\"jacket\" + 0.014*\"black\" + 0.014*\"pink\"\n",
      "INFO : topic #11 (0.067): 0.019*\"kid_toy\" + 0.018*\"book\" + 0.013*\"nail\" + 0.012*\"kid\" + 0.011*\"slime\" + 0.011*\"new\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.007*\"disney\" + 0.006*\"art_craft\"\n",
      "INFO : topic #0 (0.067): 0.053*\"dress\" + 0.027*\"size\" + 0.022*\"ring\" + 0.021*\"woman\" + 0.017*\"woman_jewelry\" + 0.017*\"dress_knee\" + 0.016*\"earring\" + 0.015*\"mini\" + 0.013*\"length\" + 0.012*\"black\"\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "INFO : topic #6 (0.067): 0.046*\"woman\" + 0.042*\"bag\" + 0.024*\"woman_handbag\" + 0.021*\"necklace\" + 0.017*\"purse\" + 0.012*\"gold\" + 0.012*\"pocket\" + 0.010*\"new\" + 0.010*\"leather\" + 0.009*\"wallet\"\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.037104, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 18\n",
      "DEBUG : 7838/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 28\n",
      "DEBUG : 7868/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 28\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7846/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7854/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7853/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.067): 0.047*\"woman\" + 0.019*\"beauty_fragrance\" + 0.016*\"new\" + 0.015*\"hat\" + 0.015*\"man\" + 0.014*\"brand_new\" + 0.014*\"perfume\" + 0.012*\"jersey\" + 0.012*\"\" + 0.011*\"accessory_hat\"\n",
      "INFO : topic #3 (0.067): 0.043*\"item\" + 0.016*\"shipping\" + 0.013*\"home_kitchen\" + 0.012*\"1\" + 0.012*\"day\" + 0.011*\"2\" + 0.011*\"rae\" + 0.011*\"new\" + 0.010*\"rm\" + 0.009*\"brand_new\"\n",
      "INFO : topic #14 (0.067): 0.054*\"size\" + 0.049*\"woman\" + 0.039*\"shoe\" + 0.020*\"short\" + 0.015*\"man\" + 0.014*\"black\" + 0.013*\"boot\" + 0.012*\"athletic\" + 0.012*\"new\" + 0.011*\"nike\"\n",
      "INFO : topic #13 (0.067): 0.073*\"man\" + 0.049*\"game\" + 0.039*\"shirt\" + 0.029*\"electronic_video\" + 0.028*\"game_console\" + 0.014*\"pant\" + 0.013*\"top_t\" + 0.013*\"size\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "INFO : topic #5 (0.067): 0.040*\"woman_athletic\" + 0.034*\"size\" + 0.029*\"legging\" + 0.027*\"lularoe\" + 0.026*\"woman\" + 0.022*\"tight_legging\" + 0.021*\"apparel_pant\" + 0.017*\"jacket\" + 0.015*\"black\" + 0.014*\"apparel\"\n",
      "INFO : topic diff=0.036874, rho=0.081043\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 25\n",
      "DEBUG : 7853/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 26\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 27\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 28\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 28DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7842/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7850/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7833/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #3 (0.067): 0.044*\"item\" + 0.017*\"shipping\" + 0.013*\"home_kitchen\" + 0.012*\"1\" + 0.012*\"day\" + 0.011*\"2\" + 0.011*\"rae\" + 0.011*\"new\" + 0.010*\"rm\" + 0.009*\"brand_new\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "INFO : topic #0 (0.067): 0.053*\"dress\" + 0.027*\"size\" + 0.022*\"ring\" + 0.022*\"woman\" + 0.018*\"woman_jewelry\" + 0.017*\"dress_knee\" + 0.017*\"earring\" + 0.016*\"mini\" + 0.014*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #1 (0.067): 0.044*\"jean\" + 0.031*\"bracelet\" + 0.027*\"woman\" + 0.024*\"size\" + 0.017*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #7 (0.067): 0.041*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.025*\"woman\" + 0.021*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #11 (0.067): 0.019*\"kid_toy\" + 0.018*\"book\" + 0.013*\"nail\" + 0.012*\"kid\" + 0.011*\"slime\" + 0.011*\"new\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.007*\"disney\" + 0.007*\"toy\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.035973, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7841/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 24\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 29\n",
      "DEBUG : 7832/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7851/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 29\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7835/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #8 (0.067): 0.039*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"accessory\" + 0.015*\"phone\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"brand_new\" + 0.012*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #14 (0.067): 0.054*\"size\" + 0.049*\"woman\" + 0.040*\"shoe\" + 0.021*\"short\" + 0.015*\"man\" + 0.015*\"black\" + 0.013*\"boot\" + 0.013*\"athletic\" + 0.012*\"new\" + 0.011*\"8\"\n",
      "INFO : topic #1 (0.067): 0.044*\"jean\" + 0.031*\"bracelet\" + 0.028*\"woman\" + 0.025*\"size\" + 0.017*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #11 (0.067): 0.019*\"kid_toy\" + 0.018*\"book\" + 0.013*\"nail\" + 0.012*\"kid\" + 0.011*\"slime\" + 0.011*\"new\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.007*\"disney\" + 0.007*\"toy\"\n",
      "INFO : topic #0 (0.067): 0.054*\"dress\" + 0.027*\"size\" + 0.022*\"ring\" + 0.022*\"woman\" + 0.018*\"woman_jewelry\" + 0.017*\"dress_knee\" + 0.016*\"earring\" + 0.016*\"mini\" + 0.014*\"length\" + 0.012*\"black\"\n",
      "INFO : topic diff=0.036123, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7856/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7863/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7852/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 26\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 30\n",
      "DEBUG : 7836/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7855/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.067): 0.040*\"woman_athletic\" + 0.034*\"size\" + 0.030*\"legging\" + 0.027*\"lularoe\" + 0.026*\"woman\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.018*\"jacket\" + 0.015*\"black\" + 0.015*\"apparel\"\n",
      "INFO : topic #12 (0.067): 0.044*\"rm\" + 0.022*\"bra\" + 0.019*\"home_dcor\" + 0.017*\"home\" + 0.013*\"pink\" + 0.013*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.009*\"1\" + 0.009*\"accent\"\n",
      "INFO : topic #0 (0.067): 0.054*\"dress\" + 0.027*\"size\" + 0.022*\"ring\" + 0.022*\"woman\" + 0.018*\"woman_jewelry\" + 0.017*\"dress_knee\" + 0.017*\"earring\" + 0.016*\"mini\" + 0.014*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #13 (0.067): 0.075*\"man\" + 0.050*\"game\" + 0.039*\"shirt\" + 0.030*\"electronic_video\" + 0.028*\"game_console\" + 0.014*\"pant\" + 0.013*\"top_t\" + 0.013*\"size\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #14 (0.067): 0.054*\"size\" + 0.049*\"woman\" + 0.040*\"shoe\" + 0.021*\"short\" + 0.015*\"man\" + 0.015*\"black\" + 0.013*\"boot\" + 0.013*\"athletic\" + 0.012*\"new\" + 0.011*\"nike\"\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.036366, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7848/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 29DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 120000 documents into a model of 1186028 documents\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.067): 0.047*\"woman\" + 0.020*\"beauty_fragrance\" + 0.016*\"new\" + 0.015*\"hat\" + 0.015*\"man\" + 0.014*\"brand_new\" + 0.014*\"perfume\" + 0.012*\"\" + 0.012*\"jersey\" + 0.011*\"accessory_hat\"\n",
      "INFO : topic #7 (0.067): 0.041*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.024*\"woman\" + 0.021*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #12 (0.067): 0.044*\"rm\" + 0.021*\"bra\" + 0.019*\"home_dcor\" + 0.017*\"home\" + 0.013*\"pink\" + 0.013*\"shipping\" + 0.012*\"2\" + 0.011*\"card\" + 0.010*\"1\" + 0.009*\"accent\"\n",
      "INFO : topic #6 (0.067): 0.047*\"woman\" + 0.043*\"bag\" + 0.024*\"woman_handbag\" + 0.021*\"necklace\" + 0.017*\"purse\" + 0.012*\"pocket\" + 0.012*\"gold\" + 0.010*\"new\" + 0.010*\"leather\" + 0.010*\"wallet\"\n",
      "INFO : topic #0 (0.067): 0.054*\"dress\" + 0.027*\"size\" + 0.022*\"ring\" + 0.022*\"woman\" + 0.018*\"woman_jewelry\" + 0.017*\"dress_knee\" + 0.017*\"earring\" + 0.016*\"mini\" + 0.014*\"length\" + 0.012*\"black\"\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.034983, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 25\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 27\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 28\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 28\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 29\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7858/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 30\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 104000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.041*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.024*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "INFO : topic #13 (0.067): 0.076*\"man\" + 0.051*\"game\" + 0.039*\"shirt\" + 0.030*\"electronic_video\" + 0.028*\"game_console\" + 0.014*\"top_t\" + 0.013*\"size\" + 0.013*\"pant\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #8 (0.067): 0.039*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"accessory\" + 0.015*\"phone\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"brand_new\" + 0.012*\"case_skin\" + 0.010*\"2\"\n",
      "INFO : topic #3 (0.067): 0.045*\"item\" + 0.017*\"shipping\" + 0.013*\"home_kitchen\" + 0.013*\"day\" + 0.012*\"1\" + 0.011*\"2\" + 0.011*\"rae\" + 0.011*\"new\" + 0.010*\"rm\" + 0.009*\"brand_new\"\n",
      "INFO : topic #0 (0.067): 0.054*\"dress\" + 0.027*\"size\" + 0.022*\"woman\" + 0.022*\"ring\" + 0.018*\"woman_jewelry\" + 0.017*\"dress_knee\" + 0.017*\"earring\" + 0.016*\"mini\" + 0.014*\"length\" + 0.012*\"black\"\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic diff=0.034413, rho=0.081043\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 28\n",
      "DEBUG : 7847/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7863/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 22\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7860/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7863/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #11 (0.067): 0.020*\"kid_toy\" + 0.018*\"book\" + 0.013*\"kid\" + 0.013*\"nail\" + 0.011*\"new\" + 0.011*\"slime\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.008*\"disney\" + 0.007*\"toy\"\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.048*\"woman\" + 0.044*\"bag\" + 0.025*\"woman_handbag\" + 0.020*\"necklace\" + 0.017*\"purse\" + 0.012*\"pocket\" + 0.012*\"gold\" + 0.011*\"leather\" + 0.010*\"new\" + 0.010*\"wallet\"\n",
      "INFO : topic #0 (0.067): 0.055*\"dress\" + 0.027*\"size\" + 0.022*\"woman\" + 0.022*\"ring\" + 0.018*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"earring\" + 0.016*\"mini\" + 0.014*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #10 (0.067): 0.047*\"woman\" + 0.020*\"beauty_fragrance\" + 0.017*\"new\" + 0.015*\"hat\" + 0.015*\"man\" + 0.014*\"brand_new\" + 0.014*\"perfume\" + 0.013*\"\" + 0.012*\"jersey\" + 0.011*\"accessory_hat\"\n",
      "INFO : topic #7 (0.067): 0.041*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.024*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic diff=0.033262, rho=0.081043\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 21\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7872/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 16\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 20\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7863/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7897/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "INFO : topic #10 (0.067): 0.047*\"woman\" + 0.020*\"beauty_fragrance\" + 0.017*\"new\" + 0.015*\"hat\" + 0.015*\"man\" + 0.014*\"perfume\" + 0.014*\"brand_new\" + 0.012*\"\" + 0.012*\"jersey\" + 0.011*\"accessory_hat\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #13 (0.067): 0.077*\"man\" + 0.051*\"game\" + 0.040*\"shirt\" + 0.030*\"electronic_video\" + 0.028*\"game_console\" + 0.014*\"top_t\" + 0.013*\"size\" + 0.012*\"pant\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "INFO : topic #12 (0.067): 0.045*\"rm\" + 0.021*\"bra\" + 0.019*\"home_dcor\" + 0.017*\"home\" + 0.013*\"shipping\" + 0.013*\"pink\" + 0.012*\"2\" + 0.011*\"card\" + 0.010*\"1\" + 0.009*\"accent\"\n",
      "INFO : topic #7 (0.067): 0.041*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.024*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #0 (0.067): 0.055*\"dress\" + 0.027*\"size\" + 0.023*\"woman\" + 0.022*\"ring\" + 0.018*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"earring\" + 0.016*\"mini\" + 0.015*\"length\" + 0.012*\"black\"\n",
      "INFO : topic diff=0.032641, rho=0.081043\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7862/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #12 (0.067): 0.045*\"rm\" + 0.021*\"bra\" + 0.019*\"home_dcor\" + 0.017*\"home\" + 0.013*\"shipping\" + 0.013*\"pink\" + 0.012*\"2\" + 0.011*\"card\" + 0.010*\"1\" + 0.009*\"accent\"\n",
      "INFO : topic #8 (0.067): 0.039*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"accessory\" + 0.015*\"phone\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"case_skin\" + 0.012*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic #6 (0.067): 0.049*\"woman\" + 0.044*\"bag\" + 0.025*\"woman_handbag\" + 0.020*\"necklace\" + 0.017*\"purse\" + 0.012*\"pocket\" + 0.012*\"gold\" + 0.011*\"leather\" + 0.011*\"new\" + 0.010*\"wallet\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.022*\"3\" + 0.021*\"0_24\" + 0.020*\"kid_girl\" + 0.017*\"2\" + 0.015*\"mo\" + 0.015*\"size\" + 0.014*\"kid_boy\" + 0.014*\"t\" + 0.013*\"new\"\n",
      "INFO : topic #3 (0.067): 0.046*\"item\" + 0.018*\"shipping\" + 0.013*\"day\" + 0.013*\"home_kitchen\" + 0.011*\"1\" + 0.011*\"2\" + 0.011*\"new\" + 0.011*\"rae\" + 0.010*\"rm\" + 0.009*\"brand_new\"\n",
      "INFO : topic diff=0.032602, rho=0.081043\n",
      "DEBUG : 1996/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 42028 documents into a model of 1186028 documents\n",
      "INFO : topic #14 (0.067): 0.055*\"size\" + 0.049*\"woman\" + 0.041*\"shoe\" + 0.021*\"short\" + 0.016*\"man\" + 0.015*\"black\" + 0.014*\"boot\" + 0.013*\"athletic\" + 0.012*\"new\" + 0.011*\"nike\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.022*\"3\" + 0.021*\"0_24\" + 0.020*\"kid_girl\" + 0.017*\"2\" + 0.015*\"mo\" + 0.015*\"size\" + 0.014*\"kid_boy\" + 0.014*\"t\" + 0.013*\"new\"\n",
      "INFO : topic #5 (0.067): 0.041*\"woman_athletic\" + 0.034*\"size\" + 0.030*\"legging\" + 0.028*\"lularoe\" + 0.026*\"woman\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.018*\"jacket\" + 0.015*\"apparel\" + 0.015*\"black\"\n",
      "INFO : topic #1 (0.067): 0.046*\"jean\" + 0.031*\"bracelet\" + 0.028*\"woman\" + 0.026*\"size\" + 0.018*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #0 (0.067): 0.055*\"dress\" + 0.027*\"size\" + 0.023*\"woman\" + 0.022*\"ring\" + 0.018*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"earring\" + 0.017*\"mini\" + 0.015*\"length\" + 0.012*\"black\"\n",
      "INFO : topic diff=0.030574, rho=0.081043\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.216 per-word bound, 148.7 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #8000/1186028, outstanding queue size 1\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #1 = documents up to #16000/1186028, outstanding queue size 2\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #2 = documents up to #24000/1186028, outstanding queue size 3\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #3 = documents up to #32000/1186028, outstanding queue size 4\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #4 = documents up to #40000/1186028, outstanding queue size 5\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #5 = documents up to #48000/1186028, outstanding queue size 6\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #6 = documents up to #56000/1186028, outstanding queue size 7\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #7 = documents up to #64000/1186028, outstanding queue size 8\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #8 = documents up to #72000/1186028, outstanding queue size 9\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #9 = documents up to #80000/1186028, outstanding queue size 10\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #10 = documents up to #88000/1186028, outstanding queue size 11\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #11 = documents up to #96000/1186028, outstanding queue size 12\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #12 = documents up to #104000/1186028, outstanding queue size 13\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #13 = documents up to #112000/1186028, outstanding queue size 14\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #14 = documents up to #120000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #15 = documents up to #128000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #16 = documents up to #136000/1186028, outstanding queue size 17\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #17 = documents up to #144000/1186028, outstanding queue size 18\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #18 = documents up to #152000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #19 = documents up to #160000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #20 = documents up to #168000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #21 = documents up to #176000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #22 = documents up to #184000/1186028, outstanding queue size 23\n",
      "DEBUG : processing chunk #0 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #23 = documents up to #192000/1186028, outstanding queue size 24\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #24 = documents up to #200000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #1 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #25 = documents up to #208000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #2 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #26 = documents up to #216000/1186028, outstanding queue size 27\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #3 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #4 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #27 = documents up to #224000/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #5 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #28 = documents up to #232000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #6 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #29 = documents up to #240000/1186028, outstanding queue size 29\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #7 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #8 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #30 = documents up to #248000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #31 = documents up to #256000/1186028, outstanding queue size 28\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #9 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7898/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #10 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #11 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #32 = documents up to #264000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #33 = documents up to #272000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #34 = documents up to #280000/1186028, outstanding queue size 28DEBUG : processing chunk #12 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7859/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #13 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #14 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #35 = documents up to #288000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #36 = documents up to #296000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #37 = documents up to #304000/1186028, outstanding queue size 28\n",
      "DEBUG : 7867/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #38 = documents up to #312000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #15 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #16 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #17 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7865/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #18 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #19 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.067): 0.055*\"size\" + 0.049*\"woman\" + 0.042*\"shoe\" + 0.021*\"short\" + 0.016*\"man\" + 0.015*\"black\" + 0.014*\"boot\" + 0.013*\"athletic\" + 0.012*\"new\" + 0.011*\"nike\"\n",
      "INFO : topic #3 (0.067): 0.047*\"item\" + 0.019*\"shipping\" + 0.014*\"day\" + 0.013*\"home_kitchen\" + 0.011*\"1\" + 0.011*\"2\" + 0.011*\"rae\" + 0.011*\"new\" + 0.010*\"rm\" + 0.009*\"price\"\n",
      "INFO : topic #11 (0.067): 0.020*\"kid_toy\" + 0.018*\"book\" + 0.013*\"kid\" + 0.012*\"nail\" + 0.011*\"new\" + 0.011*\"slime\" + 0.009*\"2\" + 0.008*\"brand_new\" + 0.008*\"disney\" + 0.007*\"toy\"\n",
      "INFO : topic #12 (0.067): 0.045*\"rm\" + 0.021*\"bra\" + 0.019*\"home_dcor\" + 0.017*\"home\" + 0.013*\"shipping\" + 0.012*\"2\" + 0.012*\"pink\" + 0.011*\"card\" + 0.010*\"1\" + 0.009*\"accent\"\n",
      "DEBUG : processing chunk #20 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #1 (0.067): 0.046*\"jean\" + 0.031*\"bracelet\" + 0.028*\"woman\" + 0.026*\"size\" + 0.018*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic diff=0.031512, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #39 = documents up to #320000/1186028, outstanding queue size 28\n",
      "DEBUG : 7869/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #21 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #22 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #23 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #24 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #40 = documents up to #328000/1186028, outstanding queue size 22\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #41 = documents up to #336000/1186028, outstanding queue size 22DEBUG : processing chunk #25 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7861/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "\n",
      "DEBUG : processing chunk #26 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #42 = documents up to #344000/1186028, outstanding queue size 22\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #43 = documents up to #352000/1186028, outstanding queue size 22\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #27 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #28 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #29 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #30 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #1 (0.067): 0.046*\"jean\" + 0.031*\"bracelet\" + 0.028*\"woman\" + 0.026*\"size\" + 0.018*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #8 (0.067): 0.040*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"accessory\" + 0.015*\"phone\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"case_skin\" + 0.012*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic #13 (0.067): 0.080*\"man\" + 0.051*\"game\" + 0.040*\"shirt\" + 0.030*\"electronic_video\" + 0.029*\"game_console\" + 0.014*\"top_t\" + 0.013*\"size\" + 0.011*\"pant\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "INFO : topic #6 (0.067): 0.049*\"woman\" + 0.045*\"bag\" + 0.025*\"woman_handbag\" + 0.020*\"necklace\" + 0.018*\"purse\" + 0.012*\"pocket\" + 0.012*\"gold\" + 0.011*\"leather\" + 0.011*\"wallet\" + 0.011*\"new\"\n",
      "INFO : topic #12 (0.067): 0.046*\"rm\" + 0.021*\"bra\" + 0.019*\"home_dcor\" + 0.017*\"home\" + 0.013*\"shipping\" + 0.013*\"2\" + 0.012*\"pink\" + 0.011*\"card\" + 0.010*\"1\" + 0.009*\"accent\"\n",
      "INFO : topic diff=0.031488, rho=0.080778\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #44 = documents up to #360000/1186028, outstanding queue size 21\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #31 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #32 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #33 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #34 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #45 = documents up to #368000/1186028, outstanding queue size 15\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #46 = documents up to #376000/1186028, outstanding queue size 16\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #47 = documents up to #384000/1186028, outstanding queue size 17\n",
      "DEBUG : processing chunk #35 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #48 = documents up to #392000/1186028, outstanding queue size 18\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #49 = documents up to #400000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #50 = documents up to #408000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #51 = documents up to #416000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #52 = documents up to #424000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #53 = documents up to #432000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #54 = documents up to #440000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #55 = documents up to #448000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #56 = documents up to #456000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #57 = documents up to #464000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #58 = documents up to #472000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #36 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #59 = documents up to #480000/1186028, outstanding queue size 29\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #37 of 8000 documents\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #38 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #60 = documents up to #488000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #61 = documents up to #496000/1186028, outstanding queue size 28\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #39 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #40 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #41 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7864/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #42 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.055*\"dress\" + 0.027*\"size\" + 0.023*\"woman\" + 0.022*\"ring\" + 0.019*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"mini\" + 0.017*\"earring\" + 0.015*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.028*\"pink\" + 0.024*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "INFO : topic #2 (0.067): 0.041*\"beauty_makeup\" + 0.018*\"lip\" + 0.018*\"color\" + 0.016*\"face\" + 0.015*\"brand_new\" + 0.015*\"eye\" + 0.014*\"new\" + 0.011*\"skin\" + 0.011*\"brush\" + 0.011*\"makeup\"\n",
      "INFO : topic #1 (0.067): 0.047*\"jean\" + 0.031*\"bracelet\" + 0.028*\"woman\" + 0.026*\"size\" + 0.018*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #10 (0.067): 0.048*\"woman\" + 0.020*\"beauty_fragrance\" + 0.017*\"new\" + 0.016*\"hat\" + 0.015*\"man\" + 0.014*\"perfume\" + 0.014*\"brand_new\" + 0.013*\"\" + 0.013*\"jersey\" + 0.011*\"accessory_hat\"\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #43 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic diff=0.031035, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #62 = documents up to #504000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #44 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7870/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #45 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7890/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #46 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #47 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7871/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #63 = documents up to #512000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #64 = documents up to #520000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #65 = documents up to #528000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #66 = documents up to #536000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #67 = documents up to #544000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #68 = documents up to #552000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #69 = documents up to #560000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #70 = documents up to #568000/1186028, outstanding queue size 27\n",
      "DEBUG : 7875/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #48 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #49 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : processing chunk #50 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #51 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #52 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.048*\"item\" + 0.019*\"shipping\" + 0.014*\"day\" + 0.013*\"home_kitchen\" + 0.012*\"2\" + 0.011*\"1\" + 0.011*\"new\" + 0.011*\"rae\" + 0.010*\"rm\" + 0.010*\"price\"\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.024*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.017*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #1 (0.067): 0.047*\"jean\" + 0.031*\"bracelet\" + 0.029*\"woman\" + 0.026*\"size\" + 0.018*\"skinny\" + 0.017*\"home\" + 0.013*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.010*\"woman_jewelry\"\n",
      "INFO : topic #6 (0.067): 0.050*\"woman\" + 0.046*\"bag\" + 0.025*\"woman_handbag\" + 0.020*\"necklace\" + 0.018*\"purse\" + 0.013*\"pocket\" + 0.012*\"gold\" + 0.011*\"leather\" + 0.011*\"wallet\" + 0.011*\"new\"\n",
      "DEBUG : processing chunk #53 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #8 (0.067): 0.040*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"phone\" + 0.015*\"accessory\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"case_skin\" + 0.012*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic diff=0.030548, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #71 = documents up to #576000/1186028, outstanding queue size 26\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #54 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #55 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #56 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #57 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #72 = documents up to #584000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #73 = documents up to #592000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #74 = documents up to #600000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #75 = documents up to #608000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #76 = documents up to #616000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #77 = documents up to #624000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #78 = documents up to #632000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #79 = documents up to #640000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #80 = documents up to #648000/1186028, outstanding queue size 28\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #58 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #59 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #81 = documents up to #656000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #82 = documents up to #664000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #60 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #83 = documents up to #672000/1186028, outstanding queue size 28DEBUG : 7856/8000 documents converged within 50 iterations\n",
      "\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : processing chunk #61 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #62 of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #63 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #64 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #10 (0.067): 0.048*\"woman\" + 0.020*\"beauty_fragrance\" + 0.017*\"new\" + 0.016*\"hat\" + 0.015*\"man\" + 0.014*\"perfume\" + 0.014*\"brand_new\" + 0.013*\"\" + 0.013*\"jersey\" + 0.011*\"accessory_hat\"\n",
      "INFO : topic #6 (0.067): 0.050*\"woman\" + 0.046*\"bag\" + 0.025*\"woman_handbag\" + 0.019*\"necklace\" + 0.018*\"purse\" + 0.013*\"pocket\" + 0.012*\"gold\" + 0.011*\"leather\" + 0.011*\"wallet\" + 0.011*\"new\"\n",
      "INFO : topic #13 (0.067): 0.082*\"man\" + 0.052*\"game\" + 0.040*\"shirt\" + 0.031*\"electronic_video\" + 0.029*\"game_console\" + 0.014*\"top_t\" + 0.013*\"size\" + 0.010*\"pant\" + 0.010*\"new\" + 0.009*\"2\"\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "INFO : topic #8 (0.067): 0.040*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"phone\" + 0.015*\"accessory\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"case_skin\" + 0.012*\"brand_new\" + 0.010*\"2\"\n",
      "INFO : topic #11 (0.067): 0.020*\"kid_toy\" + 0.018*\"book\" + 0.014*\"kid\" + 0.012*\"nail\" + 0.011*\"new\" + 0.011*\"slime\" + 0.009*\"2\" + 0.008*\"disney\" + 0.008*\"brand_new\" + 0.008*\"toy\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #65 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.029626, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #84 = documents up to #680000/1186028, outstanding queue size 28\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #66 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #67 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #68 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #69 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #85 = documents up to #688000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #86 = documents up to #696000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #87 = documents up to #704000/1186028, outstanding queue size 23\n",
      "DEBUG : 7873/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #70 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #88 = documents up to #712000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #89 = documents up to #720000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #90 = documents up to #728000/1186028, outstanding queue size 25\n",
      "DEBUG : processing chunk #71 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #91 = documents up to #736000/1186028, outstanding queue size 26\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #92 = documents up to #744000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #93 = documents up to #752000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #94 = documents up to #760000/1186028, outstanding queue size 29\n",
      "DEBUG : processing chunk #72 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #73 of 8000 documents\n",
      "DEBUG : updating topics\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #74 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #75 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7878/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #10 (0.067): 0.048*\"woman\" + 0.020*\"beauty_fragrance\" + 0.017*\"new\" + 0.016*\"hat\" + 0.015*\"man\" + 0.014*\"perfume\" + 0.014*\"brand_new\" + 0.013*\"\" + 0.013*\"jersey\" + 0.011*\"accessory_hat\"\n",
      "INFO : topic #2 (0.067): 0.041*\"beauty_makeup\" + 0.018*\"lip\" + 0.018*\"color\" + 0.016*\"face\" + 0.015*\"brand_new\" + 0.015*\"eye\" + 0.014*\"new\" + 0.011*\"skin\" + 0.011*\"makeup\" + 0.011*\"brush\"\n",
      "INFO : topic #5 (0.067): 0.042*\"woman_athletic\" + 0.034*\"size\" + 0.031*\"legging\" + 0.028*\"lularoe\" + 0.026*\"woman\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.018*\"jacket\" + 0.015*\"apparel\" + 0.015*\"black\"\n",
      "INFO : topic #3 (0.067): 0.049*\"item\" + 0.020*\"shipping\" + 0.014*\"day\" + 0.013*\"home_kitchen\" + 0.012*\"2\" + 0.011*\"1\" + 0.011*\"new\" + 0.011*\"rae\" + 0.010*\"rm\" + 0.010*\"price\"\n",
      "DEBUG : processing chunk #76 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #9 (0.067): 0.028*\"kid_toy\" + 0.025*\"hair\" + 0.023*\"accessory\" + 0.017*\"vintage_collectible\" + 0.014*\"beauty\" + 0.013*\"doll_accessory\" + 0.013*\"box\" + 0.012*\"doll\" + 0.012*\"action_figure\" + 0.012*\"woman\"\n",
      "INFO : topic diff=0.030120, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #95 = documents up to #768000/1186028, outstanding queue size 27\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #77 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #78 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #79 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #80 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #96 = documents up to #776000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #97 = documents up to #784000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #98 = documents up to #792000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #99 = documents up to #800000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #100 = documents up to #808000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #101 = documents up to #816000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #102 = documents up to #824000/1186028, outstanding queue size 27\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #103 = documents up to #832000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #81 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #104 = documents up to #840000/1186028, outstanding queue size 28\n",
      "DEBUG : 7896/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #82 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #105 = documents up to #848000/1186028, outstanding queue size 28\n",
      "DEBUG : 7893/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #83 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #84 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7877/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #106 = documents up to #856000/1186028, outstanding queue size 28\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #85 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #86 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #87 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #0 (0.067): 0.056*\"dress\" + 0.027*\"size\" + 0.024*\"woman\" + 0.022*\"ring\" + 0.019*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"mini\" + 0.017*\"earring\" + 0.016*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #8 (0.067): 0.040*\"case\" + 0.025*\"phone_accessory\" + 0.025*\"electronic_cell\" + 0.015*\"phone\" + 0.015*\"accessory\" + 0.014*\"iphone\" + 0.012*\"new\" + 0.012*\"case_skin\" + 0.012*\"brand_new\" + 0.010*\"2\"\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #88 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.038*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.023*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #2 (0.067): 0.041*\"beauty_makeup\" + 0.018*\"lip\" + 0.018*\"color\" + 0.016*\"face\" + 0.015*\"brand_new\" + 0.015*\"eye\" + 0.014*\"new\" + 0.011*\"skin\" + 0.011*\"makeup\" + 0.011*\"brush\"\n",
      "DEBUG : 7898/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.022*\"3\" + 0.022*\"0_24\" + 0.021*\"kid_girl\" + 0.018*\"2\" + 0.015*\"mo\" + 0.015*\"size\" + 0.015*\"kid_boy\" + 0.014*\"t\" + 0.013*\"girl\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #89 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic diff=0.030080, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #107 = documents up to #864000/1186028, outstanding queue size 28\n",
      "DEBUG : processing chunk #90 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #91 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #92 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #93 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #94 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #108 = documents up to #872000/1186028, outstanding queue size 19\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #109 = documents up to #880000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #110 = documents up to #888000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #111 = documents up to #896000/1186028, outstanding queue size 22\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #112 = documents up to #904000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #113 = documents up to #912000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #114 = documents up to #920000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #115 = documents up to #928000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #116 = documents up to #936000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #117 = documents up to #944000/1186028, outstanding queue size 28\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #95 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #96 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7910/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #97 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #98 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7879/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #6 (0.067): 0.051*\"woman\" + 0.046*\"bag\" + 0.025*\"woman_handbag\" + 0.019*\"necklace\" + 0.018*\"purse\" + 0.013*\"pocket\" + 0.011*\"leather\" + 0.011*\"gold\" + 0.011*\"wallet\" + 0.011*\"new\"\n",
      "INFO : topic #0 (0.067): 0.056*\"dress\" + 0.027*\"size\" + 0.024*\"woman\" + 0.022*\"ring\" + 0.019*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"mini\" + 0.017*\"earring\" + 0.016*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #3 (0.067): 0.049*\"item\" + 0.020*\"shipping\" + 0.015*\"day\" + 0.013*\"home_kitchen\" + 0.012*\"2\" + 0.011*\"1\" + 0.011*\"new\" + 0.011*\"rae\" + 0.010*\"price\" + 0.010*\"rm\"\n",
      "DEBUG : 7866/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.037*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.023*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.015*\"medium\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #99 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #5 (0.067): 0.042*\"woman_athletic\" + 0.035*\"size\" + 0.031*\"legging\" + 0.029*\"lularoe\" + 0.026*\"woman\" + 0.022*\"tight_legging\" + 0.022*\"apparel_pant\" + 0.018*\"jacket\" + 0.015*\"apparel\" + 0.015*\"black\"\n",
      "INFO : topic diff=0.029882, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #118 = documents up to #952000/1186028, outstanding queue size 28\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processing chunk #100 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #101 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7904/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #102 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #103 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #119 = documents up to #960000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #120 = documents up to #968000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #121 = documents up to #976000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #122 = documents up to #984000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #104 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #123 = documents up to #992000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #124 = documents up to #1000000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #125 = documents up to #1008000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #126 = documents up to #1016000/1186028, outstanding queue size 28\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #127 = documents up to #1024000/1186028, outstanding queue size 29\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #105 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #128 = documents up to #1032000/1186028, outstanding queue size 29\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #106 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #129 = documents up to #1040000/1186028, outstanding queue size 29\n",
      "DEBUG : 7876/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #107 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : updating topics\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #108 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7882/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #109 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #110 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7884/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "INFO : topic #11 (0.067): 0.021*\"kid_toy\" + 0.018*\"book\" + 0.014*\"kid\" + 0.012*\"nail\" + 0.011*\"new\" + 0.011*\"slime\" + 0.009*\"2\" + 0.009*\"disney\" + 0.008*\"brand_new\" + 0.008*\"toy\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.023*\"3\" + 0.022*\"kid_girl\" + 0.022*\"0_24\" + 0.018*\"2\" + 0.015*\"mo\" + 0.015*\"size\" + 0.015*\"kid_boy\" + 0.014*\"t\" + 0.013*\"girl\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : topic #2 (0.067): 0.041*\"beauty_makeup\" + 0.018*\"lip\" + 0.018*\"color\" + 0.016*\"face\" + 0.015*\"eye\" + 0.015*\"brand_new\" + 0.014*\"new\" + 0.011*\"skin\" + 0.011*\"brush\" + 0.011*\"makeup\"\n",
      "INFO : topic #0 (0.067): 0.056*\"dress\" + 0.027*\"size\" + 0.024*\"woman\" + 0.022*\"ring\" + 0.019*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.017*\"mini\" + 0.017*\"earring\" + 0.016*\"length\" + 0.012*\"black\"\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #111 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.037*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.023*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic diff=0.029247, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #130 = documents up to #1048000/1186028, outstanding queue size 29\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #112 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #113 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #114 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7874/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #115 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #116 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: pass 4, dispatched chunk #131 = documents up to #1056000/1186028, outstanding queue size 20\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #132 = documents up to #1064000/1186028, outstanding queue size 21\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #133 = documents up to #1072000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #134 = documents up to #1080000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #135 = documents up to #1088000/1186028, outstanding queue size 24\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #136 = documents up to #1096000/1186028, outstanding queue size 25\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #137 = documents up to #1104000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #138 = documents up to #1112000/1186028, outstanding queue size 27\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #139 = documents up to #1120000/1186028, outstanding queue size 28\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #117 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #118 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 96000 documents into a model of 1186028 documents\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #119 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7888/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7914/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #120 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "INFO : topic #14 (0.067): 0.055*\"size\" + 0.050*\"woman\" + 0.043*\"shoe\" + 0.021*\"short\" + 0.016*\"man\" + 0.015*\"black\" + 0.014*\"boot\" + 0.014*\"athletic\" + 0.013*\"new\" + 0.012*\"nike\"\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "INFO : topic #6 (0.067): 0.051*\"woman\" + 0.047*\"bag\" + 0.025*\"woman_handbag\" + 0.018*\"necklace\" + 0.018*\"purse\" + 0.013*\"pocket\" + 0.011*\"leather\" + 0.011*\"gold\" + 0.011*\"wallet\" + 0.011*\"new\"\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #121 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #13 (0.067): 0.084*\"man\" + 0.053*\"game\" + 0.040*\"shirt\" + 0.031*\"electronic_video\" + 0.029*\"game_console\" + 0.014*\"top_t\" + 0.013*\"size\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"pant\"\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.037*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.023*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.023*\"3\" + 0.022*\"kid_girl\" + 0.022*\"0_24\" + 0.018*\"2\" + 0.015*\"mo\" + 0.015*\"size\" + 0.015*\"kid_boy\" + 0.014*\"t\" + 0.014*\"girl\"\n",
      "INFO : topic diff=0.028407, rho=0.080778\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #140 = documents up to #1128000/1186028, outstanding queue size 27\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #122 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7887/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #123 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #124 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #141 = documents up to #1136000/1186028, outstanding queue size 22\n",
      "DEBUG : 7916/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #142 = documents up to #1144000/1186028, outstanding queue size 22\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #143 = documents up to #1152000/1186028, outstanding queue size 23\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #144 = documents up to #1160000/1186028, outstanding queue size 24\n",
      "DEBUG : processing chunk #125 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #145 = documents up to #1168000/1186028, outstanding queue size 25\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #146 = documents up to #1176000/1186028, outstanding queue size 26\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #147 = documents up to #1184000/1186028, outstanding queue size 27\n",
      "DEBUG : processing chunk #126 of 8000 documents\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #148 = documents up to #1186028/1186028, outstanding queue size 28\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7913/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #127 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #128 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7891/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #129 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : processing chunk #130 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7883/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7885/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #131 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7902/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #132 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.037*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.023*\"woman\" + 0.022*\"blouse\" + 0.021*\"small\" + 0.018*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.023*\"3\" + 0.022*\"kid_girl\" + 0.022*\"0_24\" + 0.018*\"2\" + 0.015*\"mo\" + 0.015*\"size\" + 0.015*\"kid_boy\" + 0.014*\"t\" + 0.014*\"girl\"\n",
      "INFO : topic #2 (0.067): 0.042*\"beauty_makeup\" + 0.018*\"lip\" + 0.018*\"color\" + 0.017*\"face\" + 0.016*\"eye\" + 0.015*\"brand_new\" + 0.014*\"new\" + 0.011*\"skin\" + 0.011*\"brush\" + 0.011*\"makeup\"\n",
      "INFO : topic #14 (0.067): 0.055*\"size\" + 0.050*\"woman\" + 0.043*\"shoe\" + 0.021*\"short\" + 0.016*\"man\" + 0.015*\"black\" + 0.014*\"boot\" + 0.014*\"athletic\" + 0.013*\"new\" + 0.012*\"nike\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #13 (0.067): 0.084*\"man\" + 0.053*\"game\" + 0.041*\"shirt\" + 0.031*\"electronic_video\" + 0.029*\"game_console\" + 0.014*\"top_t\" + 0.013*\"size\" + 0.010*\"new\" + 0.009*\"2\" + 0.009*\"polo\"\n",
      "INFO : topic diff=0.027042, rho=0.080778\n",
      "DEBUG : processing chunk #133 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7889/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #134 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #135 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #136 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7880/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #137 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #138 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7898/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #139 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7899/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #140 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 88000 documents into a model of 1186028 documents\n",
      "DEBUG : 7886/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #141 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7895/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #142 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7881/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #14 (0.067): 0.056*\"size\" + 0.050*\"woman\" + 0.043*\"shoe\" + 0.021*\"short\" + 0.017*\"man\" + 0.015*\"black\" + 0.014*\"boot\" + 0.014*\"athletic\" + 0.013*\"new\" + 0.012*\"nike\"\n",
      "INFO : topic #2 (0.067): 0.042*\"beauty_makeup\" + 0.018*\"lip\" + 0.018*\"color\" + 0.017*\"face\" + 0.016*\"eye\" + 0.015*\"brand_new\" + 0.014*\"new\" + 0.011*\"skin\" + 0.011*\"brush\" + 0.011*\"makeup\"\n",
      "INFO : topic #11 (0.067): 0.021*\"kid_toy\" + 0.018*\"book\" + 0.014*\"kid\" + 0.012*\"nail\" + 0.011*\"new\" + 0.010*\"slime\" + 0.009*\"2\" + 0.009*\"disney\" + 0.008*\"brand_new\" + 0.008*\"toy\"\n",
      "DEBUG : processing chunk #143 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "INFO : topic #6 (0.067): 0.051*\"woman\" + 0.047*\"bag\" + 0.026*\"woman_handbag\" + 0.018*\"purse\" + 0.018*\"necklace\" + 0.013*\"pocket\" + 0.012*\"leather\" + 0.011*\"wallet\" + 0.011*\"gold\" + 0.011*\"new\"\n",
      "INFO : topic #4 (0.067): 0.023*\"baby\" + 0.023*\"3\" + 0.022*\"kid_girl\" + 0.022*\"0_24\" + 0.018*\"2\" + 0.015*\"size\" + 0.015*\"mo\" + 0.015*\"kid_boy\" + 0.014*\"t\" + 0.014*\"girl\"\n",
      "INFO : topic diff=0.027101, rho=0.080778\n",
      "DEBUG : 7897/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #144 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7901/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #145 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : processing chunk #146 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7857/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : processing chunk #147 of 8000 documents\n",
      "DEBUG : performing inference on a chunk of 8000 documents\n",
      "DEBUG : 7892/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : processing chunk #148 of 2028 documents\n",
      "DEBUG : result put\n",
      "DEBUG : performing inference on a chunk of 2028 documents\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7894/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 2000/2028 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7906/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : 7900/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 90028 documents into a model of 1186028 documents\n",
      "DEBUG : 7910/8000 documents converged within 50 iterations\n",
      "DEBUG : processed chunk, queuing the result\n",
      "DEBUG : result put\n",
      "DEBUG : getting a new job\n",
      "INFO : topic #3 (0.067): 0.050*\"item\" + 0.021*\"shipping\" + 0.016*\"day\" + 0.013*\"home_kitchen\" + 0.012*\"2\" + 0.011*\"new\" + 0.011*\"price\" + 0.011*\"1\" + 0.011*\"rae\" + 0.010*\"rm\"\n",
      "INFO : topic #1 (0.067): 0.048*\"jean\" + 0.031*\"bracelet\" + 0.029*\"woman\" + 0.028*\"size\" + 0.018*\"skinny\" + 0.016*\"home\" + 0.014*\"sock\" + 0.013*\"pair\" + 0.012*\"jean_slim\" + 0.009*\"woman_jewelry\"\n",
      "INFO : topic #7 (0.067): 0.042*\"size\" + 0.037*\"woman_top\" + 0.037*\"shirt\" + 0.029*\"pink\" + 0.023*\"woman\" + 0.022*\"blouse\" + 0.020*\"small\" + 0.018*\"victoria_secret\" + 0.016*\"blouse_t\" + 0.015*\"medium\"\n",
      "INFO : topic #0 (0.067): 0.056*\"dress\" + 0.026*\"size\" + 0.024*\"woman\" + 0.022*\"ring\" + 0.020*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.018*\"mini\" + 0.017*\"earring\" + 0.016*\"length\" + 0.012*\"black\"\n",
      "INFO : topic #9 (0.067): 0.028*\"kid_toy\" + 0.026*\"hair\" + 0.024*\"accessory\" + 0.018*\"vintage_collectible\" + 0.014*\"beauty\" + 0.013*\"doll_accessory\" + 0.013*\"box\" + 0.012*\"action_figure\" + 0.012*\"doll\" + 0.012*\"statue\"\n",
      "INFO : topic diff=0.025602, rho=0.080778\n",
      "DEBUG : updating topics\n",
      "INFO : merging changes from 8000 documents into a model of 1186028 documents\n",
      "INFO : topic #11 (0.067): 0.021*\"kid_toy\" + 0.018*\"book\" + 0.014*\"kid\" + 0.012*\"nail\" + 0.011*\"new\" + 0.011*\"slime\" + 0.009*\"2\" + 0.009*\"disney\" + 0.008*\"toy\" + 0.008*\"brand_new\"\n",
      "INFO : topic #5 (0.067): 0.043*\"woman_athletic\" + 0.034*\"size\" + 0.031*\"legging\" + 0.029*\"lularoe\" + 0.026*\"woman\" + 0.023*\"tight_legging\" + 0.023*\"apparel_pant\" + 0.019*\"jacket\" + 0.016*\"apparel\" + 0.015*\"black\"\n",
      "INFO : topic #12 (0.067): 0.048*\"rm\" + 0.019*\"home_dcor\" + 0.018*\"home\" + 0.017*\"bra\" + 0.013*\"2\" + 0.013*\"shipping\" + 0.012*\"pink\" + 0.012*\"card\" + 0.010*\"1\" + 0.010*\"accent\"\n",
      "INFO : topic #1 (0.067): 0.048*\"jean\" + 0.031*\"bracelet\" + 0.030*\"woman\" + 0.028*\"size\" + 0.018*\"skinny\" + 0.016*\"home\" + 0.014*\"sock\" + 0.012*\"jean_slim\" + 0.012*\"pair\" + 0.009*\"woman_jewelry\"\n",
      "INFO : topic #0 (0.067): 0.055*\"dress\" + 0.026*\"size\" + 0.024*\"woman\" + 0.022*\"ring\" + 0.020*\"woman_jewelry\" + 0.018*\"dress_knee\" + 0.018*\"mini\" + 0.017*\"earring\" + 0.016*\"length\" + 0.012*\"black\"\n",
      "INFO : topic diff=0.024330, rho=0.080778\n",
      "DEBUG : bound: at document #0\n",
      "INFO : -7.216 per-word bound, 148.7 perplexity estimate based on a held-out corpus of 2028 documents with 36142 words\n"
     ]
    }
   ],
   "source": [
    "num_topics =15\n",
    "Lda = models.LdaMulticore\n",
    "lda= Lda(doc_term_matrix, num_topics=num_topics,id2word = dictionary, \n",
    "         passes=5,chunksize=8000,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : performing inference on a chunk of 1186028 documents\n",
      "DEBUG : 1170696/1186028 documents converged within 50 iterations\n"
     ]
    }
   ],
   "source": [
    "num_topics =15\n",
    "vis = pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary,sort_topics=False)\n",
    "pyLDAvis.save_html(vis,f'pyLDAvis_{num_topics}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el65711406768537248888532895063\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el65711406768537248888532895063_data = {\"mdsDat\": {\"x\": [-0.1059918028176417, -0.08059001433596688, 0.15075428031713387, 0.1743074775355913, 0.058098979825870097, -0.21860460716572994, -0.06439711735067072, -0.21837382547003265, 0.14932647630448145, 0.11180123045651165, 0.025020225749753763, 0.1325127493232008, 0.10623159844317703, -0.06773263609304402, -0.15236301472263405], \"y\": [-0.13904193421442348, -0.05267906787612503, -0.028283418362228415, -0.03850825502770747, 0.15322538523412402, 0.02052114330058686, -0.14773288198263174, 0.0036311020456942893, -0.026998496907003733, 0.0872860272856848, -0.03520873687499366, 0.024881470124647052, -0.08256915458363108, 0.22833559768610615, 0.03314122015190135], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [6.541109085083008, 4.451006889343262, 10.564756393432617, 6.533017158508301, 5.707425117492676, 9.543052673339844, 6.462804317474365, 10.656354904174805, 7.956305027008057, 4.041939735412598, 4.071247577667236, 4.92301082611084, 5.931906223297119, 4.515009880065918, 8.10104751586914]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\"], \"Freq\": [371833.0, 126240.0, 137716.0, 79502.0, 93641.0, 96818.0, 74787.0, 84257.0, 387106.0, 72650.0, 50174.0, 72642.0, 63318.0, 51754.0, 109416.0, 62746.0, 46148.0, 45821.0, 42140.0, 45669.0, 42028.0, 146487.0, 49439.0, 35102.0, 29490.0, 29457.0, 27865.0, 41907.0, 48044.0, 37460.0, 375.84637451171875, 295.6698303222656, 1445.3643798828125, 305.059326171875, 208.81692504882812, 208.81358337402344, 208.81341552734375, 207.57810974121094, 208.7991180419922, 208.79129028320312, 204.44757080078125, 214.24493408203125, 459.4332580566406, 187.74234008789062, 161.68092346191406, 209.91226196289062, 202.77149963378906, 228.72906494140625, 1379.087158203125, 154.36610412597656, 133.5046844482422, 138.19403076171875, 107.279541015625, 115.16980743408203, 330.33331298828125, 109.49610137939453, 156.1356964111328, 223.9208526611328, 1708.9737548828125, 102.44747161865234, 29694.13671875, 23643.419921875, 76134.046875, 1192.865966796875, 885.1912231445312, 885.038330078125, 1030.7275390625, 1780.05517578125, 943.0789184570312, 914.6339111328125, 466.1695251464844, 814.6647338867188, 1534.88720703125, 874.9578857421875, 6048.201171875, 1058.3883056640625, 323.38250732421875, 25409.06640625, 769.68701171875, 1372.0704345703125, 7872.68212890625, 2221.666259765625, 2216.905517578125, 4236.2216796875, 684.1976928710938, 1579.6063232421875, 1167.4530029296875, 958.1515502929688, 11523.0576171875, 1618.192626953125, 27532.6875, 6243.0830078125, 24303.34375, 22088.203125, 5824.80224609375, 16200.5703125, 3185.81298828125, 10539.23046875, 2340.142578125, 5824.7900390625, 14696.349609375, 15526.009765625, 3065.836181640625, 2929.42138671875, 9284.3623046875, 35989.81640625, 33017.1484375, 11494.4052734375, 16974.384765625, 11978.3896484375, 6671.18798828125, 12650.376953125, 4691.97509765625, 4246.15625, 4660.28369140625, 6365.01220703125, 8238.833984375, 5187.21923828125, 7449.40576171875, 7007.53173828125, 7358.89892578125, 2306.12158203125, 645.7859497070312, 6883.7734375, 915.7957153320312, 447.9366760253906, 330.9278869628906, 658.2626953125, 11653.115234375, 212.57208251953125, 234.74395751953125, 272.66204833984375, 212.24574279785156, 144.45216369628906, 357.4696960449219, 2136.00830078125, 203.6668243408203, 140.49520874023438, 136.90090942382812, 16902.564453125, 146.3391571044922, 196.71044921875, 162.7206573486328, 221.57461547851562, 29203.611328125, 160.86695861816406, 128.63018798828125, 145.63253784179688, 126.02168273925781, 156.2750244140625, 2955.84423828125, 6207.73681640625, 2006.05224609375, 2581.709228515625, 7772.91455078125, 734.8272094726562, 566.4741821289062, 841.1337890625, 247.19496154785156, 315.05548095703125, 474.693115234375, 12925.400390625, 746.2146606445312, 396.3193664550781, 520.6377563476562, 357.6589660644531, 44802.71484375, 607.9322509765625, 4259.3583984375, 2138.555419921875, 3241.684326171875, 2609.418212890625, 2696.522705078125, 1198.5572509765625, 4117.1123046875, 3990.14990234375, 3595.583251953125, 3564.56640625, 7787.2509765625, 2739.539306640625, 4547.88916015625, 3875.763916015625, 2265.771484375, 7010.45703125, 7338.69384765625, 5064.40380859375, 11639.8583984375, 5374.5, 15231.3251953125, 4583.17431640625, 27663.349609375, 26279.36328125, 8670.8193359375, 3850.924072265625, 4225.63330078125, 5889.08837890625, 6335.5146484375, 5665.16064453125, 7235.23291015625, 7120.4599609375, 4136.099609375, 4810.94287109375, 4319.69140625, 4468.2333984375, 4487.8173828125, 797.4703979492188, 2606.66552734375, 794.306640625, 884.9260864257812, 799.7374877929688, 776.4794921875, 4054.73779296875, 2023.881591796875, 345.9488525390625, 9367.416015625, 320.8573303222656, 3168.564697265625, 1114.7054443359375, 365.8215637207031, 427.5037841796875, 1311.998291015625, 478.2085876464844, 484.703125, 3209.89453125, 432.4212951660156, 806.0767822265625, 689.3181762695312, 326.6930236816406, 1570.291748046875, 249.75347900390625, 285.89691162109375, 321.4349365234375, 344.0281066894531, 200.8673095703125, 211.93453979492188, 18908.5859375, 24358.4765625, 15284.529296875, 40864.42578125, 13502.982421875, 13966.93359375, 7206.685546875, 6311.42529296875, 5657.48193359375, 4567.51123046875, 9194.015625, 5539.779296875, 3450.460693359375, 3555.805419921875, 3065.828125, 3775.661376953125, 2621.98876953125, 92660.1328125, 3792.7177734375, 2707.054443359375, 3620.43798828125, 1939.7181396484375, 1734.3828125, 36898.328125, 17174.67578125, 25232.84375, 6442.92431640625, 12172.37109375, 4075.937255859375, 34451.09375, 24356.27734375, 11554.7197265625, 3945.82958984375, 7753.287109375, 5936.41748046875, 7268.60498046875, 5443.81787109375, 6508.97607421875, 6702.8828125, 8840.5146484375, 6808.4873046875, 39336.67578125, 13007.205078125, 34464.57421875, 31093.236328125, 11636.357421875, 7860.7412109375, 20195.373046875, 16511.326171875, 12190.0947265625, 14625.109375, 16639.451171875, 16479.080078125, 14567.3662109375, 9399.65625, 10762.8701171875, 11493.177734375, 12478.85546875, 10508.20703125, 1250.78076171875, 1486.810546875, 1526.455322265625, 3998.6181640625, 4048.638916015625, 577.8670043945312, 7905.3330078125, 7904.6142578125, 421.96380615234375, 687.2827758789062, 585.0076904296875, 215.56268310546875, 282.5553283691406, 246.6854248046875, 332.7060546875, 218.56578063964844, 246.41709899902344, 207.52838134765625, 240.87628173828125, 346.8760986328125, 179.46926879882812, 351.4873962402344, 253.2071990966797, 165.88299560546875, 182.16746520996094, 763.8367309570312, 232.47621154785156, 181.9017791748047, 173.88771057128906, 243.4095458984375, 17147.15625, 9784.65625, 14893.9755859375, 946.1650390625, 2183.3173828125, 2636.0546875, 754.8226318359375, 444.07867431640625, 872.5003051757812, 937.4580078125, 481.40289306640625, 4279.22314453125, 6873.30029296875, 2913.690185546875, 2474.160400390625, 472.8569030761719, 749.636474609375, 667.8866577148438, 1037.4476318359375, 1694.2742919921875, 2134.731201171875, 3005.8408203125, 69741.4296875, 1472.94384765625, 2659.6689453125, 4179.2138671875, 5103.1552734375, 11663.603515625, 1339.73388671875, 28856.328125, 3341.132080078125, 3795.648681640625, 21873.107421875, 4723.8994140625, 5036.1494140625, 11614.9931640625, 7926.44775390625, 8151.02978515625, 2861.8310546875, 13308.7880859375, 15263.2099609375, 9584.5849609375, 15129.78515625, 5024.65625, 11013.1064453125, 15880.6943359375, 12673.6044921875, 14171.353515625, 7993.7119140625, 15529.751953125, 11001.03515625, 10980.361328125, 12789.857421875, 5779.4755859375, 6091.08935546875, 6834.1767578125, 7067.8525390625, 6361.5634765625, 360.2127685546875, 8700.5791015625, 2150.01513671875, 1545.9400634765625, 293.18878173828125, 239.03359985351562, 533.1207275390625, 528.794677734375, 264.4413757324219, 3226.277587890625, 226.447021484375, 2994.025634765625, 229.5615234375, 210.89700317382812, 188.15769958496094, 197.84280395507812, 148.30279541015625, 134.25877380371094, 166.10015869140625, 673.510498046875, 6452.5556640625, 125.79212951660156, 125.79163360595703, 125.78910827636719, 256.1727294921875, 138.27992248535156, 136.6154022216797, 162.73553466796875, 212.6102752685547, 134.6817169189453, 26234.90234375, 8096.27880859375, 10937.236328125, 3444.900146484375, 4391.65283203125, 1953.7225341796875, 18758.091796875, 1060.006103515625, 1592.3907470703125, 4024.80615234375, 305.8204650878906, 277.5215759277344, 6530.75146484375, 1537.447265625, 625.4728393554688, 3485.3544921875, 1682.18896484375, 375.8331298828125, 14299.6572265625, 7581.38916015625, 27371.232421875, 2305.452880859375, 17077.76953125, 1598.9503173828125, 26844.169921875, 9278.349609375, 5478.04345703125, 14163.322265625, 18094.34765625, 10871.947265625, 12519.8779296875, 8555.8251953125, 16927.947265625, 7044.2177734375, 27263.875, 9863.9111328125, 9520.95703125, 22189.84765625, 13907.1171875, 16032.77734375, 13529.3740234375, 14480.5517578125, 18620.748046875, 15052.7109375, 11186.0615234375, 7930.51220703125, 1181.7930908203125, 2122.896240234375, 45491.73828125, 45642.91796875, 1324.6051025390625, 1464.56396484375, 516.1201782226562, 556.1365356445312, 967.7979125976562, 2441.7763671875, 448.4302673339844, 579.4000854492188, 191.83595275878906, 62906.64453125, 162.7841033935547, 37210.02734375, 323.402587890625, 3718.701171875, 2906.33642578125, 275.9938659667969, 304.42938232421875, 229.72454833984375, 336.1881103515625, 430.4445495605469, 2394.6787109375, 1109.7799072265625, 292.9486999511719, 957.0149536132812, 163.80368041992188, 177.41015625, 15364.00390625, 4826.462890625, 4856.763671875, 7136.87939453125, 3050.935791015625, 2180.50048828125, 1563.75390625, 6513.6435546875, 18672.7890625, 2698.542236328125, 1435.5968017578125, 988.726318359375, 16059.2841796875, 58435.359375, 86019.90625, 6125.97412109375, 31601.81640625, 7500.4794921875, 10368.765625, 3090.5517578125, 5684.14501953125, 6479.36328125, 2438.527587890625, 5824.83447265625, 30522.88671875, 7316.0439453125, 11750.3388671875, 7202.12109375, 5352.22607421875, 5349.33740234375, 11044.49609375, 69120.8125, 26969.767578125, 51754.98046875, 30067.080078125, 20700.9296875, 30967.755859375, 13916.947265625, 12940.013671875, 17354.119140625, 16050.7060546875, 9573.275390625, 19516.92578125, 18937.033203125, 10735.712890625, 13082.2490234375, 9836.4169921875, 10482.7275390625, 855.25927734375, 323.00341796875, 3509.34521484375, 2164.634521484375, 6096.318359375, 243.1179962158203, 6872.59326171875, 761.45751953125, 3505.448486328125, 751.2363891601562, 34861.3984375, 2400.823486328125, 171.30404663085938, 585.5059204101562, 514.0278930664062, 145.3717803955078, 419.2884826660156, 205.8300323486328, 179.90560913085938, 533.7282104492188, 1036.7098388671875, 1247.7598876953125, 7245.5166015625, 697.302490234375, 506.2641296386719, 302.26556396484375, 444.9029235839844, 2192.963623046875, 4732.88134765625, 103.96304321289062, 25053.396484375, 8140.74951171875, 4386.4443359375, 4924.06103515625, 10914.8837890625, 1080.8990478515625, 3140.69384765625, 15329.5, 9540.443359375, 1420.216552734375, 1093.2847900390625, 1666.116455078125, 13547.021484375, 64886.20703125, 7692.78466796875, 2955.1201171875, 3804.987060546875, 10778.6591796875, 1660.2916259765625, 11892.0712890625, 4432.3681640625, 7502.62548828125, 3645.404296875, 6047.22119140625, 15733.9990234375, 23938.205078125, 17765.017578125, 9998.181640625, 70263.328125, 12788.1748046875, 14792.2724609375, 11086.8603515625, 9552.291015625, 6740.0888671875, 13964.796875, 10553.259765625, 14602.6396484375, 9046.091796875, 11349.154296875, 8043.62744140625, 7841.8857421875, 991.3573608398438, 339.1510009765625, 1085.249267578125, 36460.04296875, 6540.72216796875, 8056.984375, 2326.9404296875, 16024.9375, 1797.8939208984375, 6533.0283203125, 10264.5810546875, 897.1671142578125, 241.06101989746094, 1320.4150390625, 1310.4365234375, 164.42686462402344, 159.9208526611328, 185.11915588378906, 251.94744873046875, 828.002197265625, 334.787109375, 130.5489044189453, 533.700439453125, 160.90916442871094, 227.9781494140625, 212.14297485351562, 120.89623260498047, 123.99806213378906, 112.49430084228516, 342.3185729980469, 83406.1875, 48611.22265625, 3712.1220703125, 7207.44091796875, 6620.1904296875, 1753.7652587890625, 1515.676513671875, 4923.11083984375, 1189.5999755859375, 779.400634765625, 2289.3056640625, 667.6300048828125, 1550.371337890625, 25005.765625, 2135.779052734375, 3821.811279296875, 2253.721923828125, 18356.08203125, 5963.736328125, 9650.7724609375, 6656.2578125, 3049.650146484375, 5158.1787109375, 40689.109375, 19120.248046875, 82088.15625, 4265.5439453125, 17949.140625, 22847.84375, 64178.92578125, 8422.15625, 24968.23828125, 46124.5, 8418.111328125, 33365.55859375, 94541.65625, 26862.630859375, 9192.3134765625, 21587.5703125, 51421.58984375, 31431.935546875, 18119.3828125, 21065.509765625, 13135.1943359375, 12461.900390625, 14893.85546875, 15721.603515625, 17830.310546875, 16885.015625, 11040.826171875, 11497.17578125, 11088.4423828125, 1555.90869140625, 1300.2161865234375, 19549.701171875, 10743.8916015625, 2649.152587890625, 4427.70166015625, 1037.63916015625, 6634.77490234375, 1846.8592529296875, 1076.6099853515625, 1699.37548828125, 628.513671875, 2875.1767578125, 530.6504516601562, 678.2444458007812, 41880.515625, 657.3079223632812, 41990.55859375, 764.4050903320312, 2402.46484375, 2244.040283203125, 4312.2548828125, 897.2960205078125, 2625.289306640625, 421.779296875, 1037.8970947265625, 1796.8489990234375, 785.8334350585938, 352.0123596191406, 1273.8612060546875, 23428.966796875, 13988.5654296875, 8677.244140625, 5836.43212890625, 5198.27294921875, 7330.5234375, 2761.825927734375, 3530.687255859375, 2819.14453125, 25036.763671875, 1589.3382568359375, 1668.2227783203125, 3189.803955078125, 15984.876953125, 5227.68310546875, 1942.7030029296875, 66576.6015625, 15516.423828125, 2522.8427734375, 17046.583984375, 8868.8095703125, 3684.897705078125, 3022.40771484375, 5969.36962890625, 4487.28076171875, 7368.0009765625, 11601.033203125, 4426.7412109375, 7438.52197265625, 3875.08154296875, 24167.1796875, 6576.888671875, 12848.943359375, 5603.71826171875, 19216.00390625, 19358.359375, 16700.369140625, 12428.4169921875, 12233.9091796875, 9577.7275390625, 9035.1064453125, 9961.0712890625, 9120.224609375, 9141.8974609375, 8128.35595703125, 1929.737548828125, 4137.4091796875, 6466.74462890625, 2505.96533203125, 1063.011474609375, 7878.013671875, 3838.444091796875, 347.7395324707031, 2383.975341796875, 174.4612579345703, 412.722412109375, 226.4033660888672, 224.09286499023438, 212.7415771484375, 505.9110412597656, 349.12152099609375, 10509.509765625, 548.0525512695312, 169.66427612304688, 10230.185546875, 156.1319122314453, 112.66307067871094, 197.77459716796875, 107.48116302490234, 142.17303466796875, 166.4305419921875, 219.3504180908203, 112.75852966308594, 139.74322509765625, 111.44241333007812, 10950.8388671875, 9840.8369140625, 5616.63623046875, 2793.97900390625, 7761.97998046875, 2675.9931640625, 659.8142700195312, 800.1448974609375, 984.1002807617188, 718.528076171875, 3378.64208984375, 10677.509765625, 21945.87109375, 1423.3238525390625, 517.52587890625, 648.4183349609375, 1026.8145751953125, 638.5364990234375, 2831.1357421875, 1265.1787109375, 1299.16259765625, 5334.708984375, 4487.71435546875, 2750.576171875, 6887.05712890625, 23816.60546875, 2172.2255859375, 6180.9228515625, 15789.369140625, 12106.3857421875, 4314.8388671875, 20133.451171875, 2725.3359375, 10740.4453125, 4168.72900390625, 6702.54296875, 7182.345703125, 8803.77734375, 6092.32177734375, 9289.4345703125, 7364.16162109375, 4600.43310546875, 4918.08056640625, 4148.3740234375, 1086.7828369140625, 1584.028564453125, 358.62786865234375, 763.2714233398438, 682.3204345703125, 7108.01025390625, 311.3019714355469, 1748.28173828125, 490.4074401855469, 386.4853515625, 417.4520568847656, 334.4504089355469, 200.8685302734375, 284.05828857421875, 200.091064453125, 211.0102996826172, 200.08590698242188, 200.08364868164062, 196.31512451171875, 200.0780487060547, 200.0772705078125, 202.09536743164062, 200.07089233398438, 228.91261291503906, 693.76123046875, 182.79380798339844, 12347.638671875, 214.13059997558594, 285.953857421875, 380.9781188964844, 9612.2578125, 6468.66064453125, 11373.8623046875, 1711.73046875, 4990.40625, 1540.9473876953125, 1305.0771484375, 6833.134765625, 2646.88427734375, 615.116943359375, 3334.06787109375, 787.4791259765625, 706.7139892578125, 511.7780456542969, 502.4496154785156, 614.5394287109375, 13546.9287109375, 17460.830078125, 985.1446533203125, 827.495849609375, 6316.94140625, 3386.38427734375, 705.89990234375, 11481.6650390625, 1203.30517578125, 2131.5751953125, 1223.47900390625, 3349.818115234375, 3456.567138671875, 4015.799560546875, 4226.24755859375, 1910.0892333984375, 2366.093017578125, 3060.772216796875, 41626.58203125, 4666.3369140625, 3456.294189453125, 12662.3896484375, 4347.919921875, 14554.1865234375, 11884.6484375, 2875.886474609375, 2844.80078125, 5554.1396484375, 4679.03076171875, 5241.369140625, 6135.87255859375, 5849.16943359375, 7040.33447265625, 4828.298828125, 4981.35009765625, 4344.11865234375, 3855.1796875, 3938.181884765625, 11231.8974609375, 1292.5589599609375, 553.4222412109375, 864.7784423828125, 511.8203430175781, 346.6794128417969, 319.803466796875, 562.1151123046875, 417.6661682128906, 267.7884521484375, 247.98548889160156, 236.95831298828125, 746.5706787109375, 364.79632568359375, 234.6715087890625, 208.65396118164062, 463.24407958984375, 1749.2451171875, 250.3795166015625, 323.875, 267.7754821777344, 301.30224609375, 321.6968078613281, 6397.45556640625, 310.40380859375, 150.80123901367188, 150.20468139648438, 159.64096069335938, 215.8660888671875, 163.799560546875, 12542.3056640625, 18605.662109375, 2889.24267578125, 2595.698486328125, 2444.784912109375, 3498.65478515625, 1547.9718017578125, 949.5502319335938, 681.6630249023438, 992.9209594726562, 1096.8795166015625, 352.5135192871094, 4549.93603515625, 1599.055419921875, 1635.62353515625, 531.938232421875, 503.7953186035156, 1141.504638671875, 1573.7479248046875, 763.3473510742188, 4124.38330078125, 1471.3665771484375, 1854.6065673828125, 6491.89990234375, 3037.86083984375, 2468.89208984375, 2176.123779296875, 8403.4619140625, 9054.8916015625, 14728.3603515625, 21392.515625, 2347.725830078125, 4926.630859375, 6106.17724609375, 4076.8388671875, 4357.03466796875, 2862.127197265625, 4336.86279296875, 2800.980224609375, 5230.48193359375, 2133.48193359375, 6663.65185546875, 11771.1435546875, 9094.673828125, 8390.6328125, 4866.67919921875, 5750.76611328125, 5338.3740234375, 5390.01171875, 4844.49658203125, 3731.20947265625, 4234.51708984375, 4259.95361328125, 4212.6943359375, 4252.2724609375, 806.6419677734375, 327.09912109375, 393.73577880859375, 358.90478515625, 412.4028625488281, 244.5552520751953, 243.72032165527344, 235.4959259033203, 671.7171630859375, 203.0118865966797, 349.7707824707031, 289.6026611328125, 249.36546325683594, 718.848388671875, 754.8901977539062, 402.8138122558594, 3170.067626953125, 166.7445526123047, 166.73861694335938, 181.17308044433594, 166.72276306152344, 167.45741271972656, 429.8417663574219, 1329.2576904296875, 880.119140625, 620.2439575195312, 231.70120239257812, 213.2733612060547, 217.05479431152344, 218.43394470214844, 2271.08447265625, 6230.9853515625, 2019.15869140625, 1669.39892578125, 1004.6107177734375, 691.933349609375, 424.8578186035156, 424.2779846191406, 402.64996337890625, 622.238037109375, 394.67242431640625, 477.8069763183594, 463.1153564453125, 4165.76123046875, 521.6889038085938, 433.3475341796875, 1600.602783203125, 1400.22119140625, 728.02978515625, 4363.80078125, 11328.6689453125, 636.1815185546875, 24299.642578125, 2884.573486328125, 11853.884765625, 552.1886596679688, 12092.943359375, 4829.34521484375, 10234.962890625, 4238.75244140625, 3001.885009765625, 14392.205078125, 4771.57080078125, 1402.362548828125, 5178.17919921875, 1230.53955078125, 2451.677001953125, 60308.7421875, 5968.26611328125, 21551.337890625, 4932.92041015625, 22781.890625, 7248.31884765625, 16058.8427734375, 3078.539794921875, 3163.48291015625, 7194.283203125, 16116.064453125, 12855.8046875, 5048.64794921875, 14691.9052734375, 10776.3505859375, 7971.96337890625, 7872.361328125, 9444.892578125, 8324.078125, 8761.017578125, 7042.46337890625, 6045.876953125, 6729.7294921875, 7884.2177734375, 6318.9716796875, 6780.4384765625, 6461.736328125, 6729.98779296875, 6554.51904296875, 1751.7979736328125, 29409.72265625, 27784.802734375, 1737.7144775390625, 1094.4476318359375, 411.5909729003906, 396.3746643066406, 1012.9078979492188, 1511.3963623046875, 1665.5535888671875, 471.79022216796875, 627.4419555664062, 608.6876831054688, 5250.02734375, 445.7499694824219, 397.7352294921875, 1077.38818359375, 607.4650268554688, 528.9683837890625, 569.8492431640625, 1928.6373291015625, 3404.121337890625, 379.3958435058594, 285.37451171875, 308.98828125, 280.08575439453125, 327.9100646972656, 881.9695434570312, 331.55450439453125, 463.8387145996094, 49910.23828125, 4931.4599609375, 3552.901123046875, 3032.213623046875, 8078.5068359375, 3005.22021484375, 545.0643920898438, 2887.932861328125, 4681.90185546875, 1677.7984619140625, 993.0812377929688, 3685.75146484375, 1007.2799682617188, 868.6466674804688, 7544.0576171875, 1640.1553955078125, 81321.6953125, 903.3446044921875, 13165.0849609375, 5851.111328125, 1741.0948486328125, 38836.98828125, 3784.146240234375, 7420.673828125, 3123.802734375, 3382.81884765625, 6737.13818359375, 7121.96240234375, 6273.88671875, 7446.98095703125, 4713.08642578125, 7512.32275390625, 2423.408447265625, 6516.3095703125, 4758.34130859375, 8609.125, 11732.91796875, 9732.8115234375, 6499.9248046875, 5982.07177734375, 5813.98974609375, 5339.19189453125, 6289.8203125, 5245.501953125, 5429.15478515625, 2683.65771484375, 11649.423828125, 662.4332885742188, 641.1996459960938, 715.3441162109375, 1307.929443359375, 759.5213623046875, 605.0470581054688, 526.5502319335938, 6088.73046875, 373.1089172363281, 765.6869506835938, 412.4590148925781, 685.1788330078125, 444.4793701171875, 629.7575073242188, 15245.845703125, 935.0359497070312, 590.00537109375, 755.216552734375, 326.7684631347656, 564.7562866210938, 386.6512756347656, 1282.909912109375, 852.102294921875, 380.054931640625, 3363.445068359375, 5593.72607421875, 1918.5098876953125, 589.0194702148438, 74334.3671875, 17544.904296875, 11813.28515625, 10558.9453125, 11212.5517578125, 12900.765625, 6753.63330078125, 4301.3310546875, 4180.064453125, 4075.004638671875, 23554.716796875, 2266.6533203125, 1392.9920654296875, 3481.556396484375, 4485.2646484375, 13871.0546875, 2150.521240234375, 1277.99072265625, 24540.439453125, 4009.22802734375, 9082.2705078125, 4561.125, 5869.71484375, 5538.53564453125, 2228.597412109375, 35746.5, 5953.84765625, 3284.6630859375, 20381.8671875, 9527.1298828125, 4203.908203125, 94700.8671875, 85635.2578125, 19920.244140625, 13197.373046875, 16563.513671875, 13899.423828125, 27772.0625, 26473.93359375, 11925.2763671875, 13443.3037109375, 21591.630859375, 14627.5751953125, 12938.1103515625, 11719.15625, 11225.8466796875, 13849.880859375, 11633.0771484375, 10872.2099609375], \"Term\": [\"woman\", \"man\", \"shirt\", \"dress\", \"woman_athletic\", \"beauty_makeup\", \"shoe\", \"woman_top\", \"size\", \"bag\", \"game\", \"case\", \"legging\", \"jean\", \"item\", \"lularoe\", \"kid_toy\", \"tight_legging\", \"phone_accessory\", \"apparel_pant\", \"electronic_cell\", \"rm\", \"blouse\", \"woman_handbag\", \"electronic_video\", \"bracelet\", \"game_console\", \"lip\", \"bra\", \"jacket\", \"engagement_ring\", \"pairs-[rm\", \"925_sterling\", \"purple_grass\", \"bio_entire\", \"idea_range\", \"following_shop\", \"beer_stein\", \"choker_concerned\", \"choker_liking\", \"tumbler_rambler\", \"david_bridal\", \"jewelry_ring\", \"10day_homepage\", \"etc_limit\", \"sherri_hill\", \"total_carat\", \"carat\", \"prom\", \"engagement_wedding\", \"lid_02\", \"\\u2666_\\ufe0fi\", \"05_order\", \"buckle_trace\", \"jewelry_earring\", \"\\u2740\", \"solitaire\", \"engagement\", \"bodycon\", \"carat_weight\", \"ring\", \"earring\", \"dress\", \"formal\", \"product_menu\", \"purchase_progress\", \"cubic_zirconia\", \"cz\", \"gemstone\", \"nose_ring\", \"faux_nose\", \"order_status\", \"hoop\", \"hoop_earring\", \"sterling_silver\", \"stud_earring\", \"vacuum_cup\", \"dress_knee\", \"message_choice\", \"10k\", \"stone\", \"sterling\", \"925\", \"maxi\", \"sundress\", \"yeti_cup\", \"midi\", \"beer\", \"jewelry\", \"4+)/dresses\", \"woman_jewelry\", \"diamond\", \"mini\", \"length\", \"crystal\", \"silver\", \"wedding\", \"skirt\", \"bust\", \"mm\", \"necklace\", \"gold\", \"pearl\", \"kendra_scott\", \"beautiful\", \"size\", \"woman\", \"blue\", \"black\", \"white\", \"green\", \"color\", \"cup\", \"heart\", \"floral\", \"cute\", \"rm\", \"material\", \"brand_new\", \"2\", \"new\", \"alex_ani\", \"pandora_charm\", \"boot_cut\", \"zebra_sprinkle\", \"\\u261e\", \"electric_wax\", \"soy_wax\", \"jean_slim\", \"scented_goodness\", \"2\\\"x3_tablespoon\", \"sherpa_blanket\", \"scoop_equal\", \"\\u272f\", \"jeans\", \"straight_leg\", \"wallflow\", \"\\u273d\", \"melter_30\", \"skinny\", \"sprinkle_concentrated\", \"bellami\", \"refuge\", \"weft\", \"bracelet\", \"throw_blanket\", \"zippo\", \"\\u25aa_\\ufe0fget\", \"curtain_panel\", \"temperature_resistant\", \"jegging\", \"skinny_jean\", \"bangle\", \"rock_revival\", \"candle\", \"wax_melt\", \"warmer\", \"bulb\", \"murano\", \"wallflower\", \"window_treatment\", \"sock\", \"diffuser\", \"scentsy\", \"lokai_bracelet\", \"inseam_31\", \"jean\", \"sprinkle\", \"pandora\", \"candle_holder\", \"27\", \"29\", \"wax\", \"bootcut\", \"inseam\", \"28\", \"distressed\", \"26\", \"charm\", \"levi\", \"denim\", \"blanket\", \"true_religion\", \"american_eagle\", \"waist\", \"fragrance\", \"pair\", \"scent\", \"home\", \"stretch\", \"woman\", \"size\", \"woman_jewelry\", \"leg\", \"wash\", \"inch\", \"light\", \"home_d\\u00e9cor\", \"2\", \"new\", \"hollister\", \"3\", \"dark\", \"4\", \"brand_new\", \"instax_mini\", \"pore\", \"ultra_matte\", \"blackhead\", \"fine_line\", \"kyshadow\", \"eye_shadow\", \"smashbox\", \"tooth_whitener\", \"eyeshadow\", \"line_wrinkle\", \"liquid_lipstick\", \"complexion\", \"peel_mask\", \"blackhead_remover\", \"pigmented\", \"dark_circle\", \"redness\", \"lip_kit\", \"banana_powder\", \"stila\", \"sweet_peach\", \"koko_kollection\", \"jeffree_star\", \"appearance_fine\", \"wunderbrow\", \"lipliner\", \"pigmentation\", \"pore_blackhead\", \"mud_mask\", \"palette\", \"brush\", \"makeup_palette\", \"lip\", \"foundation\", \"lipstick\", \"urban_decay\", \"concealer\", \"primer\", \"eyeliner\", \"makeup_brush\", \"highlighter\", \"eyeshadow_palette\", \"lipsense\", \"nyx\", \"brow\", \"matte_lipstick\", \"beauty_makeup\", \"faced\", \"eyebrow\", \"naked\", \"pigment\", \"morphe\", \"face\", \"shade\", \"skin\", \"mascara\", \"powder\", \"contour\", \"eye\", \"makeup\", \"matte\", \"shadow\", \"mask\", \"kylie\", \"mac\", \"liquid\", \"cosmetic\", \"blush\", \"care_face\", \"\\u2661\", \"color\", \"product\", \"brand_new\", \"new\", \"beauty\", \"natural\", \"1\", \"box\", \"authentic\", \"free_shipping\", \"2\", \"rm\", \"bundle\", \"beauty_skin\", \"free\", \"3\", \"size\", \"black\", \"jewelry_[rm\", \"netbook\", \"tablet_laptop\", \"dining_dining\", \"entertaining\", \"houseware_ceramic\", \"tea_accessory\", \"dining_coffee\", \"rd_minor\", \"pyrex\", \"tablet_drive\", \"n64_-sega\", \"houseware_bowl\", \"responsible_breakage\", \"dining_entertaining\", \"gb_ram\", \"family_planning\", \"pregnancy_test\", \"hookah\", \"spoon_rest\", \"rae_bake\", \"ring_navel\", \"sugar_canister\", \"intel_core\", \"claim_usp\", \"medical_supply\", \"creamer\", \"oval_plate\", \"respectful_policy\", \"rae_flour\", \"home_kitchen\", \"mug\", \"rae\", \"dining_bakeware\", \"canister\", \"\\u2606_\\u2606\", \"platter\", \"vape\", \"kitchen_dining\", \"responsible_damage\", \"coffee_tea\", \"bowl\", \"\\u2606\", \"rae_dunn\", \"responsible\", \"squishie\", \"chip_crack\", \"rd\", \"equipment\", \"utensil_gadget\", \"dining_kitchen\", \"sale_final\", \"item\", \"post_office\", \"coffee\", \"usp\", \"buyer\", \"purchase\", \"transaction\", \"shipping\", \"rating\", \"fee\", \"day\", \"\\u274c\", \"happy\", \"ship\", \"discount\", \"order\", \"tea\", \"listing\", \"price\", \"question\", \"1\", \"business_day\", \"free\", \"2\", \"free_shipping\", \"rm\", \"vintage_collectible\", \"new\", \"bundle\", \"3\", \"brand_new\", \"thank\", \"package\", \"price_firm\", \"4\", \"good\", \"\\u2734_\\u2734\", \"mo_piece\", \"item_health\", \"sleeper\", \"pure_romance\", \"avent\", \"gerber\", \"tide_pod\", \"sleep_sack\", \"18_month\", \"crest_3d\", \"newborn\", \"ultra_shea\", \"garcinia_cambogia\", \"6/9_month\", \"vacuum_floor\", \"clarin\", \"\\u2714_\\ufe0f100\", \"oral_b\", \"onsie\", \"carter\", \"\\u2714_\\ufe0feverything\", \"\\u2714_\\ufe0fwe\", \"\\u2714_\\ufe0four\", \"enfamil\", \"6/9\", \"anastasia_moonchild\", \"snowsuit\", \"3/6_month\", \"moonlight_path\", \"0_24\", \"care_body\", \"2t-5t\", \"12_month\", \"onesie\", \"baby_gap\", \"mo\", \"household_cleaner\", \"bottle_feeding\", \"item_personal\", \"lip_smacker\", \"footie\", \"scrub\", \"cleaning_supply\", \"janie_jack\", \"infant\", \"gymboree\", \"garanimal\", \"month\", \"daily_travel\", \"baby\", \"2t-5t_bottom\", \"t\", \"child_place\", \"kid_girl\", \"bath_body\", \"lotion\", \"beauty_skin\", \"kid_boy\", \"care\", \"boy\", \"bottle\", \"girl\", \"outfit\", \"3\", \"oz\", \"body\", \"2\", \"6\", \"1\", \"4\", \"bundle\", \"size\", \"new\", \"brand_new\", \"shirt\", \"lularoe_julia\", \"amelia\", \"apparel_pant\", \"tight_legging\", \"puffer\", \"julia\", \"fabletic\", \"gymshark\", \"skirt_skort\", \"hoodie_sweatshirt\", \"trench\", \"raincoat\", \"zella\", \"legging\", \"sizing-_10/12\", \"jacket\", \"usp_tr.number\", \"fleece_jacket\", \"tall_curvy\", \"\\u26d4_\\ufe0fwhen\", \"\\u2764_patkat\", \"thumbhole\", \"htf_unicorn\", \"major_unicorn\", \"4+)/coats_jacket\", \"tc\", \"yogas\", \"bomber_jacket\", \"alo_yoga\", \"mosaic_bear\", \"sport_bra\", \"apparel_tracksuit\", \"windbreaker\", \"yoga\", \"blazer\", \"half_zip\", \"carly\", \"north_face\", \"hoodie\", \"llr\", \"suit_blazer\", \"vietnam\", \"coat_jacket\", \"lularoe\", \"woman_athletic\", \"man_sweat\", \"apparel\", \"lululemon\", \"sweat\", \"sweatpant\", \"background\", \"pullover\", \"dress_asymmetrical\", \"workout\", \"pant\", \"bnwt\", \"zip\", \"unicorn\", \"fleece\", \"coat\", \"top\", \"size\", \"small\", \"woman\", \"pink\", \"medium\", \"black\", \"nwt\", \"sweater\", \"large\", \"tag\", \"print\", \"brand_new\", \"new\", \"worn\", \"color\", \"nike\", \"blue\", \"glass_pipe\", \"spoon_pipe\", \"backpack_style\", \"pipe\", \"crossbody\", \"rayne\", \"messenger_crossbody\", \"hobo\", \"satchel\", \"tone_hardware\", \"woman_handbag\", \"cross_body\", \"geneva\", \"saffiano_leather\", \"velvet_choker\", \"oil_rig\", \"main_compartment\", \"selma\", \"multifunction_pocket\", \"spacious\", \"coin_purse\", \"dooney_bourke\", \"tote_shopper\", \"messenger\", \"dooney\", \"magnetic_snap\", \"weekender\", \"clutch\", \"handbag\", \"harlow\", \"purse\", \"tote\", \"wristlet\", \"vera_bradley\", \"accessory_wallet\", \"duffle_bag\", \"compartment\", \"wallet\", \"backpack\", \"gold_hardware\", \"h_x\", \"betsey_johnson\", \"coach\", \"bag\", \"kate_spade\", \"louis_vuitton\", \"cosmetic_bag\", \"michael_kor\", \"thirty\", \"watch\", \"interior\", \"chain\", \"dust_bag\", \"choker\", \"leather\", \"necklace\", \"pocket\", \"strap\", \"woman\", \"x\", \"gold\", \"authentic\", \"inch\", \"zipper\", \"black\", \"accessory\", \"new\", \"woman_jewelry\", \"brand_new\", \"color\", \"small\", \"bikini_bottom\", \"\\u25c6_\\u25c7\", \"lularoe_cassie\", \"blouse_t\", \"sweater_crewneck\", \"swimwear_piece\", \"skirt_straight\", \"blouse_tank\", \"g_string\", \"bralette\", \"bikini\", \"bandeau\", \"triangl\", \"campus_tee\", \"crewneck\", \"itsy\", \"guy_harvey\", \"hygienic_liner\", \"john_galt\", \"cover_up\", \"halter_bralette\", \"triangl_bikini\", \"love_lemon\", \"mumu\", \"monokini\", \"jolyn\", \"san_lorenzo\", \"zaful\", \"beanie_slouchy\", \"ringer_tee\", \"woman_top\", \"blouse\", \"swimsuit\", \"cardigan\", \"lingerie\", \"lularoe_randy\", \"kimono\", \"thong\", \"crew_neck\", \"varsity_crew\", \"swim\", \"ivory_ella\", \"bathing_suit\", \"tank\", \"southern\", \"panty\", \"v_neck\", \"tee\", \"sweater_cardigan\", \"pantie\", \"brandy_melville\", \"bodysuit\", \"blouse_tunic\", \"victoria_secret\", \"woman_underwear\", \"shirt\", \"urban_outfitter\", \"lace\", \"sweater\", \"pink\", \"sexy\", \"bra\", \"small\", \"crop\", \"medium\", \"size\", \"large\", \"21\", \"tag\", \"woman\", \"black\", \"cute\", \"white\", \"nwt\", \"new_tag\", \"bundle\", \"color\", \"new\", \"brand_new\", \"piece\", \"perfect\", \"worn\", \"ipad_air\", \"ipod_nano\", \"case_skin\", \"iphone_7\", \"ipad_mini\", \"iphone_6s\", \"otterbox\", \"iphone_6/6s\", \"galaxy_s7\", \"galaxy_s6\", \"iphone_5/5s\", \"otterbox_defender\", \"cable_adapter\", \"shockproof\", \"4th_generation\", \"electronic_cell\", \"5th_generation\", \"phone_accessory\", \"fitbit_charge\", \"iphone_5s\", \"iphone_5c\", \"blu_ray\", \"otter_box\", \"ipod\", \"video_gps\", \"lifeproof\", \"popsocket\", \"fitbit\", \"plus/6s\", \"6_6s\", \"iphone\", \"iphone_6\", \"smartphone\", \"medium_dvd\", \"screen_protector\", \"dvd\", \"ipod_touch\", \"6s\", \"apple_watch\", \"phone\", \"galaxy_note\", \"charger_cradle\", \"earphone\", \"cell_phone\", \"ipad\", \"unlocked\", \"case\", \"electronic\", \"android\", \"charger\", \"cable\", \"adapter\", \"headset\", \"compatible\", \"device\", \"camera\", \"apple\", \"bluetooth\", \"screen\", \"usb\", \"accessory\", \"battery\", \"5\", \"headphone\", \"brand_new\", \"new\", \"2\", \"3\", \"1\", \"free_shipping\", \"4\", \"rm\", \"color\", \"black\", \"box\", \"styling_product\", \"toy_action\", \"hair_care\", \"shampoo_conditioner\", \"hair_scalp\", \"funko_pop\", \"wig\", \"dorbz\", \"funko\", \"pop_expandable\", \"sdcc\", \"funko_dorbz\", \"keratin\", \"\\u2550_\\u2550\", \"bitty_baby\", \"wen\", \"action_figure\", \"hot_wheel\", \"pleasant_company\", \"statue\", \"redken\", \"moroccan_oil\", \"comic_con\", \"oribe\", \"gamestop_exclusive\", \"nycc\", \"1b\", \"diecast\", \"cheer_bow\", \"exc\", \"doll_accessory\", \"figure\", \"scarf\", \"bath_bomb\", \"vest\", \"conditioner\", \"joker\", \"chase\", \"monster_high\", \"lp\", \"barbie\", \"doll\", \"hair\", \"shopkin\", \"harley_quinn\", \"music_vinyl\", \"collectible_figurine\", \"lps\", \"shampoo\", \"beast\", \"comic\", \"pop\", \"exclusive\", \"lush\", \"belt\", \"kid_toy\", \"batman\", \"american\", \"vintage_collectible\", \"beauty\", \"bath\", \"accessory\", \"headband\", \"box\", \"mint\", \"girl\", \"bundle\", \"new\", \"condition\", \"woman\", \"brand_new\", \"lot\", \"description\", \"2\", \"clear_lens\", \"eau_de\", \"\\ua565_\\u2728\", \"baseball_cap\", \"eau\", \"accessory_sunglass\", \"clubmaster\", \"aviator\", \"wispie\", \"toilette\", \"viva_la\", \"edp\", \"modeling_\\u26d4\", \"edt\", \"low_advance.\", \"closet_rule\", \"mercarii_policy\", \"\\u26d4_\\ufe0frude\", \"\\u2733_\\ufe0fdescription\", \"\\u26d4_\\ufe0flowball\", \"returnss_incorrect\", \"\\u26d4_\\ufe0fplease\", \"tradess_\\u26d4\", \"candy_land\", \"parfum\", \"strapback\", \"perfume\", \"\\u26d4_\\u26d4\", \"sunnie\", \"wayfarer\", \"accessory_hat\", \"sunglass\", \"jersey\", \"ray_ban\", \"sport_fan\", \"cologne\", \"3.4_oz\", \"costume\", \"shop_nfl\", \"wispy\", \"beanie\", \"shop_mlb\", \"rollerball\", \"shop_nba\", \"house_lash\", \"dad_hat\", \"hat\", \"beauty_fragrance\", \"versace\", \"snapback\", \"\\u2705\", \"\\u2764_\\u2764\", \"era\", \"\\u2764\", \"halloween_costume\", \"nfl\", \"mink\", \"mist\", \"cap\", \"lens\", \"\\ufe0f\", \"\\u25aa\", \"\\u2728_\\u2728\", \"frame\", \"woman\", \"glass\", \"fragrance\", \"man\", \"bottle\", \"new\", \"brand_new\", \"spray\", \"lash\", \"victoria_secret\", \"pair\", \"price\", \"black\", \"rm\", \"size\", \"box\", \"pink\", \"free_shipping\", \"authentic\", \"2\", \"slime\", \"floam\", \"slime_sticky\", \"borax\", \"fluffy_slime\", \"slime_borax\", \"fishbowl_slime\", \"foam_bead\", \"minifigure\", \"container_dryness\", \"elmer_glue\", \"stretchy_noise\", \"jamberry\", \"\\u2800_\\u2800\", \"ready_disclaimer\", \"fridge_30\", \"crunchy\", \"building_toy\", \"adult_coloring\", \"manicure\", \"beat_dr\", \"jamberry_nail\", \"speaker_subwoofer\", \"art_craft\", \"tula\", \"borax_borax\", \"shipment_weather\", \"beat_solo\", \"sheet_jamberry\", \"bubblegum_slime\", \"nail\", \"book\", \"home_seasonal\", \"fiction\", \"book_literature\", \"d\\u00e9cor\", \"gear_backpack\", \"book_education\", \"paperback\", \"teaching\", \"comforter\", \"duvet\", \"animal_plush\", \"puzzle\", \"ornament\", \"beat_dre\", \"comforter_set\", \"nursery_bedding\", \"seat\", \"learning_education\", \"plush\", \"mickey\", \"mickey_mouse\", \"child\", \"hello_kitty\", \"spinner\", \"lego\", \"toy\", \"disney\", \"kid\", \"kid_toy\", \"nail_polish\", \"halloween\", \"christma\", \"sheet\", \"fun\", \"art\", \"headphone\", \"carrier\", \"hand\", \"polish\", \"baby\", \"new\", \"2\", \"brand_new\", \"set\", \"great\", \"box\", \"1\", \"home\", \"handmade\", \"free\", \"4\", \"description\", \"3\", \"usp_up\", \"random_polish\", \"6.5x10\", \"shipping\\u2022_\\u2764\", \"@mrscoolbreeze\", \"deduction\", \"\\u2665_cheap\", \"ship_uship\", \"fedex_dhl\", \"\\u2666_\\ufe0fperfect\", \"4x8\", \"comment_\\u2795\", \"\\u2764_tmfpolish\", \"wall_decor\", \"6x9\", \"contact_bad\", \"pet_supply\", \"\\u2666_\\ufe0fwater\", \"postage_\\u2666\", \"envelope_u\", \"\\u2666_\\ufe0fup\", \"\\ufe0flight_weight\", \"9x12\", \"polymailer\", \"pressure_sensitive\", \"topp\", \"garland\", \"12x15.5_14x17\", \"6x9_7.5x10.5\", \"9x12_10x13\", \"poly_mailer\", \"lanyard\", \"mailer\", \"10x13\", \"pokemon_card\", \"decal_sticker\", \"10x13_polymailer\", \"polymailer_office\", \"10x13_poly\", \"self_seal\", \"12x15.5\", \"beach_towel\", \"up_fedex\", \"decal\", \"wall_art\", \"mailer_office\", \"planner\", \"bubble_mailer\", \"shabby_chic\", \"handmade_paper\", \"dog\", \"planter\", \"home_d\\u00e9cor\", \"envelope\", \"sticker\", \"proof_measure\", \"accent\", \"decor\", \"supply\", \"trading_card\", \"organizer\", \"card\", \"label\", \"tapestry\", \"office_supply\", \"self_adhesive\", \"storage_organization\", \"rm\", \"=\", \"bra\", \"50\", \"home\", \"\\u2728\", \"shipping\", \"wall\", \"pcs\", \"woman_underwear\", \"2\", \"1\", \"20\", \"pink\", \"3\", \"listing\", \"good\", \"bundle\", \"4\", \"color\", \"price\", \"x\", \"5\", \"new\", \"10\", \"item\", \"free_shipping\", \"brand_new\", \"size\", \"wii\", \"electronic_video\", \"game_console\", \"polo_rugby\", \"ps2\", \"grand_theft\", \"boxer_brief\", \"gameboy\", \"3ds\", \"super_mario\", \"final_fantasy\", \"mario_kart\", \"gameboy_advance\", \"console\", \"gta\", \"sega_genesis\", \"gamecube\", \"baseball_softball\", \"sne\", \"n64\", \"ps3\", \"nintendo\", \"battlefield\", \"palazzo\", \"warfare\", \"cib\", \"ps1\", \"nintendo_ds\", \"gba\", \"nintendo_gamecube\", \"game\", \"xbox\", \"ps4\", \"xbox_360\", \"polo\", \"playstation\", \"nes\", \"polo_ralph\", \"controller\", \"sport_team\", \"nintendo_64\", \"lauren\", \"nintendo_3ds\", \"ds\", \"athletic_apparel\", \"golf\", \"man\", \"video_gaming\", \"top_t\", \"men\", \"cargo\", \"shirt\", \"4+)/top_t\", \"t_shirt\", \"ralph_lauren\", \"maternity\", \"sport\", \"top\", \"xl\", \"pant\", \"button\", \"description\", \"disc\", \"good_condition\", \"boy\", \"2\", \"size\", \"new\", \"large\", \"great\", \"condition\", \"great_condition\", \"brand_new\", \"medium\", \"3\", \"air_max\", \"2t-5t_shoe\", \"skecher\", \"flyknit\", \"roshe\", \"ugg_boot\", \"mule_clog\", \"nike_roshe\", \"9c\", \"4+)/shoe\", \"dansko\", \"flats\", \"10c\", \"7y\", \"huarache\", \"6c\", \"shoe_boot\", \"insole\", \"7c\", \"crocs\", \"chunky_heel\", \"8c\", \"4y\", \"birkenstock\", \"rain_boot\", \"espadrille\", \"tom\", \"shoe_pump\", \"uggs\", \"chaco\", \"shoe\", \"sneaker\", \"shoe_sandal\", \"sandal\", \"shoe_fashion\", \"heel\", \"converse\", \"wedge\", \"slip_on\", \"shoe_loafer\", \"athletic\", \"sole\", \"timberland\", \"air_jordan\", \"bootie\", \"apparel_short\", \"sol\", \"sperry\", \"boot\", \"slipper\", \"jordan\", \"toe\", \"8.5\", \"van\", \"ugg\", \"short\", \"7.5\", \"9.5\", \"nike\", \"flat\", \"6.5\", \"size\", \"woman\", \"8\", \"9\", \"7\", \"kid_boy\", \"man\", \"black\", \"kid_girl\", \"good_condition\", \"new\", \"box\", \"6\", \"10\", \"worn\", \"brand_new\", \"white\", \"great_condition\"], \"Total\": [371833.0, 126240.0, 137716.0, 79502.0, 93641.0, 96818.0, 74787.0, 84257.0, 387106.0, 72650.0, 50174.0, 72642.0, 63318.0, 51754.0, 109416.0, 62746.0, 46148.0, 45821.0, 42140.0, 45669.0, 42028.0, 146487.0, 49439.0, 35102.0, 29490.0, 29457.0, 27865.0, 41907.0, 48044.0, 37460.0, 377.23309326171875, 297.2176818847656, 1453.2427978515625, 306.7445068359375, 209.9764862060547, 209.9761199951172, 209.97642517089844, 208.74505615234375, 209.97584533691406, 209.97586059570312, 205.6214599609375, 215.49169921875, 462.5579833984375, 189.04763793945312, 162.80807495117188, 211.3788299560547, 204.19107055664062, 230.47552490234375, 1389.7041015625, 155.6171875, 134.6287078857422, 139.3582000732422, 108.25788879394531, 116.22064208984375, 333.3553771972656, 110.58242797851562, 157.70811462402344, 226.23104858398438, 1726.663818359375, 103.52742767333984, 30160.234375, 24145.193359375, 79502.4140625, 1208.880126953125, 897.0957641601562, 897.0912475585938, 1048.559326171875, 1823.8468017578125, 960.3142700195312, 931.1843872070312, 471.5506896972656, 830.1430053710938, 1575.71142578125, 893.5516357421875, 6393.53759765625, 1094.7857666015625, 327.7660827636719, 28550.55859375, 799.7572631835938, 1456.754150390625, 8902.810546875, 2420.70654296875, 2443.024169921875, 4882.43115234375, 716.423828125, 1787.5335693359375, 1291.6707763671875, 1048.8934326171875, 17026.439453125, 1884.651123046875, 45761.2421875, 8609.451171875, 40491.16796875, 36349.15234375, 8013.89501953125, 31008.462890625, 4409.1103515625, 19942.720703125, 3184.43798828125, 10960.7431640625, 38826.7890625, 48546.734375, 4863.841796875, 4794.7978515625, 31794.94921875, 387106.15625, 371833.375, 64220.3359375, 161154.3125, 81540.6484375, 28759.28515625, 129411.1484375, 14293.40625, 12230.2998046875, 18524.525390625, 55903.84765625, 146487.28125, 27382.267578125, 191853.9375, 150327.578125, 221221.171875, 2311.1591796875, 647.3660888671875, 6906.61865234375, 919.0210571289062, 450.1236572265625, 332.56024169921875, 661.6071166992188, 11713.7978515625, 213.8775634765625, 236.2507781982422, 274.5846252441406, 213.75062561035156, 145.50537109375, 360.1517333984375, 2152.108154296875, 205.28553771972656, 141.6126708984375, 138.01295471191406, 17040.123046875, 147.54483032226562, 198.332763671875, 164.12921142578125, 223.49819946289062, 29457.83984375, 162.30429077148438, 129.8003692626953, 146.97726440429688, 127.19068908691406, 157.7715606689453, 2984.8759765625, 6296.94189453125, 2035.46044921875, 2646.14208984375, 8057.58203125, 744.6251831054688, 573.4635620117188, 859.964111328125, 249.65975952148438, 319.1183776855469, 483.05120849609375, 13633.5966796875, 764.8323974609375, 402.6107482910156, 531.974609375, 363.1838073730469, 51754.734375, 627.8129272460938, 4706.6376953125, 2319.8779296875, 3581.357177734375, 2870.84912109375, 3011.994873046875, 1297.3282470703125, 4843.4873046875, 5118.83349609375, 4556.13671875, 4562.4970703125, 11039.912109375, 3440.4736328125, 6563.962890625, 5590.98388671875, 2976.26611328125, 12363.2998046875, 16584.75, 10026.5712890625, 36003.68359375, 11089.859375, 68437.515625, 9081.3681640625, 371833.375, 387106.15625, 45761.2421875, 8046.33447265625, 10733.015625, 28651.708984375, 44196.21875, 30091.7109375, 150327.578125, 221221.171875, 10522.28515625, 111007.8828125, 23427.28125, 79669.140625, 191853.9375, 799.1163940429688, 2613.588134765625, 796.4928588867188, 887.4931030273438, 802.2890625, 779.4009399414062, 4070.754638671875, 2032.382080078125, 347.41387939453125, 9407.6650390625, 322.2701721191406, 3183.030517578125, 1119.8873291015625, 367.5250549316406, 429.5148010253906, 1318.197265625, 480.5388488769531, 487.06884765625, 3225.845947265625, 434.576416015625, 810.2130737304688, 692.8779296875, 328.40899658203125, 1578.6627197265625, 251.11474609375, 287.5071105957031, 323.2462463378906, 345.97149658203125, 202.01788330078125, 213.18601989746094, 19028.896484375, 24651.443359375, 15440.65234375, 41907.93359375, 13703.7529296875, 14182.6123046875, 7278.46337890625, 6367.50439453125, 5708.720703125, 4603.3330078125, 9333.703125, 5601.26416015625, 3472.721923828125, 3585.01416015625, 3087.865234375, 3813.4833984375, 2641.5078125, 96818.296875, 3838.176513671875, 2730.77392578125, 3665.21533203125, 1952.733154296875, 1745.273681640625, 38708.9921875, 17761.28515625, 26344.65234375, 6574.83984375, 12671.21875, 4147.884765625, 37961.20703125, 26719.08203125, 12596.30078125, 4051.132080078125, 8605.3876953125, 6404.26123046875, 8495.0810546875, 6044.98583984375, 7658.32275390625, 8057.37158203125, 12334.279296875, 8909.640625, 129411.1484375, 23842.541015625, 191853.9375, 221221.171875, 27280.05078125, 11869.1455078125, 106889.3671875, 81301.390625, 37913.92578125, 94061.9453125, 150327.578125, 146487.28125, 104892.59375, 23616.189453125, 66381.8125, 111007.8828125, 387106.15625, 161154.3125, 1252.1431884765625, 1490.0914306640625, 1529.9080810546875, 4008.639404296875, 4059.640625, 579.5513305664062, 7930.04248046875, 7929.37646484375, 423.57904052734375, 689.9478759765625, 587.4470825195312, 216.54827880859375, 284.0313720703125, 247.99234008789062, 334.4919128417969, 219.7786865234375, 247.79776000976562, 208.7292022705078, 242.36569213867188, 349.09197998046875, 180.62277221679688, 353.79522705078125, 254.8953094482422, 167.01759338378906, 183.42764282226562, 769.132080078125, 234.1016082763672, 183.20901489257812, 175.14938354492188, 245.1907196044922, 17410.732421875, 10035.68359375, 15351.6416015625, 953.9886474609375, 2227.032470703125, 2696.159912109375, 762.3985595703125, 447.76202392578125, 886.4797973632812, 953.7218627929688, 486.9826965332031, 4439.119140625, 7233.83984375, 3018.668701171875, 2586.988525390625, 480.0908203125, 777.881103515625, 690.2061767578125, 1092.659912109375, 1862.0897216796875, 2394.548095703125, 3520.610107421875, 109416.7421875, 1678.0860595703125, 3357.40625, 5696.24072265625, 7503.24462890625, 20514.4375, 1527.009033203125, 69726.4296875, 4745.76025390625, 5613.77197265625, 53762.08203125, 8104.51220703125, 8944.56640625, 28555.06640625, 16982.556640625, 19719.931640625, 4214.82470703125, 53350.51953125, 69179.0390625, 32633.99609375, 106889.3671875, 11895.0400390625, 66381.8125, 150327.578125, 94061.9453125, 146487.28125, 36998.8046875, 221221.171875, 104892.59375, 111007.8828125, 191853.9375, 17215.9765625, 22677.5390625, 55851.83203125, 79669.140625, 51948.03125, 361.3330078125, 8729.0185546875, 2157.074951171875, 1553.111328125, 294.65728759765625, 240.399169921875, 536.2106323242188, 531.8684692382812, 266.0419616699219, 3246.6513671875, 227.88050842285156, 3013.331787109375, 231.19776916503906, 212.43954467773438, 189.5360565185547, 199.34326171875, 149.43263244628906, 135.28648376464844, 167.4149169921875, 678.8702392578125, 6503.97021484375, 126.80680847167969, 126.80682373046875, 126.80683898925781, 258.3050842285156, 139.44236755371094, 137.7913818359375, 164.18089294433594, 214.50970458984375, 135.9021759033203, 26797.763671875, 8195.041015625, 11162.7294921875, 3476.79150390625, 4477.10546875, 1981.37451171875, 19530.470703125, 1075.3314208984375, 1625.51025390625, 4154.0859375, 309.0418395996094, 280.1774597167969, 6995.97412109375, 1604.1590576171875, 641.4414672851562, 3781.943115234375, 1785.4493408203125, 382.8182678222656, 16557.326171875, 8924.5, 35087.28515625, 2636.5625, 23077.259765625, 1821.917236328125, 40554.66015625, 13295.080078125, 7493.29296875, 23616.189453125, 34642.15625, 20960.291015625, 25505.951171875, 15958.818359375, 41573.1015625, 13212.30078125, 111007.8828125, 25121.5625, 25567.576171875, 150327.578125, 61824.484375, 106889.3671875, 79669.140625, 104892.59375, 387106.15625, 221221.171875, 191853.9375, 137716.953125, 1185.3580322265625, 2130.417236328125, 45669.2421875, 45821.62109375, 1330.29931640625, 1471.307861328125, 518.836181640625, 559.0719604492188, 973.0342407226562, 2455.260009765625, 451.002685546875, 582.996826171875, 193.03224182128906, 63318.109375, 163.87496948242188, 37460.75, 325.6114807128906, 3744.264892578125, 2927.616455078125, 278.039306640625, 306.6876525878906, 231.49041748046875, 338.81591796875, 433.8609924316406, 2413.793212890625, 1118.758544921875, 295.4263916015625, 965.294677734375, 165.26211547851562, 178.9973602294922, 15651.58203125, 4875.53515625, 4921.93017578125, 7280.33056640625, 3088.798828125, 2200.451416015625, 1578.8614501953125, 6673.3388671875, 19363.1875, 2747.769775390625, 1453.571533203125, 1000.1099243164062, 16766.087890625, 62746.39453125, 93641.703125, 6373.80859375, 34650.2109375, 7944.05859375, 11206.919921875, 3226.878173828125, 6178.2626953125, 7126.923828125, 2536.72412109375, 6857.5986328125, 44370.09765625, 8914.4033203125, 17165.013671875, 9587.84375, 6613.98046875, 6660.1494140625, 21320.8828125, 387106.15625, 110686.9609375, 371833.375, 147597.0625, 75649.203125, 161154.3125, 39118.2421875, 36385.40234375, 70424.0078125, 69953.9453125, 21927.578125, 191853.9375, 221221.171875, 42357.98046875, 129411.1484375, 33993.1640625, 64220.3359375, 857.6132202148438, 324.0747985839844, 3524.78857421875, 2175.033447265625, 6131.4384765625, 244.5994873046875, 6914.52978515625, 766.3128662109375, 3527.906005859375, 756.3710327148438, 35102.31640625, 2417.697509765625, 172.51829528808594, 589.691650390625, 517.814697265625, 146.44500732421875, 422.3996887207031, 207.42332458496094, 181.4520721435547, 538.546875, 1046.077392578125, 1259.305908203125, 7313.25146484375, 703.8253784179688, 511.28399658203125, 305.2897644042969, 449.56256103515625, 2216.700927734375, 4784.830078125, 105.124267578125, 25399.265625, 8266.521484375, 4454.94921875, 5012.71875, 11213.6962890625, 1094.6968994140625, 3225.7431640625, 16061.3662109375, 9943.6728515625, 1452.30517578125, 1114.920654296875, 1716.853759765625, 14578.2333984375, 72650.6484375, 8190.55517578125, 3090.474609375, 4016.88916015625, 11988.6552734375, 1720.344482421875, 14558.2802734375, 5014.78662109375, 8894.9462890625, 4077.87353515625, 7380.6796875, 22851.130859375, 38826.7890625, 33041.2578125, 18103.875, 371833.375, 31617.708984375, 48546.734375, 37913.92578125, 28651.708984375, 13810.052734375, 161154.3125, 74899.4609375, 221221.171875, 45761.2421875, 191853.9375, 129411.1484375, 110686.9609375, 994.3799438476562, 340.1922607421875, 1088.636962890625, 36576.8828125, 6566.8388671875, 8089.39697265625, 2337.143798828125, 16097.6748046875, 1807.203857421875, 6566.94140625, 10318.1865234375, 901.8788452148438, 242.471435546875, 1328.22265625, 1319.3746337890625, 165.58657836914062, 161.05130004882812, 186.52407836914062, 253.96420288085938, 834.7130737304688, 337.5060119628906, 131.62115478515625, 538.11474609375, 162.2437744140625, 229.9193572998047, 214.07545471191406, 122.01914978027344, 125.15391540527344, 113.56108093261719, 345.6119689941406, 84257.7890625, 49439.8515625, 3764.806640625, 7375.900390625, 6783.82177734375, 1774.931396484375, 1534.5511474609375, 5055.48974609375, 1205.1312255859375, 787.3394165039062, 2344.206298828125, 675.9345092773438, 1608.7454833984375, 27701.166015625, 2245.619873046875, 4092.980712890625, 2394.578125, 21664.513671875, 6750.09619140625, 11326.533203125, 7797.77197265625, 3354.92822265625, 6071.22802734375, 60554.5078125, 26475.04296875, 137716.953125, 4972.44482421875, 27194.642578125, 36385.40234375, 147597.0625, 12263.0654296875, 48044.5234375, 110686.9609375, 12365.595703125, 75649.203125, 387106.15625, 70424.0078125, 15057.6875, 69953.9453125, 371833.375, 161154.3125, 55903.84765625, 81540.6484375, 39118.2421875, 35646.453125, 104892.59375, 129411.1484375, 221221.171875, 191853.9375, 36182.21875, 58945.5625, 42357.98046875, 1559.5235595703125, 1303.3973388671875, 19602.5234375, 10774.4052734375, 2657.150146484375, 4441.1650390625, 1040.845458984375, 6656.43212890625, 1852.98193359375, 1080.2030029296875, 1705.149658203125, 630.6533813476562, 2885.27197265625, 532.5177001953125, 680.6375122070312, 42028.76953125, 659.6528930664062, 42140.50390625, 767.1656494140625, 2411.30517578125, 2252.3544921875, 4328.55078125, 900.7642822265625, 2635.747802734375, 423.4651794433594, 1042.107666015625, 1804.1546630859375, 789.0512084960938, 353.4538879394531, 1279.0947265625, 23568.5859375, 14048.3525390625, 8750.6533203125, 5865.66650390625, 5233.435546875, 7429.3818359375, 2773.21728515625, 3554.928955078125, 2836.65625, 25758.28125, 1596.3223876953125, 1676.4295654296875, 3229.240966796875, 16545.759765625, 5343.84375, 1957.75732421875, 72642.1484375, 16350.74609375, 2556.412841796875, 18535.8515625, 9407.189453125, 3799.842529296875, 3095.5380859375, 6456.4248046875, 4755.13427734375, 8233.6630859375, 13818.82421875, 4739.1298828125, 8607.625, 4145.34033203125, 74899.4609375, 11081.1826171875, 59902.421875, 9960.4482421875, 191853.9375, 221221.171875, 150327.578125, 111007.8828125, 106889.3671875, 94061.9453125, 79669.140625, 146487.28125, 129411.1484375, 161154.3125, 81301.390625, 1935.8206787109375, 4150.4833984375, 6487.92138671875, 2514.319580078125, 1066.705810546875, 7909.326171875, 3854.305908203125, 349.4613342285156, 2396.607421875, 175.42135620117188, 415.3398742675781, 227.84927368164062, 225.55734252929688, 214.173095703125, 509.3749694824219, 351.6461486816406, 10592.3427734375, 552.4853515625, 171.04823303222656, 10313.646484375, 157.50045776367188, 113.70604705810547, 199.6056671142578, 108.47955322265625, 143.4960479736328, 167.99269104003906, 221.4144287109375, 113.88568878173828, 141.18829345703125, 112.61026763916016, 11118.5224609375, 9996.0888671875, 5692.8935546875, 2826.102294921875, 7951.82861328125, 2742.22412109375, 667.5023193359375, 811.43017578125, 1001.779296875, 729.2500610351562, 3488.5517578125, 11192.5458984375, 24033.115234375, 1477.2044677734375, 529.5283203125, 669.0597534179688, 1085.7606201171875, 660.3298950195312, 3179.902587890625, 1362.2962646484375, 1423.3470458984375, 6644.98681640625, 5505.4716796875, 3303.76171875, 10275.9970703125, 46148.9140625, 2798.403564453125, 10821.3154296875, 36998.8046875, 27280.05078125, 7269.23974609375, 74899.4609375, 4275.8935546875, 81301.390625, 11651.564453125, 41573.1015625, 104892.59375, 221221.171875, 61641.8046875, 371833.375, 191853.9375, 36741.3515625, 73441.6484375, 150327.578125, 1089.120849609375, 1589.6475830078125, 359.90118408203125, 766.2062377929688, 685.1781005859375, 7140.05078125, 312.8020935058594, 1756.9404296875, 492.8538513183594, 388.5292663574219, 419.67193603515625, 336.2669982910156, 202.00074768066406, 285.6664123535156, 201.2280731201172, 212.20986938476562, 201.22840881347656, 201.2288055419922, 197.44195556640625, 201.2284393310547, 201.22836303710938, 203.2626190185547, 201.22909545898438, 230.24362182617188, 697.887451171875, 183.90859985351562, 12423.4921875, 215.45205688476562, 287.75537109375, 383.4585876464844, 9724.9990234375, 6518.5341796875, 11604.69921875, 1723.8206787109375, 5081.7880859375, 1551.5845947265625, 1313.8294677734375, 6997.46630859375, 2686.54931640625, 619.3287963867188, 3423.6025390625, 797.0436401367188, 715.7377319335938, 516.8596801757812, 507.6093444824219, 624.8091430664062, 14919.345703125, 19521.515625, 1021.624755859375, 855.87109375, 7204.80078125, 3755.906005859375, 730.0699462890625, 15726.1376953125, 1340.757080078125, 2581.299072265625, 1381.6083984375, 4613.3896484375, 5039.2138671875, 6231.11181640625, 6858.0048828125, 2960.762939453125, 4169.98974609375, 6224.68896484375, 371833.375, 17479.396484375, 10026.5712890625, 126240.75, 15958.818359375, 221221.171875, 191853.9375, 7225.39794921875, 7080.16943359375, 60554.5078125, 36003.68359375, 69179.0390625, 161154.3125, 146487.28125, 387106.15625, 81301.390625, 147597.0625, 94061.9453125, 37913.92578125, 150327.578125, 11256.17578125, 1296.5833740234375, 555.1985473632812, 867.583984375, 513.5569458007812, 347.9048156738281, 321.0058898925781, 564.4172973632812, 419.4888916015625, 268.97406005859375, 249.10467529296875, 238.03147888183594, 749.9802856445312, 366.47357177734375, 235.75640869140625, 209.6927947998047, 465.55084228515625, 1758.2174072265625, 251.69952392578125, 325.609375, 269.2399597167969, 303.0235900878906, 323.7333984375, 6438.744140625, 312.41973876953125, 151.78810119628906, 151.2006378173828, 160.72921752929688, 217.343017578125, 164.93240356445312, 12693.4794921875, 18965.3125, 2913.709716796875, 2620.818603515625, 2467.934814453125, 3554.53662109375, 1567.64453125, 959.554443359375, 686.9329833984375, 1004.7899780273438, 1112.234130859375, 354.9611511230469, 4774.59326171875, 1645.9373779296875, 1694.424072265625, 539.108642578125, 510.9544372558594, 1183.1912841796875, 1647.5498046875, 784.8561401367188, 4605.81201171875, 1607.42041015625, 2078.058349609375, 8130.75390625, 3582.7216796875, 2992.87646484375, 2610.4150390625, 13380.068359375, 15450.556640625, 28653.33984375, 46148.9140625, 3055.473876953125, 8399.169921875, 12314.4638671875, 7074.95947265625, 8067.98486328125, 4545.64990234375, 9960.4482421875, 4680.65234375, 16762.099609375, 2953.547119140625, 35087.28515625, 221221.171875, 150327.578125, 191853.9375, 36122.19140625, 66730.2109375, 81301.390625, 106889.3671875, 68437.515625, 22026.4453125, 66381.8125, 79669.140625, 73441.6484375, 111007.8828125, 809.3480834960938, 328.5753479003906, 395.5169372558594, 360.60870361328125, 414.3802795410156, 245.76242065429688, 244.9457244873047, 236.71571350097656, 675.2019653320312, 204.1572265625, 351.8459777832031, 291.32568359375, 250.85769653320312, 723.302734375, 759.6763916015625, 405.37109375, 3190.510986328125, 167.83926391601562, 167.83901977539062, 182.37786865234375, 167.83883666992188, 168.5795135498047, 432.7344055175781, 1338.807373046875, 886.4546508789062, 624.7103271484375, 233.37249755859375, 214.8127899169922, 218.6258544921875, 220.0283660888672, 2294.194091796875, 6359.8154296875, 2039.1827392578125, 1683.8060302734375, 1014.3421020507812, 697.9727783203125, 427.987060546875, 427.4282531738281, 405.6734924316406, 629.690185546875, 397.6570129394531, 482.5248718261719, 467.707763671875, 4302.36474609375, 528.65673828125, 437.78009033203125, 1661.2491455078125, 1451.178955078125, 747.5144653320312, 4799.08642578125, 12971.4228515625, 651.82275390625, 30091.7109375, 3204.64794921875, 14186.8662109375, 563.332763671875, 14717.8876953125, 5586.96533203125, 12495.7431640625, 4966.8701171875, 3504.105712890625, 19297.501953125, 6145.52099609375, 1558.56201171875, 6862.75146484375, 1350.097900390625, 3095.22265625, 146487.28125, 9706.55078125, 48044.5234375, 8426.6220703125, 68437.515625, 18283.419921875, 69726.4296875, 5281.7578125, 5597.4892578125, 26475.04296875, 150327.578125, 106889.3671875, 14956.55078125, 147597.0625, 111007.8828125, 53350.51953125, 51948.03125, 104892.59375, 79669.140625, 129411.1484375, 69179.0390625, 31617.708984375, 59902.421875, 221221.171875, 46750.16796875, 109416.7421875, 94061.9453125, 191853.9375, 387106.15625, 1756.462890625, 29490.525390625, 27865.779296875, 1743.0933837890625, 1097.839599609375, 413.0064697265625, 397.73974609375, 1016.4679565429688, 1516.7974853515625, 1671.6109619140625, 473.53009033203125, 629.7567138671875, 610.9420776367188, 5269.48876953125, 447.5269470214844, 399.345947265625, 1081.9251708984375, 610.057861328125, 531.322021484375, 572.408935546875, 1937.4195556640625, 3419.843017578125, 381.15576171875, 286.7263488769531, 310.4562072753906, 281.4372253417969, 329.5355529785156, 886.4922485351562, 333.25836181640625, 466.2812194824219, 50174.76171875, 4975.52099609375, 3578.023193359375, 3051.14599609375, 8236.1923828125, 3054.774169921875, 548.0059204101562, 2953.818603515625, 4825.529296875, 1716.69189453125, 1007.1638793945312, 3847.69287109375, 1024.6431884765625, 887.61279296875, 8633.71484375, 1734.925537109375, 126240.75, 943.4647827148438, 20094.091796875, 8241.37890625, 2128.540771484375, 137716.953125, 6092.5390625, 15394.087890625, 5226.05517578125, 5888.84423828125, 16704.966796875, 21320.8828125, 25991.7578125, 44370.09765625, 19250.462890625, 73441.6484375, 4051.906494140625, 56639.06640625, 25505.951171875, 150327.578125, 387106.15625, 221221.171875, 70424.0078125, 66730.2109375, 61641.8046875, 52103.28125, 191853.9375, 75649.203125, 111007.8828125, 2689.17431640625, 11679.33984375, 664.304443359375, 643.0139770507812, 717.5072631835938, 1312.0657958984375, 761.9583129882812, 606.9938354492188, 528.2557373046875, 6108.896484375, 374.36102294921875, 768.2861328125, 413.8725891113281, 687.7860107421875, 446.1737060546875, 632.1688842773438, 15305.7626953125, 938.78515625, 592.4503173828125, 758.3613891601562, 328.1797790527344, 567.3077392578125, 388.4262390136719, 1288.7999267578125, 856.0213623046875, 381.805908203125, 3379.0361328125, 5620.224609375, 1927.857421875, 591.919921875, 74787.265625, 17672.619140625, 11876.3623046875, 10617.841796875, 11305.0419921875, 13048.41796875, 6790.2333984375, 4338.7451171875, 4223.85400390625, 4117.66796875, 24229.83203125, 2279.600341796875, 1403.195068359375, 3545.1044921875, 4599.3369140625, 14557.4736328125, 2184.061279296875, 1294.7471923828125, 26676.41796875, 4198.94921875, 9856.1484375, 4893.267578125, 6585.93115234375, 6191.2998046875, 2335.498046875, 53617.25, 7075.423828125, 3655.1201171875, 33993.1640625, 14309.63671875, 5015.7294921875, 387106.15625, 371833.375, 46613.984375, 26705.23046875, 41244.1171875, 34642.15625, 126240.75, 161154.3125, 40554.66015625, 56639.06640625, 221221.171875, 81301.390625, 61824.484375, 46750.16796875, 42357.98046875, 191853.9375, 81540.6484375, 52103.28125], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.723400115966797, 2.7218000888824463, 2.72160005569458, 2.72160005569458, 2.7214999198913574, 2.7214999198913574, 2.7214999198913574, 2.7214999198913574, 2.721400022506714, 2.721400022506714, 2.721299886703491, 2.721299886703491, 2.7202999591827393, 2.720099925994873, 2.720099925994873, 2.720099925994873, 2.720099925994873, 2.7195000648498535, 2.719399929046631, 2.7190001010894775, 2.7186999320983887, 2.7186999320983887, 2.7179999351501465, 2.7179999351501465, 2.7179999351501465, 2.7172000408172607, 2.7170000076293945, 2.7167999744415283, 2.7167999744415283, 2.716599941253662, 2.7114999294281006, 2.7060999870300293, 2.683799982070923, 2.71370005607605, 2.71370005607605, 2.7135000228881836, 2.70989990234375, 2.7028000354766846, 2.7090001106262207, 2.7091000080108643, 2.71560001373291, 2.708199977874756, 2.7007999420166016, 2.7060000896453857, 2.6714999675750732, 2.6933000087738037, 2.713599920272827, 2.6105000972747803, 2.688699960708618, 2.6672000885009766, 2.604099988937378, 2.6412999629974365, 2.6298999786376953, 2.585099935531616, 2.680999994277954, 2.6033999919891357, 2.625999927520752, 2.6366000175476074, 2.3366000652313232, 2.5745999813079834, 2.2190001010894775, 2.4056999683380127, 2.216599941253662, 2.2288999557495117, 2.4079999923706055, 2.0778000354766846, 2.402100086212158, 2.0892999172210693, 2.4189999103546143, 2.094899892807007, 1.7555999755859375, 1.5871000289916992, 2.2655999660491943, 2.234299898147583, 1.4960999488830566, 0.3515999913215637, 0.30559998750686646, 1.006600022315979, 0.4763999879360199, 0.8090999722480774, 1.2659000158309937, 0.4018000066280365, 1.6131000518798828, 1.6691999435424805, 1.347000002861023, 0.5541999936103821, -0.1509999930858612, 1.0634000301361084, -0.5214999914169312, -0.33880001306533813, -0.6761999726295471, 3.1098999977111816, 3.109600067138672, 3.1087000370025635, 3.1085000038146973, 3.1071999073028564, 3.107100009918213, 3.1070001125335693, 3.106800079345703, 3.1059000492095947, 3.105600118637085, 3.1050000190734863, 3.1050000190734863, 3.10479998588562, 3.104599952697754, 3.1045000553131104, 3.104099988937378, 3.104099988937378, 3.1038999557495117, 3.1038999557495117, 3.103800058364868, 3.103800058364868, 3.1033999919891357, 3.1033999919891357, 3.1033999919891357, 3.103100061416626, 3.1029999256134033, 3.102799892425537, 3.102799892425537, 3.1024999618530273, 3.102299928665161, 3.0978000164031982, 3.0975000858306885, 3.087399959564209, 3.0761001110076904, 3.09879994392395, 3.0998001098632812, 3.089900016784668, 3.102099895477295, 3.0992000102996826, 3.094599962234497, 3.0587000846862793, 3.087399959564209, 3.096299886703491, 3.0905001163482666, 3.0966999530792236, 2.9677999019622803, 3.079900026321411, 3.012200117111206, 3.0306999683380127, 3.012399911880493, 3.0165998935699463, 3.0013999938964844, 3.032900094985962, 2.9495999813079834, 2.8629000186920166, 2.8752999305725098, 2.8652000427246094, 2.763000011444092, 2.884200096130371, 2.7451000213623047, 2.7455999851226807, 2.8392999172210693, 2.5446999073028564, 2.2967000007629395, 2.428999900817871, 1.9829000234603882, 2.387700080871582, 1.6095000505447388, 2.4282000064849854, 0.513700008392334, 0.4221000075340271, 1.4486000537872314, 2.3750998973846436, 2.1798999309539795, 1.5298999547958374, 1.169600009918213, 1.4421000480651855, 0.07819999754428864, -0.32420000433921814, 2.178299903869629, -0.02669999934732914, 1.4213000535964966, 0.23119999468326569, -0.6432999968528748, 2.2455999851226807, 2.244999885559082, 2.2448999881744385, 2.2446999549865723, 2.244499921798706, 2.2439000606536865, 2.2437000274658203, 2.243499994277954, 2.2434000968933105, 2.2434000968933105, 2.243299961090088, 2.2430999279022217, 2.243000030517578, 2.243000030517578, 2.243000030517578, 2.2428998947143555, 2.242799997329712, 2.242799997329712, 2.2427000999450684, 2.2427000999450684, 2.242500066757202, 2.242500066757202, 2.2423999309539795, 2.242300033569336, 2.2421998977661133, 2.242000102996826, 2.242000102996826, 2.242000102996826, 2.2418999671936035, 2.24180006980896, 2.241300106048584, 2.2356998920440674, 2.237499952316284, 2.222399950027466, 2.2328999042510986, 2.232300043106079, 2.2376999855041504, 2.238800048828125, 2.238600015640259, 2.239799976348877, 2.232599973678589, 2.236599922180176, 2.2411999702453613, 2.239500045776367, 2.240499973297119, 2.2376999855041504, 2.2402000427246094, 2.203700065612793, 2.2356998920440674, 2.2388999462127686, 2.2353999614715576, 2.240999937057495, 2.2414000034332275, 2.199700117111206, 2.214099884033203, 2.2044999599456787, 2.227400064468384, 2.2074999809265137, 2.23009991645813, 2.150599956512451, 2.155100107192993, 2.16129994392395, 2.221299886703491, 2.143399953842163, 2.171799898147583, 2.0917000770568848, 2.142899990081787, 2.0850000381469727, 2.0636000633239746, 1.9146000146865845, 1.978700041770935, 1.0568000078201294, 1.641700029373169, 0.5307999849319458, 0.2854999899864197, 1.3955999612808228, 1.8356000185012817, 0.5813000202178955, 0.6535000205039978, 1.1130000352859497, 0.3864000141620636, 0.04659999907016754, 0.06279999762773514, 0.273499995470047, 1.3264000415802002, 0.42829999327659607, -0.02019999921321869, -1.187000036239624, -0.48260000348091125, 2.7272000312805176, 2.726099967956543, 2.7260000705718994, 2.725800037384033, 2.725600004196167, 2.725399971008301, 2.7251999378204346, 2.7251999378204346, 2.7244999408721924, 2.724400043487549, 2.724100112915039, 2.7237000465393066, 2.723099946975708, 2.7230000495910645, 2.722899913787842, 2.7228000164031982, 2.7227001190185547, 2.7225000858306885, 2.722100019454956, 2.72189998626709, 2.72189998626709, 2.7218000888824463, 2.7216999530792236, 2.7214999198913574, 2.721400022506714, 2.721400022506714, 2.721299886703491, 2.721100091934204, 2.721100091934204, 2.7209999561309814, 2.7130000591278076, 2.703000068664551, 2.697999954223633, 2.720099925994873, 2.7084999084472656, 2.7058000564575195, 2.7183001041412354, 2.7200000286102295, 2.712399959564209, 2.7111001014709473, 2.7167999744415283, 2.6916000843048096, 2.6772000789642334, 2.6928999423980713, 2.6837000846862793, 2.713099956512451, 2.6912999153137207, 2.6953999996185303, 2.6763999462127686, 2.6338999271392822, 2.6133999824523926, 2.570199966430664, 2.277899980545044, 2.597899913787842, 2.495300054550171, 2.418600082397461, 2.3427999019622803, 2.163599967956543, 2.5975000858306885, 1.8460999727249146, 2.3773999214172363, 2.336899995803833, 1.8289999961853027, 2.188499927520752, 2.153899908065796, 1.8287999629974365, 1.9663000106811523, 1.8447999954223633, 2.341200113296509, 1.3398000001907349, 1.2170000076293945, 1.503100037574768, 0.7731999754905701, 1.8665000200271606, 0.9319999814033508, 0.4805999994277954, 0.7239000201225281, 0.39259999990463257, 1.1960999965667725, 0.07190000265836716, 0.4733999967575073, 0.4147999882698059, 0.02019999921321869, 1.6368000507354736, 1.4138000011444092, 0.6274999976158142, 0.3059999942779541, 0.6283000111579895, 2.860300064086914, 2.860100030899048, 2.860100030899048, 2.858799934387207, 2.8584001064300537, 2.8577001094818115, 2.857599973678589, 2.857599973678589, 2.8573999404907227, 2.857100009918213, 2.857100009918213, 2.8570001125335693, 2.856300115585327, 2.856100082397461, 2.856100082397461, 2.855799913406372, 2.855799913406372, 2.855799913406372, 2.8554999828338623, 2.8554999828338623, 2.8554999828338623, 2.8554000854492188, 2.8554000854492188, 2.855299949645996, 2.85509991645813, 2.8550000190734863, 2.85479998588562, 2.854599952697754, 2.8545000553131104, 2.8543999195098877, 2.8422000408172607, 2.851300001144409, 2.8429999351501465, 2.8541998863220215, 2.844099998474121, 2.849299907684326, 2.8231000900268555, 2.8489999771118164, 2.8427999019622803, 2.8317999839782715, 2.8529000282287598, 2.8538999557495117, 2.794600009918213, 2.8208999633789062, 2.838200092315674, 2.7816998958587646, 2.803800106048584, 2.8450000286102295, 2.7167999744415283, 2.7002999782562256, 2.6150999069213867, 2.7291998863220215, 2.562299966812134, 2.7328999042510986, 2.4507999420166016, 2.503700017929077, 2.550100088119507, 2.352099895477295, 2.213900089263916, 2.2070000171661377, 2.1517999172210693, 2.240000009536743, 1.964900016784668, 2.234499931335449, 1.4594000577926636, 1.9285999536514282, 1.875599980354309, 0.9502000212669373, 1.371500015258789, 0.9661999940872192, 1.090399980545044, 0.8833000063896179, -0.17100000381469727, 0.17579999566078186, 0.021299999207258224, 0.008899999782443047, 2.346299886703491, 2.3457999229431152, 2.3454999923706055, 2.345400094985962, 2.345099925994873, 2.3447999954223633, 2.344099998474121, 2.344099998474121, 2.3440001010894775, 2.3438000679016113, 2.343600034713745, 2.3431999683380127, 2.343100070953369, 2.3427999019622803, 2.3427000045776367, 2.342600107192993, 2.3424999713897705, 2.3424999713897705, 2.342099905014038, 2.3420000076293945, 2.3420000076293945, 2.3417000770568848, 2.341599941253662, 2.3415000438690186, 2.341399908065796, 2.3413000106811523, 2.34089994430542, 2.3406999111175537, 2.3405001163482666, 2.340399980545044, 2.3308000564575195, 2.339200019836426, 2.3359999656677246, 2.3294999599456787, 2.3369998931884766, 2.3401999473571777, 2.3396999835968018, 2.3250999450683594, 2.3131000995635986, 2.3313000202178955, 2.336899995803833, 2.337899923324585, 2.306299924850464, 2.2781999111175537, 2.2644999027252197, 2.3097000122070312, 2.2572999000549316, 2.2918999195098877, 2.2716000080108643, 2.3062000274658203, 2.2660000324249268, 2.2541000843048096, 2.3099000453948975, 2.1861000061035156, 1.9752999544143677, 2.1517999172210693, 1.9703999757766724, 2.063199996948242, 2.137700080871582, 2.130199909210205, 1.69159996509552, 0.6265000104904175, 0.9373999834060669, 0.3774000108242035, 0.7583000063896179, 1.0534000396728516, 0.6998999714851379, 1.3158999681472778, 1.315500020980835, 0.9487000107765198, 0.8773000240325928, 1.5205999612808228, 0.06390000134706497, -0.10869999974966049, 0.9768000245094299, 0.0575999990105629, 1.1093000173568726, 0.5368000268936157, 2.7363998889923096, 2.73580002784729, 2.7346999645233154, 2.734299898147583, 2.7334001064300537, 2.7330000400543213, 2.7330000400543213, 2.732800006866455, 2.7327001094818115, 2.732300043106079, 2.7321999073028564, 2.732100009918213, 2.7320001125335693, 2.7320001125335693, 2.731800079345703, 2.731800079345703, 2.7316999435424805, 2.7314000129699707, 2.7304999828338623, 2.73009991645813, 2.73009991645813, 2.7298998832702637, 2.72979998588562, 2.72979998588562, 2.7291998863220215, 2.7291998863220215, 2.7286999225616455, 2.728300094604492, 2.7281999588012695, 2.7279999256134033, 2.725399971008301, 2.72379994392395, 2.723599910736084, 2.721299886703491, 2.712100028991699, 2.7263998985290527, 2.712399959564209, 2.692500114440918, 2.697700023651123, 2.7167999744415283, 2.7195000648498535, 2.7091000080108643, 2.6656999588012695, 2.6261000633239746, 2.6763999462127686, 2.6942999362945557, 2.6849000453948975, 2.632699966430664, 2.7035999298095703, 2.536799907684326, 2.615600109100342, 2.5689001083374023, 2.627000093460083, 2.539799928665161, 2.3659000396728516, 2.255500078201294, 2.1185998916625977, 2.145400047302246, 1.0729000568389893, 1.833899974822998, 1.5506999492645264, 1.5095000267028809, 1.6406999826431274, 2.0218000411987305, 0.29330000281333923, 0.7793999910354614, 0.0210999995470047, 1.1180000305175781, -0.0885000005364418, -0.039000000804662704, 0.09189999848604202, 2.2360000610351562, 2.2358999252319336, 2.2358999252319336, 2.23580002784729, 2.234999895095825, 2.234999895095825, 2.234600067138672, 2.234499931335449, 2.233799934387207, 2.233799934387207, 2.233799934387207, 2.233799934387207, 2.2332000732421875, 2.233099937438965, 2.2321999073028564, 2.2320001125335693, 2.2320001125335693, 2.2314999103546143, 2.2309999465942383, 2.2309000492095947, 2.2309000492095947, 2.230799913406372, 2.230799913406372, 2.230799913406372, 2.2304999828338623, 2.2298998832702637, 2.22979998588562, 2.2297000885009766, 2.229599952697754, 2.2293999195098877, 2.2288999557495117, 2.222100019454956, 2.224900007247925, 2.21589994430542, 2.214600086212158, 2.2269999980926514, 2.226599931716919, 2.2125000953674316, 2.2260000705718994, 2.2288999557495117, 2.2153000831604004, 2.2267000675201416, 2.2021000385284424, 2.1366000175476074, 2.1888999938964844, 2.1705000400543213, 2.1784000396728516, 2.0732998847961426, 2.1152000427246094, 2.078900098800659, 2.080699920654297, 2.1435999870300293, 2.0759999752044678, 1.8414000272750854, 1.913599967956543, 1.72160005569458, 2.085700035095215, 1.8235000371932983, 1.7736999988555908, 1.4062000513076782, 1.8632999658584595, 1.5844999551773071, 1.363700032234192, 1.8545000553131104, 1.4204000234603882, 0.8294000029563904, 1.2752000093460083, 1.7454999685287476, 1.0633000135421753, 0.2606000006198883, 0.6044999957084656, 1.1124000549316406, 0.8855000138282776, 1.1476999521255493, 1.187999963760376, 0.28700000047683716, 0.13109999895095825, -0.2791999876499176, -0.19130000472068787, 1.0520000457763672, 0.6044999957084656, 0.8988000154495239, 2.528899908065796, 2.5288000106811523, 2.5285000801086426, 2.52839994430542, 2.5281999111175537, 2.5281999111175537, 2.52810001373291, 2.527899980545044, 2.527899980545044, 2.527899980545044, 2.5278000831604004, 2.5278000831604004, 2.5276999473571777, 2.5276999473571777, 2.5276999473571777, 2.5276999473571777, 2.527600049972534, 2.527600049972534, 2.527600049972534, 2.5274999141693115, 2.5274999141693115, 2.527400016784668, 2.5272998809814453, 2.5271999835968018, 2.5271999835968018, 2.5271999835968018, 2.527100086212158, 2.527100086212158, 2.527100086212158, 2.527100086212158, 2.5253000259399414, 2.526900053024292, 2.5227999687194824, 2.52620005607605, 2.5244998931884766, 2.5178000926971436, 2.527100086212158, 2.524399995803833, 2.5250000953674316, 2.5027999877929688, 2.5267999172210693, 2.5262999534606934, 2.518899917602539, 2.4967000484466553, 2.509200096130371, 2.5234999656677246, 2.444000005722046, 2.478800058364868, 2.5179998874664307, 2.4474000930786133, 2.4723000526428223, 2.500499963760376, 2.5072999000549316, 2.4528000354766846, 2.4732000827789307, 2.420099973678589, 2.356300115585327, 2.4630000591278076, 2.385200023651123, 2.4637999534606934, 1.4000999927520752, 2.009500026702881, 0.9916999936103821, 1.9559999704360962, 0.23019999265670776, 0.09520000219345093, 0.33379998803138733, 0.3416000008583069, 0.3635999858379364, 0.2467000037431717, 0.35440000891685486, -0.15700000524520874, -0.12129999697208405, -0.3382999897003174, 0.22840000689029694, 3.2053000926971436, 3.2053000926971436, 3.205199956893921, 3.2051000595092773, 3.2049999237060547, 3.2044999599456787, 3.2042999267578125, 3.2035000324249268, 3.203200101852417, 3.203000068664551, 3.2021000385284424, 3.2021000385284424, 3.201900005340576, 3.20169997215271, 3.2016000747680664, 3.201200008392334, 3.2005999088287354, 3.2004001140594482, 3.2002999782562256, 3.2002999782562256, 3.199700117111206, 3.199199914932251, 3.199199914932251, 3.199199914932251, 3.199199914932251, 3.1991000175476074, 3.1991000175476074, 3.198499917984009, 3.198199987411499, 3.197999954223633, 3.19320011138916, 3.1928000450134277, 3.194999933242798, 3.197000026702881, 3.184299945831299, 3.184000015258789, 3.196899890899658, 3.1944000720977783, 3.1905999183654785, 3.1935999393463135, 3.1763999462127686, 3.16129994392395, 3.1175999641418457, 3.171299934387207, 3.185499906539917, 3.1770999431610107, 3.152600049972534, 3.1749000549316406, 3.0922999382019043, 3.134500026702881, 3.1171998977661133, 2.988800048828125, 3.003999948501587, 3.0251998901367188, 2.808300018310547, 2.546999931335449, 2.9551000595092773, 2.648400068283081, 2.3568999767303467, 2.3959999084472656, 2.6868999004364014, 1.894700050354004, 2.757999897003174, 1.1842999458312988, 2.1805999279022217, 1.3834999799728394, 0.5271000266075134, -0.01549999974668026, 0.89410001039505, -0.4810999929904938, -0.05169999971985817, 1.1306999921798706, 0.5048999786376953, -0.3817000091075897, 3.1991000175476074, 3.197700023651123, 3.197700023651123, 3.1974000930786133, 3.197000026702881, 3.196700096130371, 3.1963999271392822, 3.1963000297546387, 3.196199893951416, 3.1958999633789062, 3.1958999633789062, 3.1958000659942627, 3.1956000328063965, 3.1956000328063965, 3.1956000328063965, 3.1956000328063965, 3.195499897003174, 3.195499897003174, 3.195499897003174, 3.195499897003174, 3.195499897003174, 3.195499897003174, 3.1953999996185303, 3.1953999996185303, 3.1953001022338867, 3.1951000690460205, 3.1951000690460205, 3.1951000690460205, 3.1949000358581543, 3.194700002670288, 3.1895999908447266, 3.19350004196167, 3.1810998916625977, 3.194200038909912, 3.1830999851226807, 3.1942999362945557, 3.194499969482422, 3.177500009536743, 3.186300039291382, 3.1944000720977783, 3.1747000217437744, 3.1891000270843506, 3.188499927520752, 3.1912999153137207, 3.190999984741211, 3.1846001148223877, 3.1047000885009766, 3.0896999835968018, 3.164900064468384, 3.1675000190734863, 3.069700002670288, 3.0977001190185547, 3.16759991645813, 2.8866000175476074, 3.093100070953369, 3.0097999572753906, 3.079699993133545, 2.881200075149536, 2.824199914932251, 2.761899948120117, 2.717099905014038, 2.7629001140594482, 2.634500026702881, 2.4914000034332275, 1.0115000009536743, 1.8805999755859375, 2.136199951171875, 0.9017000198364258, 1.9009000062942505, 0.4799000024795532, 0.4196999967098236, 2.2799999713897705, 2.289400100708008, 0.8122000098228455, 1.1606999635696411, 0.6211000084877014, -0.06700000166893005, -0.01940000057220459, -0.8058000206947327, 0.3776000142097473, -0.1876000016927719, 0.12610000371932983, 0.9153000116348267, -0.4408999979496002, 3.0090999603271484, 3.0081000328063965, 3.007999897003174, 3.007999897003174, 3.0078999996185303, 3.007699966430664, 3.007499933242798, 3.007200002670288, 3.0069000720977783, 3.0067999362945557, 3.006700038909912, 3.006700038909912, 3.006700038909912, 3.006700038909912, 3.0065999031066895, 3.0062999725341797, 3.0062999725341797, 3.0060999393463135, 3.00600004196167, 3.0058999061584473, 3.0058000087738037, 3.0055999755859375, 3.0048999786376953, 3.0048000812530518, 3.0048000812530518, 3.004699945449829, 3.0046000480651855, 3.004499912261963, 3.0044000148773193, 3.0044000148773193, 2.999300003051758, 2.9921000003814697, 3.0027999877929688, 3.0016000270843506, 3.001800060272217, 2.9953999519348145, 2.9986000061035156, 3.0007998943328857, 3.003499984741211, 2.9993999004364014, 2.997299909591675, 3.004300117492676, 2.963099956512451, 2.9823999404907227, 2.9758999347686768, 2.9979000091552734, 2.9971001148223877, 2.975399971008301, 2.965399980545044, 2.9835000038146973, 2.9007999897003174, 2.922800064086914, 2.8975000381469727, 2.7862000465393066, 2.846299886703491, 2.8187999725341797, 2.8292999267578125, 2.546099901199341, 2.476900100708008, 2.3457999229431152, 2.2423999309539795, 2.74780011177063, 2.477799892425537, 2.309799909591675, 2.4600000381469727, 2.3951001167297363, 2.5485999584198, 2.179800033569336, 2.49780011177063, 1.84660005569458, 2.686000108718872, 1.350100040435791, 0.07769999653100967, 0.2061000019311905, -0.11840000003576279, 1.0068000555038452, 0.5598999857902527, 0.2879999876022339, 0.024000000208616257, 0.36320000886917114, 1.235700011253357, 0.2590999901294708, 0.08259999752044678, 0.15289999544620514, -0.250900000333786, 2.821500062942505, 2.8203001022338867, 2.8203001022338867, 2.8201000690460205, 2.819999933242798, 2.8199000358581543, 2.8197999000549316, 2.819700002670288, 2.8196001052856445, 2.819200038909912, 2.8189001083374023, 2.8189001083374023, 2.8189001083374023, 2.8185999393463135, 2.81850004196167, 2.81850004196167, 2.8183999061584473, 2.8183000087738037, 2.81820011138916, 2.81820011138916, 2.81820011138916, 2.8180999755859375, 2.8180999755859375, 2.817699909210205, 2.817699909210205, 2.8176000118255615, 2.8176000118255615, 2.8176000118255615, 2.8176000118255615, 2.8176000118255615, 2.81469988822937, 2.8043999671936035, 2.815000057220459, 2.816200017929077, 2.815200090408325, 2.8160998821258545, 2.817500114440918, 2.8173999786376953, 2.8173000812530518, 2.8129000663757324, 2.8173000812530518, 2.815000057220459, 2.815000057220459, 2.79259991645813, 2.8115999698638916, 2.8145999908447266, 2.787600040435791, 2.789099931716919, 2.7983999252319336, 2.7297000885009766, 2.6893999576568604, 2.80049991607666, 2.6110000610351562, 2.719599962234497, 2.64520001411438, 2.804800033569336, 2.6284000873565674, 2.6791000366210938, 2.625200033187866, 2.666300058364868, 2.670099973678589, 2.5315001010894775, 2.5717999935150146, 2.7191998958587646, 2.5432000160217285, 2.732100009918213, 2.5917000770568848, 1.937399983406067, 2.3385000228881836, 2.023099899291992, 2.289400100708008, 1.7249000072479248, 1.8996000289916992, 1.3565000295639038, 2.2850000858306885, 2.254199981689453, 1.5219000577926636, 0.5917999744415283, 0.7067999839782715, 1.738800048828125, 0.5175999999046326, 0.4925999939441681, 0.9239000082015991, 0.9379000067710876, 0.4174000024795532, 0.566100001335144, 0.13210000097751617, 0.5400999784469604, 1.1705000400543213, 0.6385999917984009, -0.5095000267028809, 0.8235999941825867, 0.043699998408555984, 0.1467999964952469, -0.5253000259399414, -1.2537000179290771, 3.095099925994873, 3.0950000286102295, 3.094899892807007, 3.0947000980377197, 3.0947000980377197, 3.0943000316619873, 3.0943000316619873, 3.0943000316619873, 3.0941998958587646, 3.094099998474121, 3.094099998474121, 3.094099998474121, 3.094099998474121, 3.094099998474121, 3.0938000679016113, 3.0936999320983887, 3.093600034713745, 3.0934998989105225, 3.0933001041412354, 3.0933001041412354, 3.0931999683380127, 3.0931999683380127, 3.093100070953369, 3.0929999351501465, 3.0929999351501465, 3.092900037765503, 3.0927999019622803, 3.092600107192993, 3.092600107192993, 3.0924999713897705, 3.0924999713897705, 3.088900089263916, 3.0906999111175537, 3.0915000438690186, 3.078399896621704, 3.081399917602539, 3.092400074005127, 3.075200080871582, 3.067500114440918, 3.0748000144958496, 3.083699941635132, 3.054800033569336, 3.080699920654297, 3.076200008392334, 2.9628000259399414, 3.041599988937378, 2.6579999923706055, 3.05430006980896, 2.6749000549316406, 2.755199909210205, 2.8968000411987305, 1.8319000005722046, 2.621500015258789, 2.368000030517578, 2.583199977874756, 2.5434000492095947, 2.18969988822937, 2.001300096511841, 1.6763999462127686, 1.312999963760376, 1.690600037574768, 0.817799985408783, 2.5838000774383545, 0.9354000091552734, 1.4186999797821045, 0.2378000020980835, -0.398499995470047, -0.02590000070631504, 0.7149999737739563, 0.6858999729156494, 0.7366999983787537, 0.819599986076355, -0.3199999928474426, 0.42899999022483826, 0.07989999651908875, 2.5111000537872314, 2.5106000900268555, 2.5104000568389893, 2.5104000568389893, 2.510200023651123, 2.509999990463257, 2.509999990463257, 2.509999990463257, 2.5099000930786133, 2.5099000930786133, 2.5097999572753906, 2.5097999572753906, 2.5097999572753906, 2.509399890899658, 2.509399890899658, 2.509399890899658, 2.5092999935150146, 2.509200096130371, 2.509000062942505, 2.509000062942505, 2.5088999271392822, 2.508699893951416, 2.5085999965667725, 2.5085999965667725, 2.5085999965667725, 2.5085999965667725, 2.5085999965667725, 2.508500099182129, 2.5083000659942627, 2.5083000659942627, 2.5071001052856445, 2.5058999061584473, 2.5078999996185303, 2.5076000690460205, 2.505000114440918, 2.501800060272217, 2.5078001022338867, 2.504499912261963, 2.5027999877929688, 2.5027999877929688, 2.4848999977111816, 2.507499933242798, 2.5058999061584473, 2.4951000213623047, 2.488100051879883, 2.464900016784668, 2.4976999759674072, 2.500200033187866, 2.4296998977661133, 2.466900110244751, 2.4314000606536865, 2.4428999423980713, 2.3980000019073486, 2.4017999172210693, 2.4663000106811523, 2.107800006866455, 2.34060001373291, 2.4063000679016113, 2.001699924468994, 2.1064000129699707, 2.3366000652313232, 1.1052000522613525, 1.044800043106079, 1.6629999876022339, 1.8083000183105469, 1.6009000539779663, 1.600000023841858, 0.9990000128746033, 0.7070000171661377, 1.289199948310852, 1.0750000476837158, 0.18629999458789825, 0.7979000210762024, 0.9491000175476074, 1.1296000480651855, 1.1851999759674072, -0.1152999997138977, 0.5659000277519226, 0.9462000131607056], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.204999923706055, -8.444899559020996, -6.858099937438965, -8.413700103759766, -8.792699813842773, -8.792699813842773, -8.792699813842773, -8.798700332641602, -8.792799949645996, -8.792799949645996, -8.813899993896484, -8.76710033416748, -8.004199981689453, -8.899100303649902, -9.048500061035156, -8.787500381469727, -8.822099685668945, -8.701600074768066, -6.90500020980835, -9.094799995422363, -9.239999771118164, -9.205499649047852, -9.458700180053711, -9.387800216674805, -8.334099769592285, -9.438300132751465, -9.083399772644043, -8.722900390625, -6.690499782562256, -9.504799842834473, -3.8355000019073486, -4.063300132751465, -2.893899917602539, -7.050099849700928, -7.348400115966797, -7.348499774932861, -7.196199893951416, -6.649799823760986, -7.284999847412109, -7.3155999183654785, -7.98960018157959, -7.431399822235107, -6.797999858856201, -7.360000133514404, -5.426700115203857, -7.1697001457214355, -8.355299949645996, -3.991300106048584, -7.4882001876831055, -6.910099983215332, -5.163000106811523, -6.428199768066406, -6.430300235748291, -5.782700061798096, -7.605899810791016, -6.769199848175049, -7.071599960327148, -7.269199848175049, -4.782100200653076, -6.745100021362305, -3.9110000133514404, -5.394899845123291, -4.035799980163574, -4.131400108337402, -5.464300155639648, -4.441400051116943, -6.067699909210205, -4.871300220489502, -6.376200199127197, -5.464300155639648, -4.53879976272583, -4.48390007019043, -6.106100082397461, -6.151599884033203, -4.9980998039245605, -3.643199920654297, -3.7293999195098877, -4.784599781036377, -4.394700050354004, -4.743299961090088, -5.32859992980957, -4.688700199127197, -5.680600166320801, -5.780399799346924, -5.687300205230713, -5.3755998611450195, -5.117599964141846, -5.5802001953125, -5.218299865722656, -5.279399871826172, -5.230500221252441, -6.005899906158447, -7.27869987487793, -4.912300109863281, -6.9293999671936035, -7.644499778747559, -7.947299957275391, -7.2596001625061035, -4.385900020599365, -8.389900207519531, -8.29069995880127, -8.140999794006348, -8.391400337219238, -8.776200294494629, -7.870100021362305, -6.082499980926514, -8.432700157165527, -8.803999900817871, -8.829899787902832, -4.013999938964844, -8.763299942016602, -8.467499732971191, -8.65719985961914, -8.348400115966797, -3.467099905014038, -8.668600082397461, -8.89229965209961, -8.768099784851074, -8.912699699401855, -8.697600364685059, -5.757699966430664, -5.015600204467773, -6.145299911499023, -5.89300012588501, -4.790800094604492, -7.149600028991699, -7.409800052642822, -7.014400005340576, -8.23900032043457, -7.996399879455566, -7.58650016784668, -4.282199859619141, -7.134200096130371, -7.767000198364258, -7.494100093841553, -7.86959981918335, -3.0392000675201416, -7.339099884033203, -5.392300128936768, -6.081299781799316, -5.665299892425537, -5.882299900054932, -5.8495001792907715, -6.660299777984619, -5.426300048828125, -5.457600116729736, -5.561699867248535, -5.570400238037109, -4.789000034332275, -5.833600044250488, -5.3267998695373535, -5.486700057983398, -6.023499965667725, -4.894000053405762, -4.848299980163574, -5.219200134277344, -4.38700008392334, -5.159800052642822, -4.118100166320801, -5.318999767303467, -3.5213000774383545, -3.572700023651123, -4.68149995803833, -5.493100166320801, -5.400300025939941, -5.068299770355225, -4.995299816131592, -5.107100009918213, -4.862500190734863, -4.878499984741211, -5.4217000007629395, -5.270500183105469, -5.378300189971924, -5.344399929046631, -5.340099811553955, -7.93209981918335, -6.747799873352051, -7.936100006103516, -7.828100204467773, -7.929299831390381, -7.958799839019775, -6.3059000968933105, -7.000800132751465, -8.76729965209961, -5.468599796295166, -8.842599868774414, -6.552599906921387, -7.5971999168396, -8.711400032043457, -8.5556001663208, -7.434299945831299, -8.443499565124512, -8.430100440979004, -6.539599895477295, -8.54419994354248, -7.92140007019043, -8.077899932861328, -8.824600219726562, -7.2546000480651855, -9.093099594116211, -8.958000183105469, -8.840800285339355, -8.772899627685547, -9.31089973449707, -9.25730037689209, -4.766200065612793, -4.513000011444092, -4.979000091552734, -3.9955999851226807, -5.10290002822876, -5.0690999031066895, -5.730800151824951, -5.863500118255615, -5.972899913787842, -6.1869001388549805, -5.487299919128418, -5.993899822235107, -6.467299938201904, -6.437300205230713, -6.5854997634887695, -6.377299785614014, -6.7418999671936035, -3.1768999099731445, -6.372799873352051, -6.710000038146973, -6.4191999435424805, -7.043300151824951, -7.155200004577637, -4.097700119018555, -4.862400054931641, -4.477700233459473, -5.842800140380859, -5.206699848175049, -6.3007001876831055, -4.166299819946289, -4.513000011444092, -5.258699893951416, -6.333199977874756, -5.657700061798096, -5.924699783325195, -5.722300052642822, -6.01140022277832, -5.832600116729736, -5.803299903869629, -5.526500225067139, -5.787700176239014, -4.033699989318848, -5.1402997970581055, -4.165900230407715, -4.268799781799316, -5.251699924468994, -5.644000053405762, -4.700399875640869, -4.901800155639648, -5.2052001953125, -5.023099899291992, -4.894100189208984, -4.90369987487793, -5.0269999504089355, -5.465199947357178, -5.329699993133545, -5.264100074768066, -5.18179988861084, -5.353700160980225, -7.001399993896484, -6.828499794006348, -6.802199840545654, -5.839200019836426, -5.8267998695373535, -7.773600101470947, -5.157599925994873, -5.157700061798096, -8.088000297546387, -7.600200176239014, -7.761300086975098, -8.759699821472168, -8.489100456237793, -8.624799728393555, -8.325699806213379, -8.745800018310547, -8.625900268554688, -8.797699928283691, -8.648599624633789, -8.284000396728516, -8.942899703979492, -8.27079963684082, -8.598699569702148, -9.021599769592285, -8.928000450134277, -7.49459981918335, -8.684100151062012, -8.929499626159668, -8.974499702453613, -8.638199806213379, -4.383299827575684, -4.944399833679199, -4.524199962615967, -7.2804999351501465, -6.444300174713135, -6.255899906158447, -7.506400108337402, -8.03689956665039, -7.361599922180176, -7.28980016708374, -7.956200122833252, -5.771399974822998, -5.297500133514404, -6.155799865722656, -6.319300174713135, -7.974100112915039, -7.513299942016602, -7.628799915313721, -7.188399791717529, -6.69789981842041, -6.466800212860107, -6.124599933624268, -2.9804000854492188, -6.837900161743164, -6.247000217437744, -5.795100212097168, -5.595300197601318, -4.768700122833252, -6.932700157165527, -3.862799882888794, -6.018899917602539, -5.891300201416016, -4.139900207519531, -5.672500133514404, -5.608500003814697, -4.772900104522705, -5.15500020980835, -5.126999855041504, -6.173699855804443, -4.6367998123168945, -4.49970006942749, -4.965000152587891, -4.508500099182129, -5.610799789428711, -4.826099872589111, -4.460100173950195, -4.685699939727783, -4.573999881744385, -5.146500110626221, -4.482399940490723, -4.827199935913086, -4.829100131988525, -4.676499843597412, -5.470900058746338, -5.418399810791016, -5.303199768066406, -5.269599914550781, -5.374899864196777, -8.111100196838379, -4.926700115203857, -6.3246002197265625, -6.654399871826172, -8.317000389099121, -8.521200180053711, -7.719099998474121, -7.727200031280518, -8.42020034790039, -5.918700218200684, -8.575300216674805, -5.993500232696533, -8.561699867248535, -8.646499633789062, -8.760600090026855, -8.710399627685547, -8.998600006103516, -9.098099708557129, -8.885199546813965, -7.485300064086914, -5.225599765777588, -9.163200378417969, -9.163200378417969, -9.163200378417969, -8.45199966430664, -9.068599700927734, -9.080699920654297, -8.905699729919434, -8.638400077819824, -9.094900131225586, -3.822999954223633, -4.998700141906738, -4.69789981842041, -5.8531999588012695, -5.610400199890137, -6.420300006866455, -4.1585001945495605, -7.031799793243408, -6.624800205230713, -5.6975998878479, -8.274800300598145, -8.371899604797363, -5.213600158691406, -6.659999847412109, -7.559299945831299, -5.8414998054504395, -6.570000171661377, -8.068699836730957, -4.429800033569336, -5.0644001960754395, -3.780600070953369, -6.254799842834473, -4.252299785614014, -6.620699882507324, -3.799999952316284, -4.862400054931641, -5.3892998695373535, -4.4394001960754395, -4.194499969482422, -4.70389986038208, -4.56279993057251, -4.94350004196167, -4.261099815368652, -5.137899875640869, -3.7844998836517334, -4.801199913024902, -4.836599826812744, -3.9904000759124756, -4.457699775695801, -4.315400123596191, -4.485199928283691, -4.417300224304199, -4.165800094604492, -4.378499984741211, -4.6753997802734375, -5.019400119781494, -7.437099933624268, -6.85129976272583, -3.786600112915039, -3.7832999229431152, -7.322999954223633, -7.222599983215332, -8.26550006866455, -8.190899848937988, -7.636899948120117, -6.711400032043457, -8.406100273132324, -8.149900436401367, -9.255200386047363, -3.4625000953674316, -9.419500350952148, -3.987499952316284, -8.732999801635742, -6.2906999588012695, -6.537199974060059, -8.891500473022461, -8.793399810791016, -9.074999809265137, -8.694199562072754, -8.447099685668945, -6.730899810791016, -7.5, -8.831899642944336, -7.648099899291992, -9.413200378417969, -9.333399772644043, -4.872099876403809, -6.03000020980835, -6.023799896240234, -5.638800144195557, -6.488699913024902, -6.8246002197265625, -7.1570000648498535, -5.730199813842773, -4.67710018157959, -6.611400127410889, -7.242499828338623, -7.615499973297119, -4.8277997970581055, -3.5362000465393066, -3.1494998931884766, -5.791600227355957, -4.150899887084961, -5.589200019836426, -5.2652997970581055, -6.475800037384033, -5.866399765014648, -5.735499858856201, -6.712699890136719, -5.8420000076293945, -4.1855998039245605, -5.614099979400635, -5.140200138092041, -5.629700183868408, -5.926599979400635, -5.92710018157959, -5.202199935913086, -3.368299961090088, -4.3094000816345215, -3.657599925994873, -4.200699806213379, -4.57390022277832, -4.171199798583984, -4.9710001945495605, -5.043799877166748, -4.75029993057251, -4.828400135040283, -5.345099925994873, -4.632800102233887, -4.663000106811523, -5.230500221252441, -5.032899856567383, -5.317999839782715, -5.25439977645874, -7.370699882507324, -8.344499588012695, -5.958899974822998, -6.4421000480651855, -5.406700134277344, -8.628600120544434, -5.286799907684326, -7.4868998527526855, -5.960100173950195, -7.500400066375732, -3.6630001068115234, -6.338600158691406, -8.978699684143066, -7.74970006942749, -7.879799842834473, -9.142800331115723, -8.083600044250488, -8.795100212097168, -8.929699897766113, -7.842199802398682, -7.178299903869629, -6.993000030517578, -5.234000205993652, -7.574900150299072, -7.895100116729736, -8.410799980163574, -8.024299621582031, -6.429100036621094, -5.659800052642822, -9.478099822998047, -3.9934000968933105, -5.117499828338623, -5.735899925231934, -5.620200157165527, -4.82420015335083, -7.136600017547607, -6.069900035858154, -4.484600067138672, -4.958799839019775, -6.86359977722168, -7.125199794769287, -6.70389986038208, -4.6082000732421875, -3.0416998863220215, -5.174099922180176, -6.130799770355225, -5.8780999183654785, -4.8368000984191895, -6.707399845123291, -4.738500118255615, -5.725399971008301, -5.199100017547607, -5.920899868011475, -5.41480016708374, -4.458499908447266, -4.038899898529053, -4.337100028991699, -4.9120001792907715, -2.962100028991699, -4.665900230407715, -4.520299911499023, -4.808599948883057, -4.957600116729736, -5.306300163269043, -4.5777997970581055, -4.857900142669678, -4.533199787139893, -5.01200008392334, -4.785200119018555, -5.129499912261963, -5.154900074005127, -7.723100185394287, -8.79580020904541, -7.632699966430664, -4.118199825286865, -5.836400032043457, -5.627900123596191, -6.869900226593018, -4.940299987792969, -7.127799987792969, -5.837600231170654, -5.385799884796143, -7.822999954223633, -9.137200355529785, -7.436500072479248, -7.4440999031066895, -9.519800186157227, -9.547499656677246, -9.401200294494629, -9.093000411987305, -7.903200149536133, -8.808699607849121, -9.750499725341797, -8.342399597167969, -9.541399955749512, -9.192999839782715, -9.265000343322754, -9.827300071716309, -9.802000045776367, -9.899299621582031, -8.786499977111816, -3.2906999588012695, -3.8306000232696533, -6.402900218963623, -5.7393999099731445, -5.8242998123168945, -7.152699947357178, -7.298600196838379, -6.120500087738037, -7.540800094604492, -7.963699817657471, -6.886199951171875, -8.118499755859375, -7.276000022888184, -4.4953999519348145, -6.955599784851074, -6.373700141906738, -6.901899814605713, -4.804500102996826, -5.928800106048584, -5.447400093078613, -5.818900108337402, -6.599400043487549, -6.07390022277832, -4.008500099182129, -4.763700008392334, -3.3066999912261963, -6.263899803161621, -4.826900005340576, -4.585599899291992, -3.552799940109253, -5.583600044250488, -4.4969000816345215, -3.8831000328063965, -5.584099769592285, -4.206900119781494, -3.1654000282287598, -4.423699855804443, -5.496099948883057, -4.642300128936768, -3.774399995803833, -4.266600131988525, -4.817500114440918, -4.666800022125244, -5.139200210571289, -5.191800117492676, -5.013500213623047, -4.959400177001953, -4.833600044250488, -4.888000011444092, -5.312900066375732, -5.27239990234375, -5.308599948883057, -6.980199813842773, -7.1596999168396, -4.4492998123168945, -5.047900199890137, -6.447999954223633, -5.9344000816345215, -7.385300159454346, -5.529900074005127, -6.808800220489502, -7.348499774932861, -6.892000198364258, -7.88670015335083, -6.366199970245361, -8.055899620056152, -7.810500144958496, -3.6875, -7.841899871826172, -3.684799909591675, -7.690899848937988, -6.54580020904541, -6.613999843597412, -5.9608001708984375, -7.530600070953369, -6.457099914550781, -8.285499572753906, -7.3850998878479, -6.83620023727417, -7.663300037384033, -8.466400146484375, -7.180200099945068, -4.2683000564575195, -4.783999919891357, -5.261600017547607, -5.658199787139893, -5.773900032043457, -5.430200099945068, -6.406400203704834, -6.160799980163574, -6.385799884796143, -4.201900005340576, -6.959000110626221, -6.9105000495910645, -6.26230001449585, -4.650599956512451, -5.7683000564575195, -6.758200168609619, -3.223900079727173, -4.6803998947143555, -6.4969000816345215, -4.586299896240234, -5.239699840545654, -6.118000030517578, -6.316199779510498, -5.6356000900268555, -5.921000003814697, -5.425099849700928, -4.971199989318848, -5.934599876403809, -5.415599822998047, -6.067699909210205, -4.237299919128418, -5.538700103759766, -4.86899995803833, -5.698800086975098, -4.4664998054504395, -4.459099769592285, -4.606800079345703, -4.902299880981445, -4.918099880218506, -5.162799835205078, -5.221199989318848, -5.123600006103516, -5.2118000984191895, -5.209400177001953, -5.326900005340576, -6.087600231170654, -5.324999809265137, -4.878399848937988, -5.826399803161621, -6.683899879455566, -4.681000232696533, -5.400000095367432, -7.801300048828125, -5.876299858093262, -8.491100311279297, -7.630000114440918, -8.230500221252441, -8.240699768066406, -8.292699813842773, -7.426400184631348, -7.797399997711182, -4.3927998542785645, -7.346399784088135, -8.519000053405762, -4.4197001457214355, -8.602100372314453, -8.928400039672852, -8.365699768066406, -8.975500106811523, -8.695699691772461, -8.538200378417969, -8.262100219726562, -8.927499771118164, -8.713000297546387, -8.939299583435059, -4.351600170135498, -4.458499908447266, -5.0192999839782715, -5.717599868774414, -4.695799827575684, -5.760700225830078, -7.160799980163574, -6.9679999351501465, -6.761099815368652, -7.0756001472473145, -5.527599811553955, -4.3769001960754395, -3.6565001010894775, -6.392000198364258, -7.40369987487793, -7.178299903869629, -6.718599796295166, -7.193600177764893, -5.704400062561035, -6.509799957275391, -6.48330020904541, -5.070799827575684, -5.24370002746582, -5.7332000732421875, -4.815400123596191, -3.574700117111206, -5.969299793243408, -4.923600196838379, -3.9856998920440674, -4.251299858093262, -5.2829999923706055, -3.7427000999450684, -5.742400169372559, -4.370999813079834, -5.317399978637695, -4.84250020980835, -4.773399829864502, -4.569900035858154, -4.938000202178955, -4.516200065612793, -4.7484002113342285, -5.218900203704834, -5.152100086212158, -5.322299957275391, -6.669000148773193, -6.292300224304199, -7.777699947357178, -7.02239990234375, -7.134500026702881, -4.790999889373779, -7.919300079345703, -6.193600177764893, -7.464799880981445, -7.702899932861328, -7.625800132751465, -7.847499847412109, -8.357399940490723, -8.0108003616333, -8.361200332641602, -8.308099746704102, -8.361300468444824, -8.361300468444824, -8.38029956817627, -8.361300468444824, -8.361300468444824, -8.351300239562988, -8.361300468444824, -8.226699829101562, -7.1178998947143555, -8.451700210571289, -4.238800048828125, -8.293399810791016, -8.004199981689453, -7.717299938201904, -4.489200115203857, -4.885300159454346, -4.320899963378906, -6.214799880981445, -5.144700050354004, -6.319900035858154, -6.486000061035156, -4.83050012588501, -5.778900146484375, -7.2382001876831055, -5.548099994659424, -6.991199970245361, -7.099400043487549, -7.422100067138672, -7.440499782562256, -7.239099979400635, -4.146100044250488, -3.8922998905181885, -6.767199993133545, -6.9415998458862305, -4.908999919891357, -5.53249979019165, -7.100500106811523, -4.311500072479248, -6.567200183868408, -5.9953999519348145, -6.550600051879883, -5.543399810791016, -5.51200008392334, -5.361999988555908, -5.3109002113342285, -6.105100154876709, -5.890999794006348, -5.633600234985352, -3.0234999656677246, -5.211900234222412, -5.5121002197265625, -4.213600158691406, -5.282599925994873, -4.074399948120117, -4.2769999504089355, -5.695899963378906, -5.7067999839782715, -5.037700176239014, -5.209199905395508, -5.095699787139893, -4.9380998611450195, -4.986000061035156, -4.800600051879883, -5.177800178527832, -5.146599769592285, -5.283400058746338, -5.4028000831604, -5.381499767303467, -4.523499965667725, -6.6855998039245605, -7.533899784088135, -7.087500095367432, -7.611999988555908, -8.00160026550293, -8.082300186157227, -7.5183000564575195, -7.815299987792969, -8.25979995727539, -8.336600303649902, -8.382100105285645, -7.234499931335449, -7.9506001472473145, -8.391799926757812, -8.509300231933594, -7.711699962615967, -6.382999897003174, -8.32699966430664, -8.069600105285645, -8.25979995727539, -8.141900062561035, -8.076399803161621, -5.086299896240234, -8.112099647521973, -8.833999633789062, -8.838000297546387, -8.777099609375, -8.475299835205078, -8.751299858093262, -4.413099765777588, -4.018799781799316, -5.881199836730957, -5.988399982452393, -6.048299789428711, -5.689899921417236, -6.505300045013428, -6.99399995803833, -7.325399875640869, -6.9492998123168945, -6.849800109863281, -7.984899997711182, -5.42710018157959, -6.472799777984619, -6.450200080871582, -7.573500156402588, -7.627799987792969, -6.809899806976318, -6.488800048828125, -7.212299823760986, -5.525300025939941, -6.556000232696533, -6.3246002197265625, -5.071700096130371, -5.831099987030029, -6.03849983215332, -6.164700031280518, -4.813600063323975, -4.738900184631348, -4.252500057220459, -3.879199981689453, -6.088799953460693, -5.347599983215332, -5.132900238037109, -5.536900043487549, -5.470399856567383, -5.890699863433838, -5.475100040435791, -5.912300109863281, -5.287700176239014, -6.184500217437744, -5.045599937438965, -4.476600170135498, -4.734499931335449, -4.815100193023682, -5.359799861907959, -5.19290018081665, -5.267300128936768, -5.257699966430664, -5.3643999099731445, -5.625500202178955, -5.499000072479248, -5.493000030517578, -5.5040998458862305, -5.494800090789795, -7.343500137329102, -8.246100425720215, -8.060700416564941, -8.153400421142578, -8.014399528503418, -8.536999702453613, -8.540399551391602, -8.574700355529785, -7.526599884033203, -8.723099708557129, -8.179100036621094, -8.367899894714355, -8.517499923706055, -7.458799839019775, -7.409800052642822, -8.0378999710083, -5.974899768829346, -8.919899940490723, -8.920000076293945, -8.836999893188477, -8.920100212097168, -8.91569995880127, -7.9730000495910645, -6.843999862670898, -7.256400108337402, -7.606299877166748, -8.590999603271484, -8.673800468444824, -8.656299591064453, -8.649900436401367, -6.3084001541137695, -5.299099922180176, -6.426000118255615, -6.616199970245361, -7.124100208282471, -7.4969000816345215, -7.9847002029418945, -7.986000061035156, -8.038299560546875, -7.603099822998047, -8.05840015411377, -7.867199897766113, -7.898399829864502, -5.7017998695373535, -7.779300212860107, -7.964900016784668, -6.658299922943115, -6.791999816894531, -7.446100234985352, -5.655300140380859, -4.701300144195557, -7.580900192260742, -3.938199996948242, -6.069300174713135, -4.656000137329102, -7.722499847412109, -4.636000156402588, -5.553899765014648, -4.802800178527832, -5.6844000816345215, -6.029399871826172, -4.461999893188477, -5.565999984741211, -6.790500164031982, -5.4842000007629395, -6.921199798583984, -6.231900215148926, -3.0292000770568848, -5.342199802398682, -4.058199882507324, -5.532700061798096, -4.002699851989746, -5.147900104522705, -4.352399826049805, -6.004199981689453, -5.9770002365112305, -5.155399799346924, -4.348800182342529, -4.574900150299072, -5.509500026702881, -4.441400051116943, -4.751299858093262, -5.052700042724609, -5.065299987792969, -4.883200168609619, -5.009500026702881, -4.9583001136779785, -5.176700115203857, -5.3292999267578125, -5.222099781036377, -5.063799858093262, -5.285099983215332, -5.214600086212158, -5.262800216674805, -5.222099781036377, -5.248499870300293, -6.295100212097168, -3.474400043487549, -3.5311999320983887, -6.303100109100342, -6.765500068664551, -7.7434000968933105, -7.781099796295166, -6.842899799346924, -6.442699909210205, -6.345600128173828, -7.606900215148926, -7.321800231933594, -7.352200031280518, -5.197500228881836, -7.663700103759766, -7.777699947357178, -6.781199932098389, -7.3541998863220215, -7.492499828338623, -7.418099880218506, -6.19890022277832, -5.63070011138916, -7.824900150299072, -8.109700202941895, -8.030200004577637, -8.128399848937988, -7.970699787139893, -6.981299877166748, -7.959700107574463, -7.623899936676025, -2.945499897003174, -5.2600998878479, -5.5879998207092285, -5.746399879455566, -4.766499996185303, -5.75540018081665, -7.462600231170654, -5.795199871063232, -5.311999797821045, -6.338200092315674, -6.86269998550415, -5.551199913024902, -6.848499774932861, -6.996500015258789, -4.835000038146973, -6.360899925231934, -2.4572999477386475, -6.957399845123291, -4.27810001373291, -5.089099884033203, -6.301199913024902, -3.1963000297546387, -5.524899959564209, -4.851399898529053, -5.716700077056885, -5.63700008392334, -4.9481000900268555, -4.892499923706055, -5.0192999839782715, -4.847899913787842, -5.3053998947143555, -4.839200019836426, -5.9704999923706055, -4.981400012969971, -5.29580020904541, -4.702899932861328, -4.3933000564575195, -4.5802001953125, -4.98390007019043, -5.06689977645874, -5.095399856567383, -5.180600166320801, -5.0167999267578125, -5.198299884796143, -5.163899898529053, -6.453100204467773, -4.985000133514404, -7.852099895477295, -7.884699821472168, -7.775300025939941, -7.171899795532227, -7.715400218963623, -7.942800045013428, -8.081700325012207, -5.633900165557861, -8.426199913024902, -7.707300186157227, -8.325900077819824, -7.818399906158447, -8.251199722290039, -7.902699947357178, -4.716000080108643, -7.507500171661377, -7.967899799346924, -7.721099853515625, -8.558799743652344, -8.011699676513672, -8.39050006866455, -7.191199779510498, -7.600399971008301, -8.407699584960938, -6.22730016708374, -5.718699932098389, -6.78879976272583, -7.969600200653076, -3.131700038909912, -4.575500011444092, -4.971099853515625, -5.0833001136779785, -5.0233001708984375, -4.882999897003174, -5.530200004577637, -5.981400012969971, -6.010000228881836, -6.035399913787842, -4.281000137329102, -6.622000217437744, -7.108799934387207, -6.192800045013428, -5.939499855041504, -4.810500144958496, -6.674600124359131, -7.195000171661377, -4.239999771118164, -6.051700115203857, -5.234000205993652, -5.922699928283691, -5.670499801635742, -5.728600025177002, -6.638899803161621, -3.863800048828125, -5.656300067901611, -6.250999927520752, -4.4257001876831055, -5.186200141906738, -6.004300117492676, -2.8896000385284424, -2.9902000427246094, -4.448599815368652, -4.860300064086914, -4.6331000328063965, -4.808499813079834, -4.116300106048584, -4.164100170135498, -4.961599826812744, -4.841800212860107, -4.368000030517578, -4.757400035858154, -4.880099773406982, -4.979100227355957, -5.02209997177124, -4.811999797821045, -4.986400127410889, -5.054100036621094]}, \"token.table\": {\"Topic\": [1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 1, 1, 4, 7, 3, 5, 9, 12, 13, 3, 13, 3, 13, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 3, 13, 13, 1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 8, 9, 10, 12, 14, 15, 1, 2, 3, 5, 6, 7, 8, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 10, 11, 5, 9, 10, 11, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 5, 6, 8, 10, 14, 15, 1, 5, 6, 7, 8, 10, 12, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 1, 2, 3, 5, 6, 7, 8, 10, 11, 14, 15, 9, 3, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 9, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 13, 5, 5, 1, 3, 4, 8, 9, 15, 5, 15, 3, 4, 6, 7, 8, 9, 12, 13, 15, 3, 13, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 5, 15, 8, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 7, 13, 14, 1, 4, 15, 3, 13, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 6, 7, 9, 11, 12, 13, 14, 15, 12, 1, 3, 5, 6, 8, 9, 10, 11, 12, 14, 15, 4, 8, 15, 2, 9, 10, 13, 6, 1, 2, 6, 7, 8, 11, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 5, 3, 4, 5, 9, 10, 12, 13, 15, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 2, 4, 6, 7, 8, 11, 13, 14, 15, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 6, 7, 9, 11, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 6, 7, 9, 11, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 5, 6, 7, 8, 10, 14, 15, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 2, 5, 6, 7, 8, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 14, 6, 8, 10, 11, 1, 2, 3, 4, 7, 9, 15, 1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 15, 8, 11, 15, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 8, 9, 10, 12, 13, 1, 2, 5, 6, 7, 8, 11, 14, 15, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 8, 1, 2, 4, 5, 6, 8, 9, 10, 12, 14, 15, 12, 9, 12, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 6, 7, 8, 10, 11, 12, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 8, 11, 1, 2, 5, 8, 10, 11, 15, 8, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 6, 7, 8, 11, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 1, 2, 4, 6, 7, 8, 9, 12, 13, 14, 15, 4, 6, 7, 9, 10, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 8, 9, 10, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 6, 7, 8, 15, 1, 2, 4, 5, 6, 7, 8, 11, 14, 15, 6, 8, 11, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 9, 10, 12, 2, 7, 9, 10, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 5, 6, 7, 8, 10, 11, 14, 15, 1, 2, 6, 15, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 3, 12, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 5, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 9, 12, 13, 14, 12, 1, 2, 3, 9, 10, 12, 2, 4, 9, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 6, 9, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 6, 8, 11, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 2, 3, 4, 5, 9, 11, 13, 15, 11, 4, 6, 8, 10, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 5, 6, 10, 11, 12, 14, 15, 1, 6, 13, 14, 15, 1, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 15, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 9, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 9, 10, 11, 14, 15, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 10, 14, 15, 1, 2, 4, 7, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 14, 4, 5, 2, 3, 4, 5, 9, 11, 12, 13, 15, 11, 11, 11, 1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 6, 10, 13, 4, 7, 10, 12, 15, 1, 3, 4, 10, 12, 13, 3, 5, 9, 10, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 8, 10, 11, 12, 15, 3, 12, 15, 1, 3, 4, 5, 9, 10, 11, 12, 13, 14, 15, 10, 13, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 9, 10, 12, 13, 15, 3, 4, 5, 6, 9, 10, 11, 12, 14, 15, 3, 12, 13, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 1, 2, 4, 6, 9, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 7, 8, 10, 11, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 15, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 8, 13, 15, 4, 5, 1, 4, 6, 8, 10, 14, 15, 1, 3, 6, 7, 8, 10, 14, 5, 8, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 1, 2, 7, 8, 11, 12, 13, 15, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 7, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 7, 11, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 9, 11, 13, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 10, 2, 3, 4, 5, 7, 9, 15, 4, 7, 9, 10, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 9, 12, 13, 15, 4, 1, 2, 4, 9, 10, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 7, 8, 10, 15, 3, 7, 8, 10, 12, 15, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 6, 10, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 12, 13, 14, 15, 2, 3, 7, 8, 9, 10, 12, 13, 15, 1, 2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 7, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 7, 8, 11, 3, 7, 9, 11, 11, 11, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 12, 5, 1, 13, 1, 1, 1, 2, 4, 5, 6, 7, 8, 9, 12, 13, 15, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 13, 2, 4, 5, 9, 10, 12, 13, 14, 1, 4, 7, 9, 10, 11, 14, 15, 1, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 5, 6, 7, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 11, 13, 1, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 1, 3, 5, 7, 11, 12, 13, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 4, 1, 3, 4, 3, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 4, 6, 7, 9, 10, 12, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 3, 12, 1, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 8, 10, 14, 15, 3, 6, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 15, 3, 12, 1, 2, 5, 1, 2, 4, 6, 7, 8, 11, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 9, 10, 11, 12, 14, 10, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 2, 7, 8, 13, 4, 9, 13, 15, 9, 12, 4, 6, 9, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 9, 11, 14, 14, 9, 10, 14, 10, 5, 6, 14, 15, 5, 13, 4, 14, 4, 5, 7, 9, 12, 13, 15, 1, 2, 3, 4, 7, 9, 10, 7, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 6, 7, 8, 11, 13, 15, 1, 2, 4, 6, 7, 9, 10, 11, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 8, 1, 5, 6, 7, 10, 15, 6, 11, 4, 7, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 2, 10, 1, 2, 3, 4, 6, 7, 8, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15, 1, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 7, 8, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 6, 7, 8, 9, 10, 11, 13, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 9, 10, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 7, 8, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 6, 8, 10, 11, 13, 15, 4, 1, 2, 3, 4, 7, 11, 13, 15, 1, 4, 7, 4, 9, 10, 14, 2, 3, 11, 13, 2, 3, 4, 5, 11, 4, 4, 6, 9, 15, 8, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 1, 2, 3, 5, 6, 7, 8, 11, 14, 15, 2, 6, 14, 15, 3, 10, 15, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 4, 9, 2, 4, 5, 7, 9, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 8, 9, 4, 6, 9, 12, 13, 15, 4, 9, 10, 12, 13, 15, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 5, 6, 7, 9, 11, 12, 13, 15, 2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 15, 1, 4, 9, 10, 12, 13, 15, 9, 12, 15, 4, 8, 9, 12, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 5, 10, 13, 14, 2, 3, 4, 5, 7, 9, 10, 13, 14, 15, 8, 1, 5, 7, 8, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 13, 12, 1, 5, 6, 7, 8, 11, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 2, 15, 2, 3, 5, 8, 12, 13, 1, 2, 5, 6, 7, 8, 11, 15, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 1, 4, 1, 4, 8, 9, 10, 12, 8, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 6, 8, 9, 12, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 4, 5, 6, 7, 8, 9, 11, 13, 14, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 5, 6, 8, 10, 11, 14, 15, 1, 4, 9, 10, 12, 13, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 3, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 3, 4, 5, 9, 10, 11, 12, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 5, 9, 10, 11, 12, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 5, 6, 8, 10, 11, 13, 14, 15, 1, 4, 9, 11, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 6, 8, 12, 13, 14, 3, 5, 3, 3, 4, 5, 7, 9, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 3, 5, 7, 8, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 6, 7, 8, 9, 12, 13, 14, 15, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 7, 8, 13, 11, 2, 3, 4, 9, 10, 11, 13, 14, 2, 4, 10, 11, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 8, 15, 2, 6, 15, 1, 6, 7, 8, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 7, 15, 1, 3, 7, 9, 13, 14, 3, 13, 7, 15, 6, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 6, 8, 10, 11, 13, 14, 15, 12, 14, 2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 6, 7, 11, 13, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 4, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 11, 1, 3, 9, 4, 7, 8, 10, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 1, 2, 3, 4, 6, 7, 8, 10, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 2, 3, 10, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 11, 8, 2, 3, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 10, 1, 3, 5, 9, 11, 12, 14, 15, 6, 3, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 10, 15, 7, 8, 2, 3, 10, 12, 13, 14, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 3, 4, 5, 7, 9, 10, 11, 12, 13, 15, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 9, 10, 14, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 6, 7, 8, 10, 12, 14, 15, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 15, 1, 2, 4, 5, 9, 10, 12, 13, 14, 9, 14, 4, 9, 10, 14, 9, 14, 14, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 1, 3, 4, 11, 4, 5, 9, 12, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 10, 3, 4, 5, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 5, 6, 14, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 9, 11, 3, 4, 5, 6, 7, 9, 12, 13, 14, 15, 10, 1, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 9, 15, 4, 9, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 15, 9, 10, 12, 3, 7, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 5, 7, 11, 12, 3, 3, 5, 7, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 7, 12, 3, 4, 6, 7, 9, 10, 11, 12, 13, 15, 2, 4, 13, 15, 4, 8, 13, 3, 4, 6, 9, 10, 14, 10, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 6, 10, 13, 14, 1, 3, 4, 5, 7, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 5, 6, 8, 9, 10, 12, 13, 14, 15, 1, 3, 6, 12, 14, 3, 5, 7, 12, 13, 3, 9, 13, 14, 3, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 10, 4, 9, 10, 13, 3, 5, 11, 14, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 3, 9, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 7, 9, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 9, 1, 3, 6, 7, 11, 13, 15, 3, 13, 14, 10, 14, 4, 5, 9, 14, 2, 4, 5, 6, 9, 10, 12, 14, 4, 6, 10, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 9, 5, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 9, 10, 11, 12, 14, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 4, 2, 4, 5, 6, 10, 13, 4, 3, 5, 7, 15, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 6, 9, 10, 11, 14, 15, 7, 4, 13, 4, 12, 10, 3, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 4, 3, 4, 5, 10, 13, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 4, 6, 8, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 6, 8, 11, 14, 15, 3, 7, 11, 15, 7, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 1, 4, 5, 6, 7, 8, 11, 12, 13, 15, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 2, 5, 13, 15, 2, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 6, 7, 9, 10, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 9, 10, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 1, 3, 4, 5, 9, 12, 13, 3, 9, 13, 7, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 7, 10, 12, 15, 2, 3, 4, 5, 8, 10, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 12, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 6, 7, 8, 10, 11, 12, 15, 1, 2, 3, 5, 7, 8, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 15, 4, 7, 11, 13, 14, 1, 11, 13, 14, 1, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 6, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 6, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 2, 6, 15, 2, 6, 8, 12, 15, 5, 3, 4, 5, 8, 14, 15, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 15, 12, 12, 1, 2, 3, 6, 7, 8, 10, 11, 12, 15, 3, 5, 6, 8, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 3, 4, 5, 7, 12, 14, 10, 11, 14, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 1, 2, 3, 4, 5, 7, 8, 10, 11, 12, 15, 1, 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 15, 2, 15, 7, 8, 15, 12, 5, 7, 8, 10, 11, 15, 1, 2, 3, 4, 5, 6, 9, 11, 12, 13, 14, 15, 7, 4, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 6, 8, 9, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 5, 15, 2, 3, 4, 14, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 7, 9, 10, 11, 13, 14, 15, 1, 2, 3, 4, 5, 7, 8, 9, 10, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 7, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 6, 7, 12, 13, 14, 15, 2, 6, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 1, 4, 7, 9, 2, 3, 10, 12, 15, 4, 1, 3, 6, 8, 15, 1, 6, 15, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 11, 1, 4, 9, 10, 11, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 6, 7, 8, 11, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 1, 2, 4, 5, 6, 8, 11, 13, 14, 15, 3, 5, 7, 1, 2, 4, 5, 6, 8, 10, 11, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 6, 9, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 7, 13, 14, 15, 6, 9, 11, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 4, 9, 10, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 4, 5, 6, 7, 8, 10, 12, 13, 15, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 15, 2, 6, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 3, 5, 7, 11, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 3, 11, 1, 2, 3, 5, 7, 8, 10, 11, 15, 4, 7, 15, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 1, 5, 10, 12, 13, 1, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 6, 9, 10, 11, 12, 13, 14, 11, 1, 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 6, 15, 8, 8, 1, 2, 5, 6, 7, 8, 10, 11, 12, 14, 15, 5, 12, 1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 5, 11, 15, 1, 2, 3, 5, 6, 8, 10, 11, 15, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 9, 10, 13, 15, 3, 9, 13, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 6, 7, 8, 9, 12, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 6, 3, 13, 1, 2, 4, 12, 13, 14, 1, 2, 4, 5, 6, 7, 8, 11, 14, 15, 1, 9, 5, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 3, 4, 9, 1, 4, 6, 8, 11, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 7, 8, 9, 10, 11, 14, 15, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 4, 9, 10, 14, 9, 2, 6, 9, 12, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 13, 15, 2, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 2, 9, 15, 14, 2, 4, 5, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 7, 8, 9, 10, 12, 13, 15, 2, 4, 5, 10, 15, 6, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 5, 8, 11, 12, 14, 15, 3, 7, 8, 2, 11, 2, 3, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 5, 7, 8, 10, 11, 13, 15, 2, 9, 11, 14, 1, 3, 5, 6, 8, 10, 14, 15, 2, 13, 15, 11, 13, 2, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 15, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 4, 5, 6, 9, 10, 11, 13, 14, 15, 4, 9, 10, 11, 13, 14, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 9, 11, 13, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 6, 8, 2, 3, 15, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 6, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 1, 2, 3, 4, 6, 7, 8, 15, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13, 1, 13, 13, 13, 11, 11, 11, 11, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 5, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 2, 11, 5, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 6, 13, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 11, 12, 11, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 13], \"Freq\": [0.9883806109428406, 0.00018658273620530963, 7.46330915717408e-05, 0.0002985323662869632, 0.00011194963735761121, 0.9789996147155762, 0.0006716978386975825, 0.0002985323662869632, 0.0005224316264502704, 3.73165457858704e-05, 0.00048511510249227285, 0.00018658273620530963, 7.46330915717408e-05, 3.73165457858704e-05, 0.00070901436265558, 0.017277561128139496, 0.025250406935811043, 0.03948942944407463, 0.18893367052078247, 0.14154821634292603, 0.14999620616436005, 0.007587284315377474, 0.029217125847935677, 0.01624109223484993, 0.11445479094982147, 0.01961841620504856, 0.03574724122881889, 0.05042596906423569, 0.1202738881111145, 0.02490425482392311, 0.03631792590022087, 0.08299435675144196, 0.04727255925536156, 0.09285528212785721, 0.08930449187755585, 0.04447042942047119, 0.054438307881355286, 0.03636350482702255, 0.007058798335492611, 0.0639142096042633, 0.027165677398443222, 0.02017104998230934, 0.04010680690407753, 0.1351652890443802, 0.008021361194550991, 0.2506729066371918, 0.9954754710197449, 0.9944583177566528, 0.9418198466300964, 0.0006864576134830713, 0.05628952383995056, 0.00534503348171711, 0.0005938926478847861, 0.0011877852957695723, 0.0005938926478847861, 0.9912068247795105, 0.0024650366976857185, 0.9934097528457642, 0.002336519304662943, 0.9930206537246704, 0.0002876215148717165, 0.0002876215148717165, 0.0008628644864074886, 0.001150486059486866, 0.9908560514450073, 0.001150486059486866, 0.0002876215148717165, 0.0008628644864074886, 0.0002876215148717165, 0.001150486059486866, 0.0002876215148717165, 0.0014381075743585825, 0.001150486059486866, 0.0025147299747914076, 0.9933183193206787, 0.9915610551834106, 0.0003080096794292331, 0.0003080096794292331, 0.0006160193588584661, 0.9936391711235046, 0.0012320387177169323, 0.0003080096794292331, 0.0012320387177169323, 0.0006160193588584661, 0.0003080096794292331, 0.0003080096794292331, 0.0006160193588584661, 0.0009240289800800383, 0.9890954494476318, 0.04661819338798523, 0.04812822863459587, 0.11068494617938995, 0.10564262419939041, 0.14761097729206085, 0.040311966091394424, 0.030792752280831337, 0.042680125683546066, 0.11109072715044022, 0.02759307436645031, 0.026196124032139778, 0.060501206666231155, 0.10720587521791458, 0.05726826936006546, 0.03766441345214844, 0.9947057366371155, 0.004232790321111679, 0.1291073113679886, 0.06351731717586517, 0.13880205154418945, 0.10142713040113449, 0.03831097111105919, 0.026744134724140167, 0.040450502187013626, 0.006418592296540737, 0.022999955341219902, 0.02814820036292076, 0.01263660378754139, 0.033229585736989975, 0.33757781982421875, 0.013973809778690338, 0.006619173102080822, 0.1613793671131134, 0.03201022744178772, 0.0037190306466072798, 0.00889910850673914, 0.007703706156462431, 0.07643935829401016, 0.02012261189520359, 0.6104522943496704, 0.010094511322677135, 0.0017931039910763502, 0.006641125772148371, 0.007039593532681465, 0.007570883724838495, 0.0034533855505287647, 0.04263602942228317, 0.05545208975672722, 0.7813703417778015, 0.00789041630923748, 0.0291507039219141, 0.008986307308077812, 0.016219189390540123, 0.024767139926552773, 0.0170959010720253, 0.00043835645192302763, 0.014246584847569466, 0.0006575346924364567, 0.00876712892204523, 0.017315080389380455, 0.003068495076149702, 0.014904119074344635, 0.022617124021053314, 0.9052433967590332, 0.0013961187796667218, 0.002513013780117035, 0.006980593781918287, 0.014798859134316444, 0.01619497872889042, 0.00809748936444521, 0.00027922375011257827, 0.0008376712794415653, 0.002513013780117035, 0.003909132443368435, 0.004746804013848305, 0.009772831574082375, 0.043955326080322266, 0.7794744372367859, 0.01934034377336502, 0.0031257120426744223, 0.021879984065890312, 0.022270698100328445, 0.026177838444709778, 0.007423566188663244, 0.0005860710516571999, 0.013479633256793022, 0.0017582130385562778, 0.01074463501572609, 0.021293913945555687, 0.012112134136259556, 0.016409989446401596, 0.02507968805730343, 0.9087903499603271, 0.001741645042784512, 0.001741645042784512, 0.002786631928756833, 0.008011567406356335, 0.014629817567765713, 0.003831618931144476, 0.00034832899109460413, 0.003831618931144476, 0.0010449870023876429, 0.0006966579821892083, 0.0020899740047752857, 0.014978147111833096, 0.010449869558215141, 0.005375029519200325, 8.95838238648139e-05, 8.95838238648139e-05, 8.95838238648139e-05, 0.9797782897949219, 0.009316718205809593, 0.0001791676477296278, 0.0012541735777631402, 8.95838238648139e-05, 0.00026875146431848407, 0.0003583352954592556, 8.95838238648139e-05, 8.95838238648139e-05, 0.0019708441104739904, 0.0009854220552369952, 0.0003792817296925932, 0.0007585634593851864, 0.0003792817296925932, 0.8742443919181824, 0.08533839136362076, 0.0003792817296925932, 0.0003792817296925932, 0.0003792817296925932, 0.0003792817296925932, 0.0003792817296925932, 0.03641104698181152, 0.00017124255828093737, 8.562127914046869e-05, 0.00017124255828093737, 0.0010274553205817938, 8.562127914046869e-05, 0.00034248511656187475, 0.00017124255828093737, 0.00017124255828093737, 8.562127914046869e-05, 8.562127914046869e-05, 8.562127914046869e-05, 0.9974022507667542, 0.04220420867204666, 0.043339263647794724, 0.10353318601846695, 0.09891189634799957, 0.24560417234897614, 0.01845814846456051, 0.025962119922041893, 0.0249802079051733, 0.11195601522922516, 0.025205416604876518, 0.0316644161939621, 0.03830358758568764, 0.09707418829202652, 0.048906438052654266, 0.04387976601719856, 0.002283401321619749, 0.0015222675865516067, 0.0015222675865516067, 0.9932795763015747, 0.9929620623588562, 0.001318567548878491, 0.0006592837744392455, 0.0006592837744392455, 0.9961777925491333, 0.05306948348879814, 0.05608193948864937, 0.08566679805517197, 0.08871690928936005, 0.1698148101568222, 0.06801880896091461, 0.03390271216630936, 0.019028697162866592, 0.11340651661157608, 0.02679833024740219, 0.018639588728547096, 0.053471144288778305, 0.10448211431503296, 0.04399444907903671, 0.06486827880144119, 0.0004142856923863292, 0.0004142856923863292, 0.0016571427695453167, 0.9922142624855042, 0.0004142856923863292, 0.0004142856923863292, 0.0004142856923863292, 0.0037285713478922844, 0.8585143089294434, 0.0026530108880251646, 0.00477541983127594, 0.0005306021776050329, 0.0005306021776050329, 0.0010612043552100658, 0.0005306021776050329, 0.13052813708782196, 0.00016369568766094744, 0.00016369568766094744, 0.0003273913753218949, 0.00016369568766094744, 0.0004910870338790119, 0.00016369568766094744, 0.00016369568766094744, 0.0004910870338790119, 0.0004910870338790119, 0.00016369568766094744, 0.00016369568766094744, 0.00016369568766094744, 0.9967430233955383, 0.0009848111076280475, 0.00016413518460467458, 0.00032827036920934916, 0.07172707468271255, 0.08912540227174759, 0.00032827036920934916, 0.20664618909358978, 0.00016413518460467458, 0.00016413518460467458, 0.6210875511169434, 0.009191569872200489, 0.9961249232292175, 0.002842152724042535, 0.9947534203529358, 0.9963281750679016, 0.07807697355747223, 0.039464179426431656, 0.10502079874277115, 0.058311499655246735, 0.11744099110364914, 0.007378666661679745, 0.030967028811573982, 0.015308229252696037, 0.214498832821846, 0.01802932098507881, 0.03271988034248352, 0.03433917835354805, 0.11234938353300095, 0.024523215368390083, 0.11159815639257431, 0.02136087231338024, 0.010324421338737011, 0.1046682745218277, 0.07381368428468704, 0.025158360600471497, 0.002848116448149085, 0.017563384026288986, 0.008188334293663502, 0.025751719251275063, 0.005458889529109001, 0.06253989040851593, 0.03370270878076553, 0.5854066014289856, 0.02017415687441826, 0.0030854593496769667, 0.9959783554077148, 0.0015159487957134843, 0.07581138610839844, 0.04739222675561905, 0.06660791486501694, 0.024601902812719345, 0.22494323551654816, 0.07125008851289749, 0.023874036967754364, 0.007521292194724083, 0.09193768352270126, 0.01606159843504429, 0.007618341129273176, 0.031217405572533607, 0.09358751773834229, 0.008297683671116829, 0.20926983654499054, 0.05502689257264137, 0.007775539066642523, 0.0021931007504463196, 0.003788083093240857, 0.012560485862195492, 0.0001993727928493172, 0.06958110630512238, 0.0007974911713972688, 0.000598118407651782, 0.0021931007504463196, 0.000598118407651782, 0.001196236815303564, 0.005383065436035395, 0.8381631970405579, 0.9961646795272827, 0.9896561503410339, 0.9918956756591797, 0.0007818029262125492, 0.0007818029262125492, 0.0007818029262125492, 0.0007818029262125492, 0.9960169196128845, 0.0007818029262125492, 0.0015818558167666197, 0.9965691566467285, 0.0002812995808199048, 0.0005625991616398096, 0.0002812995808199048, 0.0002812995808199048, 0.0002812995808199048, 0.9932687878608704, 0.0005625991616398096, 0.0002812995808199048, 0.004219493828713894, 0.0026326999068260193, 0.993844211101532, 0.9925633072853088, 0.11208871006965637, 0.040296655148267746, 0.058917492628097534, 0.053728874772787094, 0.03954503312706947, 0.0015759823145344853, 0.027882764115929604, 0.0014547528699040413, 0.12845468521118164, 0.009819582104682922, 0.018887542188167572, 0.028998075053095818, 0.06454253941774368, 0.012219924479722977, 0.40160879492759705, 0.033920228481292725, 0.04098694398999214, 0.001554677146486938, 0.00028266856679692864, 0.004664031323045492, 0.00014133428339846432, 0.065861776471138, 0.0008480057585984468, 0.00014133428339846432, 0.003109354292973876, 0.0019786800257861614, 0.0018373457714915276, 0.002968020038679242, 0.8415043354034424, 0.0016879052855074406, 0.9958640933036804, 0.0014539406402036548, 0.9959493279457092, 0.08982282876968384, 0.046016234904527664, 0.03340199217200279, 0.03453899174928665, 0.09138888120651245, 0.0498562827706337, 0.0354185551404953, 0.00326082413084805, 0.046102043241262436, 0.01218518428504467, 0.0057922531850636005, 0.05807270109653473, 0.05727894976735115, 0.00950358621776104, 0.4273395836353302, 0.0063772303983569145, 0.0066809081472456455, 0.002277582185342908, 0.0024294210597872734, 0.0037959704641252756, 0.0003036776324734092, 0.044944290071725845, 0.0003036776324734092, 0.0003036776324734092, 0.030671441927552223, 0.0007591940811835229, 0.002125743543729186, 0.007440102286636829, 0.0001518388162367046, 0.8912938833236694, 0.001762711675837636, 0.9959321022033691, 0.07118456065654755, 0.03579822927713394, 0.032502997666597366, 0.012282238341867924, 0.16663402318954468, 0.0019471840932965279, 0.054034359753131866, 0.0005616877460852265, 0.017711885273456573, 0.010971633717417717, 0.007301940582692623, 0.018385911360383034, 0.07167135179042816, 0.004830514546483755, 0.4941728413105011, 0.004377420991659164, 0.0008207664359360933, 0.0005471776239573956, 0.0019151216838508844, 0.0002735888119786978, 0.0870012417435646, 0.0002735888119786978, 0.0002735888119786978, 0.0013679440598934889, 0.0008207664359360933, 0.0013679440598934889, 0.0016415328718721867, 0.0005471776239573956, 0.8987392783164978, 0.9074817895889282, 0.07736313343048096, 0.0004093287279829383, 0.013507848605513573, 0.0004093287279829383, 0.0004093287279829383, 0.9943280220031738, 0.003440581262111664, 0.9976228475570679, 0.0023108862806111574, 0.9936810731887817, 0.9907813668251038, 0.003811858594417572, 0.0010302320588380098, 0.03956091031432152, 0.08252158761024475, 0.0014423249522224069, 0.0013393016997724771, 0.0020604641176760197, 0.0006181392236612737, 0.13671179115772247, 0.0004120928351767361, 0.10580483824014664, 0.0011332553112879395, 0.6148425340652466, 0.00020604641758836806, 0.00865394901484251, 0.994255781173706, 0.0487162284553051, 0.02072308212518692, 0.00033972266828641295, 0.001019167946651578, 0.00020383359515108168, 0.01576313190162182, 0.04912389814853668, 0.006047063507139683, 6.794452929170802e-05, 0.00013588905858341604, 0.0014947797171771526, 0.0012909461511299014, 0.8216532468795776, 0.00020383359515108168, 0.033292822539806366, 0.00913224183022976, 0.028090989217162132, 0.07197648286819458, 0.008117548190057278, 0.02539404109120369, 0.000493995554279536, 0.1408955454826355, 0.0009078836883418262, 0.3226591944694519, 0.2688003182411194, 0.007209664676338434, 0.018838586285710335, 0.051482345908880234, 0.04369857907295227, 0.0022964116651564837, 0.00020565555314533412, 0.00010282777657266706, 0.00010282777657266706, 0.00020565555314533412, 0.00020565555314533412, 0.0006169666303321719, 0.00041131110629066825, 0.0006169666303321719, 0.9883805513381958, 0.00020565555314533412, 0.008534705266356468, 0.00041131110629066825, 0.0001400550245307386, 0.0002801100490614772, 0.0001400550245307386, 0.0001400550245307386, 0.0008403301471844316, 0.0004201650735922158, 0.0001400550245307386, 0.0005602200981229544, 0.0011204401962459087, 0.9955111145973206, 0.0001400550245307386, 0.0002801100490614772, 0.0004201650735922158, 0.00017835332255344838, 0.00026752997655421495, 0.00017835332255344838, 0.001159296603873372, 8.917666127672419e-05, 0.00026752997655421495, 0.9733632802963257, 0.0026752997655421495, 8.917666127672419e-05, 0.014981679618358612, 0.0007134132902137935, 8.917666127672419e-05, 0.0005350599531084299, 8.917666127672419e-05, 0.005439776461571455, 0.00028322346042841673, 0.00018881564028561115, 9.440782014280558e-05, 0.00028322346042841673, 0.00018881564028561115, 9.440782014280558e-05, 0.004814798943698406, 0.9922261834144592, 0.000660854740999639, 0.00018881564028561115, 9.440782014280558e-05, 0.000660854740999639, 0.00018881564028561115, 0.00026316879666410387, 0.00026316879666410387, 0.010000414215028286, 0.00026316879666410387, 0.00026316879666410387, 0.9697770476341248, 0.00026316879666410387, 0.003947531804442406, 0.0005263375933282077, 0.014211115427315235, 0.00026316879666410387, 0.9932478070259094, 0.0005641582538373768, 0.0005641582538373768, 0.0011283165076747537, 0.0002820791269186884, 0.0002820791269186884, 0.007334057241678238, 0.0005641582538373768, 0.0002820791269186884, 0.0002820791269186884, 0.006205740850418806, 0.9821995496749878, 0.000743722717743367, 0.0003718613588716835, 0.9980758428573608, 0.9977676868438721, 0.0008653666009195149, 0.00043268330045975745, 0.00043268330045975745, 0.9923629760742188, 0.0009387832251377404, 0.0004693916125688702, 0.9965184330940247, 0.0004693916125688702, 0.0004693916125688702, 0.0004693916125688702, 0.0004693916125688702, 0.0004693916125688702, 0.00905620027333498, 0.10756547749042511, 0.00018482041195966303, 0.0022178450599312782, 0.0005544612649828196, 0.011920916847884655, 0.0004620510444510728, 0.27889400720596313, 0.00036964082391932607, 0.5711874961853027, 0.002402665326371789, 0.0015709735453128815, 0.0014785632956773043, 0.002402665326371789, 0.009795482270419598, 0.002183882985264063, 0.5670007467269897, 0.0002426536666462198, 0.00016176911594811827, 0.012698874808847904, 0.0002426536666462198, 0.32151609659194946, 0.00040442278259433806, 0.0007279610144905746, 8.088455797405913e-05, 8.088455797405913e-05, 0.01933140866458416, 0.07530351728200912, 0.9942566752433777, 0.00039117311825975776, 0.008605808950960636, 0.00039117311825975776, 0.9869297742843628, 0.00039117311825975776, 0.0019558656495064497, 0.0007823462365195155, 0.00039117311825975776, 0.0002094419178320095, 0.001675535342656076, 0.0002094419178320095, 0.003141628811135888, 0.0010472096037119627, 0.0002094419178320095, 0.0025133031886070967, 0.03099740482866764, 0.001675535342656076, 0.952960729598999, 0.0006283257971517742, 0.004817164037376642, 0.0002094419178320095, 0.0012986933579668403, 0.012178857810795307, 0.00020201897132210433, 0.0012409737100824714, 0.00011543941218405962, 0.9120290875434875, 0.00031745838350616395, 0.0597110353410244, 0.0005483372369781137, 5.771970609202981e-05, 0.0036652013659477234, 5.771970609202981e-05, 0.0008080758852884173, 0.006926364731043577, 0.0008657955913804471, 0.00010948287672363222, 0.0005255177966319025, 0.000153276021592319, 8.758630428928882e-05, 4.379315214464441e-05, 0.9961190223693848, 0.00010948287672363222, 8.758630428928882e-05, 0.0007663801079615951, 0.00039413836202584207, 0.0006568972603417933, 0.00017517260857857764, 0.000153276021592319, 0.000613104086369276, 0.0016486376989632845, 0.0002747729595284909, 6.869323988212273e-05, 6.869323988212273e-05, 0.04286457970738411, 6.869323988212273e-05, 0.0013051715213805437, 6.869323988212273e-05, 0.0005495459190569818, 0.00013738647976424545, 6.869323988212273e-05, 0.9528439044952393, 0.00020510569447651505, 0.0008204227779060602, 0.989840030670166, 0.0004102113889530301, 0.0016408455558121204, 0.005948064848780632, 0.00020510569447651505, 0.00020510569447651505, 0.00020510569447651505, 0.9955608248710632, 0.00043419035500846803, 0.006295760162174702, 0.008104885928332806, 0.04189936816692352, 0.07468073815107346, 0.000289460236672312, 0.0003618252812884748, 0.000289460236672312, 0.8395070433616638, 0.001157840946689248, 0.010854758322238922, 0.012229694984853268, 0.003545887768268585, 7.2365059168078e-05, 0.0003618252812884748, 0.0003525277425069362, 0.0003525277425069362, 0.0003525277425069362, 0.0031727496534585953, 0.9937756657600403, 0.0003525277425069362, 0.0007050554850138724, 0.0003525277425069362, 0.007479678373783827, 0.0015399338444694877, 0.027058837935328484, 0.010339555330574512, 0.0006599716143682599, 0.0002199905429733917, 0.01737925224006176, 0.0002199905429733917, 0.0030798676889389753, 0.01759924367070198, 0.004839791916310787, 0.629612922668457, 0.23714980483055115, 0.04135822132229805, 0.0017599243437871337, 0.00015530978271272033, 0.00015530978271272033, 0.0007765489281155169, 0.00015530978271272033, 0.00015530978271272033, 0.00031061956542544067, 0.00015530978271272033, 0.0007765489281155169, 0.00031061956542544067, 0.00015530978271272033, 0.993516743183136, 0.002950886031612754, 0.00015530978271272033, 0.00031061956542544067, 0.00020635718828998506, 0.00012381431588437408, 0.00024762863176874816, 0.0003301714896224439, 8.254287240561098e-05, 0.02472159080207348, 0.00016508574481122196, 0.0005365287070162594, 0.00012381431588437408, 0.00016508574481122196, 0.00020635718828998506, 0.00028890007524751127, 8.254287240561098e-05, 0.0005778001504950225, 0.9721487164497375, 0.00011582499428186566, 0.0002316499885637313, 0.0002316499885637313, 0.000579125015065074, 0.00011582499428186566, 0.00474882498383522, 0.00011582499428186566, 0.00011582499428186566, 0.0006949499947950244, 0.00011582499428186566, 0.11431927233934402, 0.00011582499428186566, 0.00011582499428186566, 0.8737837672233582, 0.00474882498383522, 0.010418335907161236, 0.04127771034836769, 0.3215177357196808, 0.014506543055176735, 0.009547943249344826, 0.010708466172218323, 0.2924255430698395, 0.016300078481435776, 0.02917134016752243, 0.011974492110311985, 0.10167767852544785, 0.013741652481257915, 0.01862112618982792, 0.019465142861008644, 0.08864816278219223, 0.9941797852516174, 0.0022766850888729095, 0.0005691712722182274, 0.0005691712722182274, 0.9949113726615906, 0.0005691712722182274, 0.001995024736970663, 0.000912011309992522, 0.0007980098598636687, 0.007296090479940176, 0.780083179473877, 0.0012540154857560992, 0.0013110162690281868, 0.0013395166024565697, 0.0005130063509568572, 0.00242252997122705, 0.0029070358723402023, 0.1899263560771942, 0.0029925371054559946, 0.0007980098598636687, 0.005443567410111427, 0.0010094002354890108, 0.9861840605735779, 0.005551701411604881, 0.0005047001177445054, 0.0005047001177445054, 0.0005047001177445054, 0.0005047001177445054, 0.005551701411604881, 0.0056650228798389435, 0.0008092889911495149, 0.0030752981547266245, 0.0006474311812780797, 0.9199997186660767, 0.0019422936020419002, 0.005826880689710379, 0.008254747837781906, 0.00016185779531951994, 0.0022660091053694487, 0.018451789394021034, 0.03204784542322159, 0.00016185779531951994, 0.0006474311812780797, 0.0002011329197557643, 0.0007039652555249631, 0.00010056645987788215, 0.00030169938690960407, 0.0004022658395115286, 0.0002011329197557643, 0.9594040513038635, 0.0017096298979595304, 0.008648715913295746, 0.0004022658395115286, 0.011364010162651539, 0.011866842396557331, 0.0015084969345480204, 0.002916427329182625, 0.00028370495419949293, 0.00028370495419949293, 0.00028370495419949293, 0.9955207109451294, 0.00028370495419949293, 0.00028370495419949293, 0.00028370495419949293, 0.0005674099083989859, 0.0019859347958117723, 0.00022023203200660646, 0.00995173491537571, 0.02129368484020233, 0.0042807599529623985, 0.0014590371865779161, 0.0010598666267469525, 0.8931235074996948, 0.007474124431610107, 0.001362685696221888, 0.0008946926100179553, 0.008836810477077961, 0.02668936923146248, 0.0206192247569561, 0.00024776102509349585, 0.0024913749657571316, 0.9940714240074158, 0.0023010913282632828, 0.0011087963357567787, 0.9945903420448303, 0.0011087963357567787, 0.0011087963357567787, 0.003930314444005489, 0.9855263829231262, 0.0004912893055006862, 0.0004912893055006862, 0.005895472131669521, 0.0019651572220027447, 0.0004912893055006862, 0.00028665189165621996, 0.00028665189165621996, 0.0005733037833124399, 0.00028665189165621996, 0.0008599557331763208, 0.0005733037833124399, 0.0005733037833124399, 0.9685967564582825, 0.005733037833124399, 0.021212240681052208, 0.0008599557331763208, 0.0005733037833124399, 0.001305131590925157, 0.9958154559135437, 0.001305131590925157, 0.9949876070022583, 0.00013756596308667213, 0.010455013252794743, 0.003164017340168357, 0.0004126979038119316, 0.3755550980567932, 0.00013756596308667213, 0.00013756596308667213, 0.0004126979038119316, 0.0023386215325444937, 0.5935971736907959, 0.0004126979038119316, 0.010867711156606674, 0.0024761874228715897, 0.00013756596308667213, 7.52157939132303e-05, 0.058141808956861496, 0.005415536928921938, 0.0006769421161152422, 0.6978521347045898, 0.0001504315878264606, 0.0001504315878264606, 0.0010530210565775633, 0.0014291000552475452, 0.23301853239536285, 0.0015795317012816668, 7.52157939132303e-05, 0.0003008631756529212, 0.0001504315878264606, 0.0003538442251738161, 0.0014153769006952643, 0.0003538442251738161, 0.0007076884503476322, 0.005661507602781057, 0.0007076884503476322, 0.0010615326464176178, 0.9886407852172852, 0.0003538442251738161, 0.0003538442251738161, 0.0006216023466549814, 0.0006216023466549814, 0.030458515509963036, 0.0006216023466549814, 0.0006216023466549814, 0.9634836912155151, 0.0006216023466549814, 0.0006216023466549814, 0.0012432046933099627, 0.0003573466092348099, 0.0010720398277044296, 0.0003573466092348099, 0.0003573466092348099, 0.021798143163323402, 0.0007146932184696198, 0.0003573466092348099, 0.007861625403165817, 0.7761568427085876, 0.0014293864369392395, 0.03716404736042023, 0.0007146932184696198, 0.15008556842803955, 0.0010720398277044296, 0.00036097230622544885, 0.0013536461628973484, 0.0020755906589329243, 0.1606326699256897, 0.0015341322869062424, 0.0002707292151171714, 0.10134297609329224, 0.0002707292151171714, 0.593528687953949, 0.008753578178584576, 0.0002707292151171714, 0.07345786690711975, 0.007851148024201393, 0.04809955880045891, 0.00018048615311272442, 0.9943441152572632, 0.9906225204467773, 0.006217296235263348, 0.0005841799429617822, 0.0005841799429617822, 0.0002920899714808911, 0.0005841799429617822, 0.016064949333667755, 0.0005841799429617822, 0.0002920899714808911, 0.0008762699435465038, 0.0002920899714808911, 0.0037971697747707367, 0.9738280177116394, 0.0008762699435465038, 0.0002920899714808911, 0.0008762699435465038, 0.9862533807754517, 0.0014681094326078892, 0.0014681094326078892, 0.03743679076433182, 0.0007340547163039446, 0.008808656595647335, 0.0007340547163039446, 0.016883257776498795, 0.9285792112350464, 0.0014681094326078892, 0.0022021641489118338, 0.0007340547163039446, 0.9953945875167847, 0.011129481717944145, 0.9868140816688538, 0.9954630732536316, 0.2919960618019104, 0.030507991090416908, 0.1005505621433258, 0.021166883409023285, 0.008932236582040787, 0.11347714066505432, 0.151218980550766, 0.12467388808727264, 0.021449947729706764, 0.018870921805500984, 0.0262305811047554, 0.01827334240078926, 0.039157163351774216, 0.0004088699643034488, 0.03308701515197754, 0.001282988814637065, 0.002199409296736121, 0.4265387952327728, 0.0006964796339161694, 0.11187662184238434, 0.00029325459036044776, 0.0024193504359573126, 0.000769793288782239, 0.0019794683903455734, 0.4437674880027771, 0.0027859185356646776, 0.0042155347764492035, 0.0008431069436483085, 7.331364759011194e-05, 0.00029325459036044776, 5.122553193359636e-05, 0.09650889784097672, 0.000461029791040346, 0.00015367659216281027, 0.006300740409642458, 0.00010245106386719272, 0.0003585787198971957, 0.00030735318432562053, 0.0003585787198971957, 0.00025612764875404537, 0.89444899559021, 0.00015367659216281027, 0.00010245106386719272, 5.122553193359636e-05, 0.0003585787198971957, 0.00011361488577676937, 0.00020657252753153443, 0.9570505023002625, 0.0002892015327233821, 0.008686374872922897, 8.26290124678053e-05, 0.0004544595431070775, 0.00010328626376576722, 0.0001342721370747313, 5.164313188288361e-05, 0.006300461944192648, 0.02413799986243248, 0.0020657251589000225, 0.0002169011568184942, 9.295763447880745e-05, 4.234383231960237e-05, 0.0002964068262372166, 0.3980320394039154, 0.00038109449087642133, 0.5997157096862793, 4.234383231960237e-05, 4.234383231960237e-05, 0.0001270314969588071, 0.00021171916159801185, 0.00021171916159801185, 0.00021171916159801185, 0.00033875065855681896, 0.00021171916159801185, 0.0001270314969588071, 4.234383231960237e-05, 0.9133434891700745, 0.017160942777991295, 0.000953385722823441, 0.000953385722823441, 0.013347399421036243, 0.014300785958766937, 0.018114328384399414, 0.016207557171583176, 0.0028601570520550013, 0.000953385722823441, 0.000953385722823441, 0.9964307546615601, 0.9932801723480225, 0.005042031407356262, 0.14830677211284637, 0.03668743744492531, 0.00019462831551209092, 0.0011677698930725455, 0.0002919424732681364, 0.026177508756518364, 0.033281441777944565, 0.0034059954341500998, 0.061502546072006226, 0.6702026128768921, 0.0011677698930725455, 0.0007785132620483637, 0.0012650840217247605, 0.0014597123954445124, 0.014110553078353405, 0.004077225457876921, 0.004077225457876921, 0.000582460779696703, 0.001164921559393406, 0.001164921559393406, 0.970379650592804, 0.01223167683929205, 0.000582460779696703, 0.002912303898483515, 0.000582460779696703, 0.002329843118786812, 9.691625746199861e-05, 0.0008722462807781994, 0.0007753300596959889, 9.691625746199861e-05, 0.00019383251492399722, 0.00019383251492399722, 0.00029074877966195345, 0.9948453903198242, 9.691625746199861e-05, 0.0011629951186478138, 0.0005814975593239069, 0.00019383251492399722, 9.691625746199861e-05, 9.691625746199861e-05, 0.00038766502984799445, 0.9966009259223938, 0.0010056517785415053, 0.9953495264053345, 0.0007759156287647784, 0.0007759156287647784, 0.0007759156287647784, 0.0007759156287647784, 0.0007759156287647784, 0.9954997301101685, 0.001963190268725157, 0.9933742880821228, 0.001963190268725157, 0.001963190268725157, 0.10532762110233307, 0.02262427844107151, 0.06520458310842514, 0.005212395451962948, 0.00434366287663579, 0.1921636462211609, 0.08665607124567032, 0.19504287838935852, 0.05672823637723923, 0.010778489522635937, 0.03807530924677849, 0.009022408165037632, 0.01705818437039852, 0.027489179745316505, 0.1642773300409317, 0.9971908330917358, 0.0011267693480476737, 0.9964732527732849, 0.0001788593945093453, 0.6932590007781982, 0.0003577187890186906, 0.0001788593945093453, 0.048113178461790085, 0.0016097346087917686, 0.0003577187890186906, 0.0007154375780373812, 0.0001788593945093453, 0.005902360193431377, 0.0001788593945093453, 0.24664710462093353, 0.001788593945093453, 0.0003577187890186906, 0.0001788593945093453, 0.002913753967732191, 0.00032375045702792704, 0.00032375045702792704, 0.9877626299858093, 0.00032375045702792704, 0.005503757391124964, 0.00032375045702792704, 0.00032375045702792704, 0.0016187522560358047, 0.00513755576685071, 4.045319656142965e-05, 4.045319656142965e-05, 0.00014158619160298258, 0.00010113298776559532, 0.00014158619160298258, 0.0001618127862457186, 0.9832351803779602, 2.0226598280714825e-05, 0.00010113298776559532, 0.0006067979265935719, 8.09063931228593e-05, 4.045319656142965e-05, 0.009991939179599285, 0.0001618127862457186, 0.001066247234120965, 5.467934533953667e-05, 8.2019018009305e-05, 0.00013669836334884167, 0.000246057054027915, 0.00043743476271629333, 0.000246057054027915, 0.9968044757843018, 8.2019018009305e-05, 8.2019018009305e-05, 8.2019018009305e-05, 0.00013669836334884167, 8.2019018009305e-05, 0.00035541574470698833, 0.00010935869067907333, 0.0027333139441907406, 0.00018636231834534556, 0.0001242415455635637, 0.0002484830911271274, 0.0003106038784608245, 0.00018636231834534556, 0.9954853653907776, 6.212077278178185e-05, 0.00018636231834534556, 0.0001242415455635637, 6.212077278178185e-05, 6.212077278178185e-05, 0.0002484830911271274, 0.14659307897090912, 0.00016471132403239608, 0.00032942264806479216, 0.0009882679441943765, 0.00032942264806479216, 0.8495810031890869, 0.00016471132403239608, 0.00032942264806479216, 0.00016471132403239608, 0.0008235566201619804, 0.00032942264806479216, 0.00023102419800125062, 0.00023102419800125062, 0.00023102419800125062, 0.9961763620376587, 0.0009240967920050025, 0.0013861452462151647, 0.00023102419800125062, 0.1789775788784027, 0.05288044735789299, 0.017953814938664436, 0.014232251793146133, 0.027140935882925987, 0.1632349044084549, 0.034957773983478546, 0.15607206523418427, 0.06371813267469406, 0.010495117865502834, 0.027919504791498184, 0.04551517590880394, 0.07874453067779541, 0.04834917187690735, 0.07985009253025055, 0.0002110092027578503, 0.0004220184055157006, 0.02088990993797779, 0.0002110092027578503, 0.9341377019882202, 0.0004220184055157006, 0.0415688119828701, 0.0016880736220628023, 0.0002110092027578503, 0.0002110092027578503, 0.04170094430446625, 0.0004964398103766143, 0.8319090008735657, 0.000744659686461091, 0.023332670331001282, 0.0042197383008897305, 0.016134293749928474, 0.0662747174501419, 0.0003723298432305455, 0.00012410995259415358, 0.0060813878662884235, 0.0023580892011523247, 0.0024821991100907326, 0.00024821990518830717, 0.0033509687054902315, 0.006394146475940943, 0.003926230128854513, 0.00011217800783924758, 0.00011217800783924758, 0.00011217800783924758, 0.8206943273544312, 0.012115225195884705, 0.14695319533348083, 0.0005608900100924075, 0.00011217800783924758, 0.00044871203135699034, 0.0037018742877990007, 0.0021313822362571955, 0.0006730680470354855, 0.0019070261623710394, 0.099696584045887, 0.00797885563224554, 0.062305476516485214, 0.07032344490289688, 0.372385710477829, 0.023584559559822083, 0.01619238406419754, 0.10916169732809067, 0.044900618493556976, 0.04427482560276985, 0.13841749727725983, 0.0018382658017799258, 0.007392175029963255, 0.00086046481737867, 0.000743128708563745, 0.9897699952125549, 0.001158303115516901, 0.0034749091137200594, 0.0005791515577584505, 0.001158303115516901, 0.0028957577887922525, 0.0005961379501968622, 0.0002980689750984311, 0.0002980689750984311, 0.08524772524833679, 0.0014903448754921556, 0.0002980689750984311, 0.9091103672981262, 0.0011922759003937244, 0.0005961379501968622, 0.0008942069252952933, 0.9914070963859558, 0.002071906113997102, 0.0031078592874109745, 0.001035953056998551, 0.00015818352403584868, 0.00021091137023176998, 0.00021091137023176998, 0.004745505750179291, 0.0005272784037515521, 0.00010545568511588499, 0.0012127403169870377, 5.2727842557942495e-05, 0.008120087906718254, 0.002794575644657016, 0.00010545568511588499, 0.9810542464256287, 0.00021091137023176998, 0.00010545568511588499, 0.00031636704807169735, 0.006252902094274759, 0.001042150310240686, 0.001042150310240686, 0.9900428056716919, 0.00040519709000363946, 0.00040519709000363946, 0.0072935475036501884, 0.00040519709000363946, 0.9907068610191345, 0.0007497258484363556, 0.07249849289655685, 0.0009371573105454445, 0.0002624040353111923, 0.0009746436262503266, 0.0011245887726545334, 0.0011245887726545334, 0.0008621847373433411, 3.74862938770093e-05, 0.0002624040353111923, 0.00022491774871014059, 0.00022491774871014059, 0.0002624040353111923, 0.0005622943863272667, 0.9199135899543762, 0.000289577292278409, 0.9967250823974609, 0.0001447886461392045, 0.0001447886461392045, 0.000579154584556818, 0.0001447886461392045, 0.0001447886461392045, 0.0001447886461392045, 0.0001447886461392045, 0.000289577292278409, 0.001158309169113636, 0.0007708149496465921, 0.9242070913314819, 0.071685791015625, 0.0015416298992931843, 0.0034787622280418873, 0.00021742263925261796, 0.00021742263925261796, 0.016741544008255005, 0.0006522679468616843, 0.0004348452785052359, 0.0010871132835745811, 0.00021742263925261796, 0.0015219585038721561, 0.00021742263925261796, 0.00021742263925261796, 0.9751405715942383, 0.0011526261223480105, 0.9970216155052185, 0.9948078989982605, 0.00037596767651848495, 0.00018798383825924248, 0.07018063217401505, 0.016354594379663467, 0.536129891872406, 0.0001253225636901334, 0.0028824189212173223, 0.00037596767651848495, 0.002067822264507413, 0.03703281655907631, 0.272451251745224, 0.056144509464502335, 0.005388870369642973, 6.26612818450667e-05, 0.0001253225636901334, 0.0006151914712972939, 0.0006151914712972939, 0.9793847799301147, 0.0012303829425945878, 0.015994977205991745, 0.0006151914712972939, 0.0004505398392211646, 0.0002252699196105823, 0.0006758097442798316, 0.9639299511909485, 0.0013516194885596633, 0.0002252699196105823, 0.022076452150940895, 0.0002252699196105823, 0.0004505398392211646, 0.0004505398392211646, 0.0018021593568846583, 0.008334986865520477, 0.020811451599001884, 0.02078685164451599, 0.2030838578939438, 0.06593982875347137, 0.03293916583061218, 0.0002951979113277048, 0.043467890471220016, 0.00030749780125916004, 0.09997368603944778, 0.13210105895996094, 0.059383977204561234, 0.06565693020820618, 0.052311528474092484, 0.02300083637237549, 0.17992311716079712, 0.995625913143158, 0.0006273046019487083, 0.001176196034066379, 0.000352858827682212, 0.00019603267719503492, 0.4908658266067505, 0.027013303712010384, 7.841307524358854e-05, 0.016662778332829475, 0.0006273046019487083, 0.004116686526685953, 0.0014506418956443667, 0.001411435310728848, 0.0013330222573131323, 0.18654470145702362, 0.2675061821937561, 0.026808466762304306, 0.00016651221085339785, 0.0003122104099020362, 0.00024976831628009677, 0.00016651221085339785, 0.003330244217067957, 6.244207907002419e-05, 0.5196846127510071, 0.00029139636899344623, 0.00010407013178337365, 6.244207907002419e-05, 4.162805271334946e-05, 0.4485630989074707, 4.162805271334946e-05, 8.325610542669892e-05, 0.0019689155742526054, 0.9913829565048218, 0.00010184045822825283, 0.0005431491299532354, 6.789364124415442e-05, 3.394682062207721e-05, 0.002478117821738124, 0.00023762774071656168, 0.0023762774653732777, 0.00010184045822825283, 3.394682062207721e-05, 6.789364124415442e-05, 0.00033946821349672973, 3.394682062207721e-05, 0.00023762774071656168, 0.00045683368807658553, 0.0003045557823497802, 0.0001522778911748901, 0.0007613894413225353, 0.0003045557823497802, 0.0016750568756833673, 0.9948315024375916, 0.0001522778911748901, 0.0001522778911748901, 0.0001522778911748901, 0.0009136673761531711, 0.0001522778911748901, 0.0001522778911748901, 0.038826413452625275, 0.023392796516418457, 0.17964187264442444, 0.06666529923677444, 0.05830477178096771, 0.10172843188047409, 0.05915437638759613, 0.08800966292619705, 0.10015952587127686, 0.0383833646774292, 0.06194816902279854, 0.043736398220062256, 0.03507876768708229, 0.03278535604476929, 0.07219033688306808, 0.03680538386106491, 0.013337142765522003, 0.00038472528103739023, 0.0006412087823264301, 0.0002564835303928703, 0.002180109964683652, 0.0008976923418231308, 0.8535771369934082, 0.0002564835303928703, 0.00038472528103739023, 0.0038472528103739023, 0.0002564835303928703, 0.0848960429430008, 0.0002564835303928703, 0.0019236264051869512, 0.0002622274332679808, 0.001311137224547565, 0.9901708364486694, 0.0002622274332679808, 0.0005244548665359616, 0.0002622274332679808, 0.0007866822998039424, 0.0002622274332679808, 0.0002622274332679808, 0.00550677627325058, 0.0002622274332679808, 0.0002622274332679808, 0.00016226230945903808, 0.0002028278831858188, 0.988096296787262, 8.113115472951904e-05, 0.003529205219820142, 0.0025961969513446093, 0.000486786913825199, 0.00012169672845629975, 0.0011764017399400473, 0.0004056557663716376, 0.0004056557663716376, 0.0017443198012188077, 0.00032452461891807616, 0.00036509017809294164, 0.0002839590306393802, 0.029631080105900764, 0.0006890948861837387, 0.002067284658551216, 0.0006890948861837387, 0.964732825756073, 0.0013781897723674774, 0.9943467378616333, 0.989497184753418, 0.0005687578814104199, 0.0011375157628208399, 0.0011375157628208399, 0.0011375157628208399, 0.9947575330734253, 0.9779477715492249, 0.002325678477063775, 0.0011628392385318875, 0.011628392152488232, 0.0034885175991803408, 0.0011628392385318875, 0.051786307245492935, 0.037438295781612396, 0.13887539505958557, 0.10487870872020721, 0.13805550336837769, 0.055561598390340805, 0.012889374978840351, 0.14199286699295044, 0.030917340889573097, 0.06847003847360611, 0.03544578328728676, 0.031956497579813004, 0.09004449099302292, 0.04239574819803238, 0.01929592899978161, 0.06792747229337692, 0.0013450984843075275, 0.16107554733753204, 0.4224449694156647, 0.003026471473276615, 0.003362746210768819, 0.0054644625633955, 0.0016813731053844094, 0.1345098465681076, 0.0005044119316153228, 0.1051698848605156, 0.002774265594780445, 0.07952894270420074, 0.008995345793664455, 0.0024379908572882414, 0.7348235249519348, 0.0012561086332425475, 0.0009420814458280802, 0.0012561086332425475, 0.030146606266498566, 0.0009420814458280802, 0.2251574695110321, 0.0003140271583106369, 0.0003140271583106369, 0.0003140271583106369, 0.0012561086332425475, 0.002512217266485095, 0.07475145161151886, 0.05677785724401474, 0.0006753084599040449, 0.036258868873119354, 0.008830956183373928, 0.06888145953416824, 0.01994757167994976, 0.25547438859939575, 0.17802169919013977, 0.0013506169198080897, 0.0013506169198080897, 0.020207306370139122, 0.0011428296566009521, 0.24482528865337372, 0.03153171017765999, 0.012794464826583862, 0.004797924309968948, 0.05637561157345772, 0.6801057457923889, 0.006530507933348417, 0.013860669918358326, 0.015993081033229828, 0.005730853881686926, 0.011728259734809399, 0.018525319173932076, 0.02279014140367508, 0.0034651674795895815, 0.12021466344594955, 0.007063610944896936, 0.019991351291537285, 0.0007441117195412517, 0.0004252067010384053, 0.00021260335051920265, 0.006697005592286587, 0.00021260335051920265, 0.00031890501850284636, 0.0014882234390825033, 0.006378100719302893, 0.9427895545959473, 0.00021260335051920265, 0.006059195380657911, 0.003826860338449478, 0.0004252067010384053, 0.02976446971297264, 0.0005315083544701338, 0.0017329389229416847, 0.00034658776712603867, 0.9964398741722107, 0.00034658776712603867, 0.00034658776712603867, 0.00034658776712603867, 0.0003643578675109893, 0.00048581050941720605, 0.048216693103313446, 0.047730881720781326, 0.00012145262735430151, 0.0019432420376688242, 0.0006072631222195923, 0.0017003368120640516, 0.894862949848175, 0.0007287157350219786, 0.00048581050941720605, 0.00048581050941720605, 0.00048581050941720605, 0.0012145262444391847, 0.0006072631222195923, 0.0007528858259320259, 0.0007528858259320259, 0.9938092827796936, 0.0030115433037281036, 0.00012410670751705766, 0.9646814465522766, 0.0018616006709635258, 0.0012410670751705766, 0.014272271655499935, 0.0004964268300682306, 0.00012410670751705766, 0.0007446402451023459, 0.00012410670751705766, 0.00012410670751705766, 0.0008687469526194036, 0.014272271655499935, 0.0011169604258611798, 0.9220312833786011, 0.0004310571530368179, 0.0004310571530368179, 0.001293171546421945, 0.0004310571530368179, 0.0004310571530368179, 0.0732797160744667, 0.001293171546421945, 0.9945986866950989, 0.9802281856536865, 0.00044902804074808955, 0.00044902804074808955, 0.00044902804074808955, 0.00044902804074808955, 0.017063064500689507, 0.026988336816430092, 0.0009922182653099298, 0.051000017672777176, 0.0027782111428678036, 0.07163815945386887, 0.00019844365306198597, 0.0005953309591859579, 0.0023813238367438316, 0.014883274212479591, 0.1083502322435379, 0.6860197186470032, 0.02936965972185135, 0.0011906619183719158, 0.0005953309591859579, 0.0029766547959297895, 0.9935979247093201, 0.004338855389505625, 0.9852461814880371, 0.00020728071103803813, 0.0018137062434107065, 0.0019173466134816408, 0.016530636698007584, 0.00020728071103803813, 0.00041456142207607627, 0.12421296536922455, 0.00015546054055448622, 0.06778079271316528, 0.02684285305440426, 0.0005182017921470106, 0.0031092106364667416, 0.7457960247993469, 0.010001294314861298, 0.0004663816071115434, 0.0037961467169225216, 0.0004067300178576261, 0.0001355766726192087, 0.0012201899662613869, 0.004474029876291752, 0.010032673366367817, 0.0002711533452384174, 0.9771010279655457, 0.0001355766726192087, 0.0014913433697074652, 0.0002711533452384174, 0.0001355766726192087, 0.0005423066904768348, 0.006202204152941704, 0.007108679972589016, 0.08773732930421829, 0.1953217089176178, 0.5186951160430908, 0.000906475936062634, 0.03296709805727005, 0.001288150087930262, 0.005295727867633104, 0.06307164579629898, 0.04489441588521004, 0.013835685327649117, 0.015171544626355171, 0.003387357573956251, 0.004102996550500393, 0.0002440500247757882, 0.010006051510572433, 0.0003660750517155975, 0.987914502620697, 0.0001220250123878941, 0.0003660750517155975, 0.0003660750517155975, 0.0001220250123878941, 0.0001220250123878941, 0.0002440500247757882, 0.0002432245819363743, 0.7167828679084778, 0.0004053743032272905, 0.28132978081703186, 8.10748606454581e-05, 8.10748606454581e-05, 8.10748606454581e-05, 0.0001621497212909162, 8.10748606454581e-05, 0.0001621497212909162, 0.0002432245819363743, 0.0001621497212909162, 0.0001621497212909162, 8.10748606454581e-05, 0.000469805410830304, 0.000939610821660608, 0.0399334616959095, 0.005167859606444836, 0.006577276159077883, 0.000469805410830304, 0.000939610821660608, 0.000469805410830304, 0.8179312348365784, 0.12590785324573517, 0.0006333678029477596, 0.9905872344970703, 0.0006333678029477596, 0.0006333678029477596, 0.006333678029477596, 0.00021364542772062123, 0.00042729085544124246, 0.019014444202184677, 0.01944173499941826, 0.006195717491209507, 0.2055269032716751, 0.00021364542772062123, 0.00021364542772062123, 0.5984208583831787, 0.1480562835931778, 0.00021364542772062123, 0.0017091634217649698, 0.00015375224757008255, 0.00015375224757008255, 0.0003075044951401651, 0.00015375224757008255, 0.9921632409095764, 0.0021525314077734947, 0.00015375224757008255, 0.0015375224174931645, 0.0007687612087465823, 0.0004612567136064172, 0.0003075044951401651, 0.0009225134272128344, 0.0009225134272128344, 0.0003166206006426364, 0.00019272557983640581, 0.006759161129593849, 0.004997098818421364, 8.259667811216787e-05, 0.00045428171870298684, 0.00695188669487834, 0.0005093461950309575, 0.9165064692497253, 0.00028908837703056633, 0.02498549409210682, 0.0019685542210936546, 0.0013077807379886508, 0.03391970321536064, 0.0007433700957335532, 5.10138415847905e-05, 5.10138415847905e-05, 5.10138415847905e-05, 0.000816221465356648, 5.10138415847905e-05, 0.00030608303495682776, 0.0002550692006479949, 0.0002550692006479949, 0.9973205924034119, 5.10138415847905e-05, 5.10138415847905e-05, 0.000102027683169581, 0.000204055366339162, 0.000102027683169581, 0.00035709686926566064, 6.0438444052124396e-05, 6.0438444052124396e-05, 0.000181315335794352, 0.0011483305133879185, 0.00012087688810424879, 0.00012087688810424879, 0.026653354987502098, 0.00012087688810424879, 0.9661085605621338, 0.00048350755241699517, 6.0438444052124396e-05, 0.0009065766935236752, 0.003626306774094701, 6.0438444052124396e-05, 0.0003021922311745584, 0.001689417753368616, 0.9950670599937439, 0.14918583631515503, 0.0023608910851180553, 0.00022484677901957184, 0.0025857379660010338, 0.00011242338950978592, 0.8435126543045044, 0.00022484677901957184, 0.0005621169693768024, 0.0004496935580391437, 0.00011242338950978592, 0.00022484677901957184, 0.00011242338950978592, 0.00022484677901957184, 0.00011242338950978592, 5.394950494519435e-05, 5.394950494519435e-05, 0.00016184851119760424, 0.020123165100812912, 0.0001078990098903887, 0.00016184851119760424, 0.0001078990098903887, 0.0002157980197807774, 0.919677197933197, 0.0002157980197807774, 0.00016184851119760424, 0.0010789900552481413, 0.0002157980197807774, 0.057510171085596085, 0.0002157980197807774, 0.0023860232904553413, 0.9949716925621033, 0.0005965058226138353, 0.0817035511136055, 0.7053498029708862, 0.000815223902463913, 0.0010869652032852173, 9.058043360710144e-05, 9.058043360710144e-05, 0.19357037544250488, 0.0002717413008213043, 0.0009963847696781158, 0.0010869652032852173, 9.058043360710144e-05, 0.005615986883640289, 0.008514560759067535, 9.058043360710144e-05, 0.0005434826016426086, 0.00616195984184742, 0.9859135150909424, 0.0012323919218033552, 0.0036971757654100657, 0.0012323919218033552, 0.9915836453437805, 0.01008516550064087, 0.0011069084284827113, 0.003935674205422401, 0.007379389367997646, 0.11831621080636978, 0.003074745647609234, 0.0011069084284827113, 0.0002459796378389001, 0.0012298983056098223, 0.0059035117737948895, 0.007379389367997646, 0.7984499335289001, 0.0024597966112196445, 0.0006149491528049111, 0.03861880674958229, 0.0010977446800097823, 0.0010977446800097823, 0.0005488723400048912, 0.8776468634605408, 0.015917297452688217, 0.0005488723400048912, 0.0016466170782223344, 0.0005488723400048912, 0.0021954893600195646, 0.09824815392494202, 0.001285543548874557, 0.001285543548874557, 0.9641576409339905, 0.001285543548874557, 0.029567500576376915, 0.15919943153858185, 0.001490377588197589, 0.000948422122746706, 0.013413398526608944, 0.00013548888091463596, 0.00013548888091463596, 0.8193012475967407, 0.0035227108746767044, 0.0002709777618292719, 0.00040646662819199264, 0.0002709777618292719, 0.0002709777618292719, 0.0002709777618292719, 0.00013548888091463596, 0.995352566242218, 0.9953525066375732, 0.06480184942483902, 0.006252809893339872, 0.037029627710580826, 0.05189020186662674, 0.07154189050197601, 0.07560215145349503, 0.017053117975592613, 0.027853425592184067, 0.014860574156045914, 0.03573034331202507, 0.03467467054724693, 0.4958396852016449, 0.05530082434415817, 0.0069024525582790375, 0.004466292914003134, 0.9964050650596619, 0.9948932528495789, 0.9922168850898743, 0.9904128313064575, 0.02992221899330616, 0.004987036809325218, 0.0006233796011656523, 0.9581344127655029, 0.0006233796011656523, 0.0006233796011656523, 0.0031168980058282614, 0.0006233796011656523, 0.0006233796011656523, 0.998052716255188, 0.9942986965179443, 0.9942388534545898, 0.001353362575173378, 0.0009022416779771447, 0.0009022416779771447, 0.00045112083898857236, 0.9893079996109009, 0.0009022416779771447, 0.001353362575173378, 0.001353362575173378, 0.0009022416779771447, 0.00045112083898857236, 0.0018044833559542894, 0.00020578625844791532, 0.0002743816585280001, 0.0008917404338717461, 0.00020578625844791532, 6.859541463200003e-05, 0.0007545495755039155, 0.929262101650238, 0.009534763172268867, 0.00013719082926400006, 0.0017834808677434921, 0.0016462900675833225, 0.00020578625844791532, 0.0002743816585280001, 0.05473914369940758, 0.000600587110966444, 0.001051027444191277, 0.0457947701215744, 0.000300293555483222, 0.017567172646522522, 0.8031351566314697, 0.000150146777741611, 0.002252201782539487, 0.000600587110966444, 0.005855724681168795, 0.000300293555483222, 0.11501243710517883, 0.004654550459235907, 0.000150146777741611, 0.002552495338022709, 0.0004175094363745302, 0.000238576831179671, 0.0001192884155898355, 0.0004175094363745302, 0.013539235107600689, 0.9578263163566589, 0.0001192884155898355, 0.004711892455816269, 5.964420779491775e-05, 0.02099476009607315, 0.000238576831179671, 0.00017893261974677444, 5.964420779491775e-05, 0.0005367978592403233, 0.000656086253002286, 0.007744073402136564, 0.0026806406676769257, 0.07565364241600037, 0.7922782897949219, 0.010424713604152203, 0.0008935469086281955, 0.0008935469086281955, 0.005659130401909351, 0.0005956979584880173, 0.007744073402136564, 0.0029784897342324257, 0.001787093817256391, 0.08995039016008377, 0.00029784897924400866, 0.0008935469086281955, 0.0020534610375761986, 0.9877147674560547, 0.0020534610375761986, 0.0020534610375761986, 0.0020534610375761986, 0.002867856528609991, 0.9913224577903748, 0.0019119044300168753, 0.0009559522150084376, 0.0009559522150084376, 0.0009210132993757725, 0.0009210132993757725, 0.04512965306639671, 0.9458807110786438, 0.004605066496878862, 0.001842026598751545, 0.0006445024046115577, 0.0006445024046115577, 0.0038670143112540245, 0.0006445024046115577, 0.9931781888008118, 0.0977504625916481, 0.018050994724035263, 0.3039691746234894, 0.013167335651814938, 0.0038404727820307016, 0.10108866542577744, 0.06215847656130791, 0.12148875743150711, 0.07047306001186371, 0.00849231332540512, 0.02097191847860813, 0.03111787512898445, 0.06769895553588867, 0.013870520517230034, 0.06585986167192459, 0.006293639075011015, 0.0008990912465378642, 0.0008990912465378642, 0.0026972738560289145, 0.9863031506538391, 0.0017981824930757284, 0.009785608388483524, 0.9863893389701843, 0.0019571217708289623, 0.0007025693194009364, 0.0007025693194009364, 0.03653360530734062, 0.0007025693194009364, 0.00843083206564188, 0.9126375913619995, 0.0007025693194009364, 0.025292497128248215, 0.012646248564124107, 0.0007025693194009364, 0.0007025693194009364, 0.9919558167457581, 0.9954494833946228, 0.0003100060857832432, 0.0003100060857832432, 0.0003100060857832432, 0.004650091286748648, 0.0003100060857832432, 0.0003100060857832432, 0.9737290740013123, 0.0003100060857832432, 0.0006200121715664864, 0.0003100060857832432, 0.001550030428916216, 0.01581031084060669, 0.0006200121715664864, 0.0012400243431329727, 0.0003097689477726817, 0.026485245674848557, 0.001084191375412047, 0.028808513656258583, 0.0003097689477726817, 0.00015488447388634086, 0.0003097689477726817, 0.00015488447388634086, 0.9245054721832275, 0.0006195378955453634, 0.00015488447388634086, 0.01424937229603529, 0.000929306901525706, 0.0015488448552787304, 0.000464653450762853, 0.9956358671188354, 0.0017858939245343208, 0.00015704739780630916, 0.9911261200904846, 0.0004711422079708427, 0.004554374609142542, 0.00015704739780630916, 0.00015704739780630916, 0.00015704739780630916, 0.00015704739780630916, 0.00015704739780630916, 0.00015704739780630916, 0.002355711068958044, 0.00015704739780630916, 0.00031409479561261833, 0.04144914448261261, 0.04406100884079933, 0.018283046782016754, 0.034749146550893784, 0.013692006468772888, 0.12347140163183212, 0.09587649255990982, 0.13044719398021698, 0.06075422465801239, 0.09882903099060059, 0.010285227559506893, 0.0397619754076004, 0.025842851027846336, 0.09431910514831543, 0.16816510260105133, 0.0010940025094896555, 0.006928682327270508, 0.001458670012652874, 0.012763362377882004, 0.0003646675031632185, 0.0003646675031632185, 0.9758502244949341, 0.0003646675031632185, 0.0003646675031632185, 0.0003646675031632185, 0.0001897717302199453, 0.0003795434604398906, 0.0001897717302199453, 0.0003795434604398906, 0.0017079456010833383, 0.0003795434604398906, 0.0001897717302199453, 0.0001897717302199453, 0.9963015913963318, 0.0001897717302199453, 0.002466875361278653, 0.002466875361278653, 0.9941508173942566, 0.9963786005973816, 0.0007232602220028639, 0.0007232602220028639, 0.9826695322990417, 0.00024108673096634448, 0.0038573876954615116, 0.00048217346193268895, 0.00048217346193268895, 0.00024108673096634448, 0.00024108673096634448, 0.0009643469238653779, 0.007955862209200859, 0.00048217346193268895, 0.0007232602220028639, 0.00020723114721477032, 0.00041446229442954063, 0.0010361557360738516, 0.00020723114721477032, 0.02673281915485859, 0.000621693441644311, 0.00020723114721477032, 0.00020723114721477032, 0.9702562689781189, 0.00020723114721477032, 0.002798136556521058, 0.0001472703443141654, 0.0001472703443141654, 0.0001472703443141654, 0.0002945406886283308, 0.0001472703443141654, 0.0005890813772566617, 0.000883622036781162, 0.0001472703443141654, 0.9946638941764832, 0.00039173068944364786, 0.00013057689648121595, 0.8499249815940857, 0.009009805507957935, 0.0018280765507370234, 0.00039173068944364786, 0.06280748546123505, 0.00039173068944364786, 0.03068557009100914, 0.015277496539056301, 0.002611537929624319, 0.00678999861702323, 0.013188266195356846, 0.005745383445173502, 0.0010446151718497276, 0.000248948868829757, 0.000497897737659514, 0.04481079429388046, 0.000248948868829757, 0.000248948868829757, 0.9472504258155823, 0.0037342330906540155, 0.000248948868829757, 0.001244744285941124, 0.000248948868829757, 0.000497897737659514, 0.000497897737659514, 0.006716717034578323, 0.000571635493542999, 0.00014290887338574976, 0.002572359750047326, 0.001143270987085998, 0.0017149064224213362, 0.002429450862109661, 0.00014290887338574976, 0.002286541974171996, 0.9764962792396545, 0.0038585395086556673, 0.00042872660560533404, 0.0002858177467714995, 0.001000362099148333, 0.003594049485400319, 0.9919576048851013, 0.0011980164563283324, 0.0011980164563283324, 0.9910226464271545, 0.9917478561401367, 0.000829785130918026, 0.000829785130918026, 0.002489355392754078, 0.9874443411827087, 0.000829785130918026, 0.007468066178262234, 0.000829785130918026, 0.000757934816647321, 0.000757934816647321, 0.000757934816647321, 0.000757934816647321, 0.9928946495056152, 0.001515869633294642, 0.002273804508149624, 0.0013186325086280704, 0.0013186325086280704, 0.9955675601959229, 0.002506955759599805, 0.038817379623651505, 0.00016173908079508692, 0.0004043476947117597, 0.00032347816159017384, 0.2750372886657715, 0.00016173908079508692, 0.6807597875595093, 8.086954039754346e-05, 8.086954039754346e-05, 0.00024260861391667277, 0.00016173908079508692, 0.0012130431132391095, 0.00041361668263562024, 0.00041361668263562024, 0.9930936098098755, 0.004136166535317898, 0.00041361668263562024, 0.00041361668263562024, 0.00041361668263562024, 0.0008272333652712405, 0.00032618772820569575, 0.00016309386410284787, 0.00032618772820569575, 0.00016309386410284787, 0.00016309386410284787, 0.9942201972007751, 0.002609501825645566, 0.00016309386410284787, 0.00016309386410284787, 0.00016309386410284787, 0.0006523754564113915, 0.0008154693059623241, 0.9945207834243774, 0.7268625497817993, 0.016845740377902985, 0.03306756541132927, 0.03493931517004967, 0.0006239163340069354, 0.00012478326971177012, 0.06151815131306648, 0.0003743497945833951, 0.059022486209869385, 0.003868281375616789, 0.05066200718283653, 0.00985787808895111, 0.000998266157694161, 0.0007486995891667902, 0.0003743497945833951, 0.9832538366317749, 0.0019073790172114968, 0.012397963553667068, 0.0009536895086057484, 0.3282632529735565, 0.0010494349990040064, 0.0033581918105483055, 0.33728837966918945, 0.0009794726502150297, 0.0026585685554891825, 0.0005596986156888306, 0.18337126076221466, 0.007695856038480997, 0.0006296609644778073, 0.00013992465392220765, 0.030433613806962967, 0.1026347354054451, 0.000349811656633392, 0.0005596986156888306, 0.9906385540962219, 0.11385620385408401, 0.04353904351592064, 0.009033367037773132, 0.014095630496740341, 0.04108840599656105, 0.12158375978469849, 0.08199793100357056, 0.3241100609302521, 0.011859648860991001, 0.008782937191426754, 0.013362228870391846, 0.051713794469833374, 0.026098381727933884, 0.0007334020920097828, 0.13814792037010193, 0.9759591817855835, 0.0016448750393465161, 0.0005482916603796184, 0.0142555832862854, 0.0060312082059681416, 0.9843005537986755, 0.012803910300135612, 0.00011205109331058338, 0.0007843576604500413, 0.008964087814092636, 0.13334080576896667, 0.8494593501091003, 0.00011205109331058338, 0.00033615328720770776, 0.00011205109331058338, 0.00022410218662116677, 0.0012325620045885444, 0.00011205109331058338, 0.00011205109331058338, 0.004369992762804031, 0.0005602554883807898, 0.00011205109331058338, 0.996364414691925, 0.056472621858119965, 0.18440039455890656, 0.19315087795257568, 0.00490880711004138, 0.012805582955479622, 0.17223510146141052, 0.04230111092329025, 0.10931699723005295, 0.009433446452021599, 0.03845943510532379, 0.02633681707084179, 0.024885516613721848, 0.02783080004155636, 0.02975163795053959, 0.06778421998023987, 0.9947166442871094, 0.9930776953697205, 0.02752869576215744, 0.01568019576370716, 0.1667718142271042, 0.40684807300567627, 0.019660696387290955, 0.0338156558573246, 0.01809825748205185, 0.030207162722945213, 0.09644343703985214, 0.013038929551839828, 0.060377124696969986, 0.013541142456233501, 0.05390416085720062, 0.02496183104813099, 0.019139884039759636, 0.00023243032046593726, 0.0004648606409318745, 0.0004648606409318745, 0.0004648606409318745, 0.00023243032046593726, 0.00023243032046593726, 0.010459364391863346, 0.01510797068476677, 0.0032540245447307825, 0.9683046936988831, 0.0004648606409318745, 0.0028654413763433695, 0.0028654413763433695, 0.9914426803588867, 0.00017898804799187928, 0.0443890355527401, 0.00017898804799187928, 0.0624668262898922, 0.00017898804799187928, 0.00035797609598375857, 0.00035797609598375857, 0.00017898804799187928, 0.0008949402254074812, 0.0005369641585275531, 0.0225524939596653, 0.8643332719802856, 0.00017898804799187928, 0.003221784718334675, 0.9968977570533752, 0.02559429407119751, 0.6928741335868835, 0.0010664289584383368, 0.00030469399644061923, 0.007465002592653036, 0.10070136189460754, 0.006550920661538839, 0.03625858575105667, 0.0006093879928812385, 0.0030469398479908705, 0.0004570409655570984, 0.0007617349619977176, 0.007465002592653036, 0.11669779568910599, 0.053702499717473984, 0.04185635969042778, 0.060238298028707504, 0.053770579397678375, 0.046499501913785934, 0.09245435148477554, 0.05721549317240715, 0.13864067196846008, 0.06829912215471268, 0.06696473062038422, 0.030459556728601456, 0.05736526846885681, 0.045532748103141785, 0.10228528827428818, 0.08470670133829117, 0.00021029899653512985, 0.0004205979930702597, 0.002102989936247468, 0.024815281853079796, 0.00021029899653512985, 0.00021029899653512985, 0.0004205979930702597, 0.9436116218566895, 0.0004205979930702597, 0.00021029899653512985, 0.025656478479504585, 0.0006308970041573048, 0.0004205979930702597, 0.00021029899653512985, 0.7251333594322205, 0.0040652998723089695, 0.047854386270046234, 0.0006969085079617798, 0.005342965479940176, 0.000929211382754147, 0.07909911870956421, 0.008595204912126064, 0.01231205090880394, 0.001974574290215969, 0.042279116809368134, 0.008362902328372002, 0.015331988222897053, 0.046925175935029984, 0.0011615141993388534, 0.9922230243682861, 0.975377082824707, 0.0013074759626761079, 0.0013074759626761079, 0.014382235705852509, 0.0013074759626761079, 0.0013074759626761079, 0.0039224280044436455, 0.9916260838508606, 0.001048230449669063, 0.001048230449669063, 0.001048230449669063, 0.001048230449669063, 0.003144691465422511, 0.001048230449669063, 0.00037833995884284377, 0.00012611331476364285, 0.00012611331476364285, 0.9969258308410645, 0.00012611331476364285, 0.0002522266295272857, 0.0002522266295272857, 0.00012611331476364285, 0.00012611331476364285, 0.0010089065181091428, 0.00037833995884284377, 0.00012611331476364285, 0.00012611331476364285, 0.0002494612126611173, 0.0002494612126611173, 0.9975953698158264, 0.0002494612126611173, 0.0002494612126611173, 0.0002494612126611173, 0.0002494612126611173, 0.0002494612126611173, 0.0002494612126611173, 0.0004989224253222346, 0.0002494612126611173, 0.9955397844314575, 0.0012528459774330258, 0.00041761534521356225, 0.891608715057373, 0.0008352306904271245, 0.00041761534521356225, 0.09730437397956848, 0.007517076097428799, 0.0008352306904271245, 0.0004935947945341468, 0.0007403921918012202, 0.0009871895890682936, 0.00271477154456079, 0.0009871895890682936, 0.0002467973972670734, 0.0002467973972670734, 0.3867315351963043, 0.007897516712546349, 0.0002467973972670734, 0.0007403921918012202, 0.597990095615387, 0.03957001492381096, 0.033917155116796494, 0.11994660645723343, 0.46671417355537415, 0.027734339237213135, 0.03650804981589317, 0.006889421958476305, 0.07242725789546967, 0.021198222413659096, 0.018312908709049225, 0.037096887826919556, 0.008655940182507038, 0.09762958437204361, 0.006830538157373667, 0.006477234419435263, 0.004465858452022076, 0.0004530581063590944, 0.00012944517948199064, 0.006860594265162945, 0.043364133685827255, 0.13695299625396729, 0.0063428133726119995, 0.002394735813140869, 0.020387614145874977, 0.1804465800523758, 0.0021358453668653965, 0.586063027381897, 0.0011650065425783396, 0.005954477936029434, 0.0027830712497234344, 0.0013169051380828023, 0.7892651557922363, 0.00021948419453110546, 0.00021948419453110546, 0.00021948419453110546, 0.0032922630198299885, 0.0032922630198299885, 0.08537935465574265, 0.00021948419453110546, 0.007242978550493717, 0.00021948419453110546, 0.04433580860495567, 0.003950715530663729, 0.06123609095811844, 0.002081498736515641, 0.0016960359644144773, 0.00015418509428855032, 0.010176216252148151, 0.003006609156727791, 0.0355396643280983, 0.0019273136276751757, 0.0412445105612278, 0.00030837018857710063, 0.018579304218292236, 0.001079295645467937, 0.009096920490264893, 0.8733814358711243, 0.0012334807543084025, 0.000693832931574434, 0.00017869035946205258, 0.0006254162290133536, 0.002233629347756505, 0.00509267533197999, 0.0008041065884754062, 0.0005360710783861578, 8.934517973102629e-05, 0.002591010183095932, 8.934517973102629e-05, 0.9540278315544128, 0.005628746002912521, 0.027875695377588272, 0.00017869035946205258, 8.934517973102629e-05, 8.934517973102629e-05, 8.994001109385863e-05, 0.0005396400229074061, 0.0003597600443754345, 0.000719520088750869, 0.00026982001145370305, 0.00017988002218771726, 8.994001109385863e-05, 0.0005396400229074061, 0.0003597600443754345, 0.9849330186843872, 0.0016189201269298792, 0.009803460910916328, 8.994001109385863e-05, 0.00026982001145370305, 0.00017988002218771726, 0.9896652698516846, 0.001955860061571002, 0.001955860061571002, 0.001955860061571002, 0.0007940882351249456, 0.9910221099853516, 0.0031763529404997826, 0.0007940882351249456, 0.0007940882351249456, 0.0031763529404997826, 0.9958183169364929, 0.9576312899589539, 0.00020125175069551915, 0.00012578234600368887, 0.00025156469200737774, 0.0029055720660835505, 0.03270341083407402, 0.0004276599793229252, 0.0005282858619466424, 2.5156468836939894e-05, 0.0010943063534796238, 0.0003396123356651515, 0.00025156469200737774, 0.0004653946671169251, 0.0008678981685079634, 0.0021760344970971346, 0.0240467619150877, 0.9614762663841248, 0.00039420920074917376, 0.00039420920074917376, 0.00039420920074917376, 0.0011826276313513517, 0.01182627584785223, 0.8899650573730469, 0.00021015350648667663, 7.0051166403573e-05, 0.000280204665614292, 3.50255832017865e-05, 0.10644274950027466, 0.00035025583929382265, 0.000560409331228584, 3.50255832017865e-05, 0.00021015350648667663, 0.00010507675324333832, 0.000140102332807146, 0.0002451790787745267, 7.0051166403573e-05, 0.00133097218349576, 0.0011266174260526896, 0.0011266174260526896, 0.0011266174260526896, 0.011266173794865608, 0.9790304899215698, 0.005633086897432804, 0.001826989697292447, 0.0009134948486462235, 0.9874879717826843, 0.0027404846623539925, 0.0009134948486462235, 0.0009134948486462235, 0.0009134948486462235, 0.0009134948486462235, 0.0027404846623539925, 0.0056401947513222694, 0.002452258486300707, 0.0002452258486300707, 0.0002452258486300707, 0.8938482403755188, 0.0009809033945202827, 0.0002452258486300707, 0.0019618067890405655, 0.0002452258486300707, 0.0002452258486300707, 0.0002452258486300707, 0.0012261292431503534, 0.09195969253778458, 0.9944750070571899, 0.00013460070476867259, 0.00013460070476867259, 0.00040380211430601776, 0.007806840818375349, 0.00026920140953734517, 0.00026920140953734517, 0.00013460070476867259, 0.986757755279541, 0.00040380211430601776, 0.0010768056381493807, 0.001346006989479065, 0.0010768056381493807, 0.00013460070476867259, 0.00028133063460700214, 0.00590794300660491, 0.00028133063460700214, 0.0014066530857235193, 0.0005626612692140043, 0.00028133063460700214, 0.00028133063460700214, 0.00028133063460700214, 0.0005626612692140043, 0.0019693144131451845, 0.9843758344650269, 0.002531975507736206, 0.0005626612692140043, 0.000843991874717176, 0.0003096702857874334, 0.0006193405715748668, 0.0003096702857874334, 0.9878482222557068, 0.0003096702857874334, 0.0003096702857874334, 0.009599778801202774, 0.0003096702857874334, 0.0003096702857874334, 0.979201078414917, 0.0014495638897642493, 0.00020708054944407195, 0.015613873489201069, 0.00016566444537602365, 0.00024849665351212025, 0.0013253155630081892, 0.00024849665351212025, 0.00012424832675606012, 8.283222268801183e-05, 0.00012424832675606012, 0.0002899127721320838, 0.00016566444537602365, 0.0005798255442641675, 0.00012424832675606012, 0.00145947455894202, 0.00145947455894202, 0.9953616261482239, 0.0006290702731348574, 0.0006290702731348574, 0.0006290702731348574, 0.9964472651481628, 0.9932583570480347, 0.9941665530204773, 0.9953083992004395, 0.00012231858272571117, 0.00018347786681260914, 0.003853035159409046, 0.035900503396987915, 0.00018347786681260914, 0.00018347786681260914, 0.00024463716545142233, 0.0003057964495383203, 0.948947548866272, 0.0010397079167887568, 0.00018347786681260914, 0.0029968051239848137, 0.00024463716545142233, 0.005382017232477665, 0.00018347786681260914, 4.7586450818926096e-05, 4.7586450818926096e-05, 9.517290163785219e-05, 0.0009755222708918154, 7.137967622838914e-05, 0.00019034580327570438, 0.00019034580327570438, 0.00016655257786624134, 0.996484100818634, 0.00016655257786624134, 7.137967622838914e-05, 0.0004044848319608718, 0.0007613832131028175, 7.137967622838914e-05, 0.00026172547950409353, 0.000203455172595568, 0.000203455172595568, 6.781839329050854e-05, 0.00023736436560284346, 0.000203455172595568, 0.00013563678658101708, 3.390919664525427e-05, 0.0008477299124933779, 0.00037300115218386054, 0.00023736436560284346, 6.781839329050854e-05, 6.781839329050854e-05, 0.9972694516181946, 3.390919664525427e-05, 0.9955654144287109, 0.9910761117935181, 0.9901381731033325, 0.004420259967446327, 0.9967312216758728, 0.9896079301834106, 0.00024632722488604486, 0.00024632722488604486, 0.997378945350647, 0.00024632722488604486, 0.00024632722488604486, 0.00024632722488604486, 0.00024632722488604486, 0.00024632722488604486, 0.00024632722488604486, 0.0004926544497720897, 0.00024632722488604486, 0.006552981678396463, 0.000624093518126756, 0.028084207326173782, 0.022467367351055145, 0.000624093518126756, 0.016538478434085846, 0.0009361402480863035, 0.000312046759063378, 0.014666197821497917, 0.007489121984690428, 0.9002549052238464, 0.000312046759063378, 0.000312046759063378, 0.9924449920654297, 0.0009151978301815689, 0.9490601420402527, 0.0009151978301815689, 0.031116727739572525, 0.0009151978301815689, 0.0009151978301815689, 0.0009151978301815689, 0.014643165282905102, 0.0027394634671509266, 0.0041091954335570335, 0.0027394634671509266, 0.0041091954335570335, 0.016436781734228134, 0.9670306444168091, 0.0013697317335754633, 0.9952700734138489, 0.9950366616249084, 0.9857005476951599, 0.0010898248292505741, 0.0030878372490406036, 0.10044552385807037, 0.008900236338376999, 0.003996024373918772, 0.00036327497218735516, 0.002542924601584673, 0.0005449124146252871, 0.024884333834052086, 0.815189003944397, 0.007628774270415306, 0.026337435469031334, 0.0009081874159164727, 0.0034511121921241283, 0.0009081874159164727, 0.006743726786226034, 0.002370841335505247, 0.90753173828125, 0.0008429658482782543, 0.00882479827851057, 0.0002370841393712908, 0.0032401499338448048, 0.00039514023228548467, 0.001659589004702866, 0.003319178009405732, 0.05695287883281708, 0.004214829299598932, 0.0031611218582838774, 0.0002897694939747453, 0.00021074146206956357, 0.00024565469357185066, 0.9961298108100891, 0.00024565469357185066, 0.00024565469357185066, 0.0004913093871437013, 0.00024565469357185066, 0.0004913093871437013, 0.0012282734969630837, 0.0004913093871437013, 0.00024565469357185066, 0.0007323931204155087, 0.003661965485662222, 0.9912940859794617, 0.001830982742831111, 0.0003661965602077544, 0.0003661965602077544, 0.0003661965602077544, 0.0003661965602077544, 0.0003661965602077544, 0.00021723390091210604, 0.0006517017027363181, 0.9923244714736938, 0.00021723390091210604, 0.0041274442337453365, 0.0004344678018242121, 0.0004344678018242121, 0.0004344678018242121, 0.0006517017027363181, 0.00021723390091210604, 0.00021723390091210604, 0.00021723390091210604, 0.00010629629832692444, 0.00031888889498077333, 0.9956774711608887, 0.00010629629832692444, 0.0007440741173923016, 0.00031888889498077333, 0.0005314815207384527, 0.0002125925966538489, 0.00010629629832692444, 0.0008503703866153955, 0.0005314815207384527, 0.0004251851933076978, 0.00010629629832692444, 0.00010629629832692444, 0.00028795856633223593, 0.993457019329071, 0.0023036685306578875, 0.0005759171326644719, 0.0011518342653289437, 0.0011518342653289437, 0.0005759171326644719, 0.9945335984230042, 0.001927390694618225, 0.0005683433846570551, 0.0004391744441818446, 0.9532151818275452, 0.0009816840756684542, 0.022837070748209953, 7.75013686507009e-05, 0.010591854341328144, 0.0002066703309537843, 0.0016791963716968894, 0.0005683433846570551, 0.003022553399205208, 0.004030071198940277, 0.0005166758201085031, 0.0011625206097960472, 0.00010333516547689214, 0.0002605404006317258, 0.0002605404006317258, 0.9882296919822693, 0.0002605404006317258, 0.007555671036243439, 0.0002605404006317258, 0.0010421616025269032, 0.0002605404006317258, 0.0002605404006317258, 0.0010421616025269032, 0.0002605404006317258, 0.0002605404006317258, 0.0002605404006317258, 0.992745041847229, 0.9882288575172424, 0.004241325426846743, 0.004241325426846743, 0.001481038285419345, 0.9952577948570251, 0.0037408003117889166, 0.017635202035307884, 0.080872543156147, 0.6761941909790039, 0.02974826842546463, 0.028145069256424904, 0.009084801189601421, 0.02547306939959526, 0.0021376002114266157, 0.012825600802898407, 0.011044267565011978, 0.0037408003117889166, 0.07891307026147842, 0.009797333739697933, 0.011044267565011978, 0.00038156018126755953, 0.00038156018126755953, 0.00038156018126755953, 0.00038156018126755953, 0.0072496430948376656, 0.00038156018126755953, 0.9905301928520203, 0.00038156018126755953, 0.004401721525937319, 0.0006002347799949348, 0.0004001565102953464, 0.0038014869205653667, 0.0002000782551476732, 0.0010003913193941116, 0.0001000391275738366, 0.0001000391275738366, 0.0009003521408885717, 0.9844850301742554, 0.0007002739002928138, 0.0021008215844631195, 0.0006002347799949348, 0.0006002347799949348, 0.0002000782551476732, 0.9967687726020813, 0.9971468448638916, 0.9968664646148682, 0.0012673449236899614, 0.9961330890655518, 0.9958735704421997, 0.05325082689523697, 0.021384190768003464, 0.05681486055254936, 0.0069882976822555065, 0.000838595733512193, 0.05751368775963783, 0.020825127139687538, 0.014815190806984901, 0.028372488915920258, 0.0011181276058778167, 0.006568999961018562, 0.03934411704540253, 0.02236255258321762, 0.0040532127022743225, 0.6657751202583313, 0.9970243573188782, 0.0004535846528597176, 0.004082262050360441, 0.00015119487943593413, 0.0006047795177437365, 0.16102254390716553, 0.8091949820518494, 0.0007559744408354163, 0.007257354445755482, 0.00015119487943593413, 0.0015119488816708326, 0.0019655334763228893, 0.003175092628225684, 0.00030238975887186825, 0.002267923206090927, 0.007106159348040819, 0.00026707511278800666, 0.00026707511278800666, 0.00026707511278800666, 0.00026707511278800666, 0.00026707511278800666, 0.9932523965835571, 0.004807352088391781, 0.00026707511278800666, 0.00026707511278800666, 0.00026707511278800666, 0.0007712577935308218, 0.0007712577935308218, 0.9972363114356995, 0.251558393239975, 0.0024292119778692722, 0.0007017723401077092, 0.0015115097630769014, 0.0059920563362538815, 0.25010088086128235, 0.03379303961992264, 0.3552587628364563, 0.004642494022846222, 0.0015115097630769014, 0.035736408084630966, 0.0012415972305461764, 0.02580362930893898, 0.0010796497808769345, 0.028610719367861748, 0.9969683289527893, 0.9968678951263428, 0.0017717387527227402, 0.9957171678543091, 0.9953498244285583, 0.0035691664088517427, 0.9922282695770264, 0.9868637919425964, 0.0008272118866443634, 0.0008272118866443634, 0.0033088475465774536, 0.0016544237732887268, 0.0008272118866443634, 0.0016544237732887268, 0.0008272118866443634, 0.00248163565993309, 7.297271076822653e-05, 0.00021891813958063722, 0.985350489616394, 0.00021891813958063722, 0.011383743025362492, 0.00014594542153645307, 0.00021891813958063722, 7.297271076822653e-05, 0.00021891813958063722, 7.297271076822653e-05, 0.00014594542153645307, 0.0015324269188567996, 0.00014594542153645307, 0.00021891813958063722, 7.297271076822653e-05, 9.973499254556373e-05, 0.5050579905509949, 0.03640327230095863, 0.0006981449550949037, 0.1055196225643158, 9.973499254556373e-05, 0.0002992049849126488, 9.973499254556373e-05, 0.0002992049849126488, 0.0025931098498404026, 0.3446841239929199, 9.973499254556373e-05, 0.001994699938222766, 0.00209443480707705, 0.0035343130584806204, 0.00016065059753600508, 0.0014458553632721305, 0.049159083515405655, 0.00016065059753600508, 0.0004819517780561, 0.02072392776608467, 0.00016065059753600508, 0.059440720826387405, 0.0008032529731281102, 0.491751492023468, 0.018314167857170105, 0.34989699721336365, 0.00016065059753600508, 0.0038556142244488, 0.047287046909332275, 0.02767324261367321, 0.16213779151439667, 0.16590388119220734, 0.04546425864100456, 0.0678800418972969, 0.03967954218387604, 0.05459326505661011, 0.08375788480043411, 0.036259930580854416, 0.0398603156208992, 0.06379759311676025, 0.07813887298107147, 0.022792387753725052, 0.06476171314716339, 0.05808937922120094, 0.029055321589112282, 0.15548264980316162, 0.13474099338054657, 0.05062621086835861, 0.08320049196481705, 0.03847464546561241, 0.0877719447016716, 0.101826511323452, 0.03559356555342674, 0.04618233069777489, 0.043099258095026016, 0.068699412047863, 0.030203500762581825, 0.03696500137448311, 0.9966961145401001, 0.07982166856527328, 0.007684694603085518, 0.007560748141258955, 0.033837445080280304, 0.004586027469485998, 0.08044140040874481, 0.018592003732919693, 0.077466681599617, 0.022310404106974602, 0.025037230923771858, 0.022558297961950302, 0.5400357246398926, 0.00855232123285532, 0.055404167622327805, 0.015865176916122437, 0.0004172564949840307, 0.0004172564949840307, 0.0004172564949840307, 0.0025035389699041843, 0.9947394728660583, 0.0004172564949840307, 0.0004172564949840307, 0.0004172564949840307, 0.9918838143348694, 0.00012643302034121007, 0.00012643302034121007, 0.00012643302034121007, 0.0005057320813648403, 0.00025286604068242013, 0.00012643302034121007, 0.0018964953487738967, 0.9960393309593201, 0.00012643302034121007, 0.00012643302034121007, 0.00012643302034121007, 0.00025286604068242013, 0.00025286604068242013, 0.0016600230010226369, 0.000553341000340879, 0.9949071407318115, 0.0016600230010226369, 0.0006264398689381778, 0.9954129457473755, 0.0006264398689381778, 0.0012528797378763556, 0.9970347881317139, 0.0009257519268430769, 0.0005396706401370466, 0.0005396706401370466, 0.9967717528343201, 0.0005396706401370466, 0.0005396706401370466, 0.0005396706401370466, 0.0002989550703205168, 0.00025909440591931343, 0.00011958202958339825, 0.000538119173143059, 0.00027902473811991513, 7.972135790623724e-05, 7.972135790623724e-05, 1.993033947655931e-05, 0.000637770863249898, 0.0011360292555764318, 0.0005580494762398303, 0.0003986067895311862, 0.0007772832177579403, 0.9947232007980347, 9.965169738279656e-05, 0.0002153178647859022, 0.00017943154671229422, 3.588631079765037e-05, 0.0002153178647859022, 0.00017943154671229422, 0.0001076589323929511, 3.588631079765037e-05, 0.0010407030349597335, 0.00035886309342458844, 0.0002153178647859022, 7.177262159530073e-05, 0.0001076589323929511, 0.9971011281013489, 3.588631079765037e-05, 0.0009837988764047623, 0.0009837988764047623, 0.9965882301330566, 0.9968211650848389, 0.0009242783416993916, 0.0009242783416993916, 0.9954477548599243, 0.9895743131637573, 0.9821892976760864, 0.0052244109101593494, 0.0026122054550796747, 0.007836616598069668, 0.9932237267494202, 0.994118869304657, 0.9964569807052612, 0.996224045753479, 0.0006378997350111604, 0.00318949855864048, 0.0038273981772363186, 0.0006378997350111604, 0.9874687790870667, 0.0006378997350111604, 0.0038273981772363186, 0.9819701910018921, 0.005206628702580929, 0.0062479544430971146, 0.0010413257405161858, 0.0010413257405161858, 0.0010413257405161858, 0.0010413257405161858, 0.9911991953849792, 0.0018649387639015913, 0.9940123558044434, 0.01804051175713539, 0.007865662686526775, 0.0520288348197937, 0.002477563451975584, 0.407186359167099, 0.047675058245658875, 0.0031991840805858374, 0.018930509686470032, 0.0003127022064290941, 0.1612340658903122, 0.0022610772866755724, 0.02273104339838028, 0.009789983741939068, 0.0012267547426745296, 0.24506230652332306, 0.024257130920886993, 0.011384832672774792, 0.004920078441500664, 0.14502789080142975, 0.0008581532165408134, 0.0004004715010523796, 0.1379910409450531, 5.721021443605423e-05, 0.23644980788230896, 0.004748447798192501, 0.2669428586959839, 0.11504974216222763, 0.05126035213470459, 0.00011442042887210846, 0.00045768171548843384, 0.0011660268064588308, 0.9969528913497925, 0.3198155462741852, 0.014954661950469017, 0.08210644870996475, 0.00869265478104353, 0.005932427942752838, 0.005170275457203388, 0.30469608306884766, 0.01798267289996147, 0.11644449830055237, 0.0012359224492684007, 0.046676672995090485, 0.015449031256139278, 0.01855943538248539, 0.001606699195690453, 0.040661849081516266, 0.0013771210797131062, 0.0006885605398565531, 0.0006885605398565531, 0.0006885605398565531, 0.9777559041976929, 0.0027542421594262123, 0.0006885605398565531, 0.0006885605398565531, 0.01514833141118288, 0.027666892856359482, 0.0005763936205767095, 0.0005763936205767095, 0.0017291808035224676, 0.0005763936205767095, 0.002305574482306838, 0.004611148964613676, 0.0034583616070449352, 0.9452854990959167, 0.012104266323149204, 0.0327250137925148, 0.03615151345729828, 0.1112842932343483, 0.12246854603290558, 0.019057506695389748, 0.05107026919722557, 0.0535920225083828, 0.06623927503824234, 0.06650877743959427, 0.039847515523433685, 0.03628626465797424, 0.06710552424192429, 0.15153606235980988, 0.05532452091574669, 0.09078303724527359, 0.01643741875886917, 0.04558691009879112, 0.004449225962162018, 0.012129437178373337, 0.04772324487566948, 0.1291687935590744, 0.09922479838132858, 0.13294710218906403, 0.03822449967265129, 0.06110623478889465, 0.006815084256231785, 0.041985154151916504, 0.011793979443609715, 0.11504426598548889, 0.23734501004219055, 0.9975630640983582, 0.0629100352525711, 0.037913862615823746, 0.07265075296163559, 0.0808928981423378, 0.026434803381562233, 0.07047782093286514, 0.0736098363995552, 0.0674956664443016, 0.07521330565214157, 0.04410296306014061, 0.036160532385110855, 0.0861828550696373, 0.08408485352993011, 0.08964455127716064, 0.09222209453582764, 0.03285781666636467, 0.0580577626824379, 0.006621464155614376, 0.01053676474839449, 0.03921058401465416, 0.15058553218841553, 0.09270049631595612, 0.15987476706504822, 0.04780888929963112, 0.03606298938393593, 0.010555957444012165, 0.035218510776758194, 0.00875184778124094, 0.1024695560336113, 0.2086624801158905, 0.23195986449718475, 0.017420461401343346, 0.04419442266225815, 0.038526687771081924, 0.027817104011774063, 0.1650945097208023, 0.028686387464404106, 0.13686014711856842, 0.0408911406993866, 0.02319251000881195, 0.026495790109038353, 0.0585550032556057, 0.06526587903499603, 0.040578197687864304, 0.05452152341604233, 0.9965880513191223, 0.9934722781181335, 0.0005600830772891641, 0.9420597553253174, 0.00840124674141407, 0.0005600830772891641, 0.0005600830772891641, 0.04648689553141594, 0.9945052266120911, 0.0017886784626170993, 0.005381548777222633, 0.9803388118743896, 0.0008969248156063259, 0.009866173379123211, 0.0026907743886113167, 0.00016643702110741287, 0.015104159712791443, 0.06399503350257874, 0.00020804627274628729, 0.00020804627274628729, 0.00037448329385370016, 0.0008321850909851491, 0.00016643702110741287, 0.0012898868881165981, 0.9131566882133484, 0.002829429227858782, 0.00037448329385370016, 0.0004993110778741539, 4.160925527685322e-05, 0.0007073573069646955, 0.00015413257642649114, 0.0010789280058816075, 0.00046239770017564297, 0.00015413257642649114, 0.00015413257642649114, 0.00015413257642649114, 0.00015413257642649114, 0.00015413257642649114, 0.00015413257642649114, 0.9967753291130066, 0.00015413257642649114, 0.00015413257642649114, 0.0003082651528529823, 0.0009374655783176422, 0.9965259432792664, 0.0004544522089418024, 0.0009089044178836048, 0.0004544522089418024, 0.0004544522089418024, 0.9911602735519409, 0.0004544522089418024, 0.004998974036425352, 0.0009089044178836048, 0.0016668314347043633, 0.0014287126250565052, 0.003452722216024995, 0.03345568850636482, 0.02797895483672619, 0.20204377174377441, 0.00023811876599211246, 0.007619800511747599, 0.0008334157173521817, 0.0086913350969553, 0.09322349727153778, 0.5866056084632874, 0.031312618404626846, 0.0005952969077043235, 0.0008334157173521817, 0.04400498792529106, 0.0007458472391590476, 0.0007458472391590476, 0.0014916944783180952, 0.012679403647780418, 0.020137876272201538, 0.0014916944783180952, 0.008204319514334202, 0.8972542881965637, 0.005220931023359299, 0.0007458472391590476, 0.005966777913272381, 0.0029629101045429707, 0.99257493019104, 0.054289139807224274, 0.02553379349410534, 0.07147075980901718, 0.059181131422519684, 0.1415693759918213, 0.024519601836800575, 0.11096461862325668, 0.017062300816178322, 0.06771228462457657, 0.019866246730089188, 0.014854940585792065, 0.3120134174823761, 0.061090197414159775, 0.0066817402839660645, 0.013184505514800549, 0.0008359753410331905, 0.00041798767051659524, 0.9891678094863892, 0.006687802728265524, 0.0006269814912229776, 0.00020899383525829762, 0.00020899383525829762, 0.00020899383525829762, 0.0012539629824459553, 0.14600631594657898, 0.041132375597953796, 0.006628395989537239, 0.04830557107925415, 0.002542398404330015, 0.00036319976788945496, 0.19077068567276, 0.0016797989374026656, 0.0036319978535175323, 0.12117252498865128, 0.015299790538847446, 0.16938729584217072, 0.25169745087623596, 9.079994197236374e-05, 0.0012711992021650076, 0.00020837299234699458, 0.00041674598469398916, 0.00020837299234699458, 0.00020837299234699458, 0.00020837299234699458, 0.0006251189624890685, 0.00020837299234699458, 0.00395908672362566, 0.08439106494188309, 0.9093397259712219, 0.00020837299234699458, 0.00020837299234699458, 0.022024543955922127, 0.017776155844330788, 0.04382548853754997, 0.5630233883857727, 0.04695587977766991, 0.018894152715802193, 0.011515370570123196, 0.02739093080163002, 0.015204761177301407, 0.04360188916325569, 0.06316684186458588, 0.028173528611660004, 0.07937780022621155, 0.0045837885700166225, 0.014533963054418564, 0.0018884731689468026, 0.0018884731689468026, 0.0018884731689468026, 0.0018884731689468026, 0.009442365728318691, 0.9782291054725647, 0.0037769463378936052, 0.0018884731689468026, 0.989305317401886, 0.0002010812022490427, 6.702706741634756e-05, 0.00026810826966539025, 0.00013405413483269513, 0.07875680178403854, 0.0005362165393307805, 0.0004021624044980854, 0.0005362165393307805, 0.00026810826966539025, 0.004557840526103973, 0.9080156683921814, 0.0016086496179923415, 0.00046918948646634817, 0.003217299235984683, 0.0008043248089961708, 0.004209646489471197, 0.0032741695176810026, 0.00023386924294754863, 0.23761115968227386, 0.003975776955485344, 0.0035080385860055685, 0.00046773848589509726, 0.004209646489471197, 0.6372936964035034, 0.08629775047302246, 0.0114595927298069, 0.0035080385860055685, 0.00023386924294754863, 0.0035080385860055685, 0.00020079418027307838, 0.00010039709013653919, 0.0008031767210923135, 0.00010039709013653919, 0.00010039709013653919, 0.00010039709013653919, 0.00010039709013653919, 0.5626252889633179, 0.00010039709013653919, 0.00010039709013653919, 0.4354221820831299, 0.00010039709013653919, 0.00010039709013653919, 0.00010039709013653919, 0.0006460912409238517, 0.9762438535690308, 0.0012921824818477035, 0.013567916117608547, 0.0009691368322819471, 0.006460912059992552, 0.00032304562046192586, 0.34717056155204773, 0.05919723957777023, 0.018560461699962616, 0.04423440247774124, 0.020768092945218086, 0.05453668534755707, 0.13850845396518707, 0.07121656835079193, 0.02371160127222538, 0.008585235103964806, 0.037693269550800323, 0.05232905223965645, 0.12019329518079758, 0.0015535186976194382, 0.0017988111358135939, 0.004215070512145758, 0.0010729270288720727, 0.0013028399553149939, 7.663763972232118e-05, 0.0007663764408789575, 0.00022991292644292116, 0.00030655055888928473, 0.00145611516200006, 7.663763972232118e-05, 0.00015327527944464236, 0.001226202235557139, 0.00022991292644292116, 0.00022991292644292116, 7.663763972232118e-05, 0.9887022376060486, 0.0008373522432520986, 0.00446587847545743, 0.0013955871108919382, 0.0066988179460167885, 0.03321497142314911, 0.0013955871108919382, 0.05889377370476723, 0.005303230602294207, 0.010885578580200672, 0.0005582348094321787, 0.00027911740471608937, 0.8479586839675903, 0.013397635892033577, 0.002232939237728715, 0.0125602837651968, 0.0001785311324056238, 0.0001785311324056238, 0.9890624284744263, 0.0001785311324056238, 0.00589152704924345, 0.0001785311324056238, 0.0010711867362260818, 0.0001785311324056238, 0.0001785311324056238, 0.0003570622648112476, 0.0016067801043391228, 0.0005355933681130409, 0.0003570622648112476, 0.9930669665336609, 0.002609899966046214, 0.001304949983023107, 0.002851091790944338, 0.3930705189704895, 0.00019007278024218976, 9.503639012109488e-05, 0.00019007278024218976, 0.10881666839122772, 0.00019007278024218976, 0.4098919630050659, 9.503639012109488e-05, 9.503639012109488e-05, 0.002756055211648345, 9.503639012109488e-05, 9.503639012109488e-05, 0.028700990602374077, 0.05255512520670891, 0.003755250247195363, 0.22255337238311768, 0.0090155228972435, 0.04810227081179619, 0.0901990681886673, 0.0926392450928688, 0.01778264343738556, 0.03404565528035164, 0.014728764072060585, 0.012347029522061348, 0.003945204429328442, 0.07077988982200623, 0.33288758993148804, 0.007919632829725742, 0.039305925369262695, 9.969522943720222e-05, 0.1882578283548355, 9.969522943720222e-05, 0.0004984761471860111, 0.0005649396334774792, 6.646348629146814e-05, 0.0001329269725829363, 6.646348629146814e-05, 3.323174314573407e-05, 6.646348629146814e-05, 0.0002326222020201385, 0.0003655491746030748, 0.8075313568115234, 6.646348629146814e-05, 0.001960672903805971, 0.0004020508495159447, 0.0001148716764873825, 5.743583824369125e-05, 0.9848523139953613, 0.000229743352974765, 0.00017230751109309494, 0.00028717919485643506, 0.00017230751109309494, 0.000229743352974765, 0.0005743583897128701, 5.743583824369125e-05, 0.010912809520959854, 0.0016082033980637789, 0.000229743352974765, 0.0001148716764873825, 0.0003432050871197134, 0.0003432050871197134, 0.0013728203484788537, 0.0006864101742394269, 0.0003432050871197134, 0.0003432050871197134, 0.0003432050871197134, 0.0006864101742394269, 0.0020592305809259415, 0.9915195107460022, 0.0013728203484788537, 0.0003432050871197134, 0.0007746658520773053, 0.00015493316459469497, 0.00010328878124710172, 0.0004647994937840849, 0.00010328878124710172, 0.9643557071685791, 0.00020657756249420345, 0.03041854500770569, 5.164439062355086e-05, 0.00010328878124710172, 0.00010328878124710172, 5.164439062355086e-05, 0.002014131285250187, 0.0006713770562782884, 0.0004131551249884069, 0.0016291553620249033, 0.0004072888405062258, 0.9945993423461914, 0.0016291553620249033, 0.0004072888405062258, 0.0004072888405062258, 0.0004072888405062258, 0.0004072888405062258, 0.9943651556968689, 0.9741631746292114, 0.0006346339941956103, 0.0006346339941956103, 0.010154143907129765, 0.0006346339941956103, 0.0006346339941956103, 0.0006346339941956103, 0.012058045715093613, 0.9792383313179016, 0.01790607161819935, 0.0011191294761374593, 0.0018100027227774262, 0.0036200054455548525, 0.9918814897537231, 0.0018100027227774262, 0.001970018958672881, 0.003940037917345762, 0.9889494776725769, 0.001970018958672881, 0.0074395667761564255, 0.0037197833880782127, 0.0009299458470195532, 0.9857426285743713, 0.0009299458470195532, 0.9963688254356384, 0.997323215007782, 0.9916889667510986, 0.0029514553025364876, 0.9951280951499939, 0.9918290376663208, 0.995351254940033, 0.06854739785194397, 0.20553748309612274, 0.0046768588945269585, 0.03357565775513649, 0.0008376463665626943, 0.003909016493707895, 0.3333832621574402, 0.0026176448445767164, 0.04903721436858177, 0.0506427027285099, 0.002512939041480422, 0.016787828877568245, 0.16913476586341858, 0.002233723644167185, 0.05650622770190239, 0.0002644143532961607, 0.0002644143532961607, 0.0005288287065923214, 0.9214839935302734, 0.0010576574131846428, 0.0002644143532961607, 0.0002644143532961607, 0.002644143532961607, 0.0002644143532961607, 0.0005288287065923214, 0.010840987786650658, 0.0002644143532961607, 0.06134412810206413, 0.0006193884182721376, 0.8500074148178101, 0.00020646280609071255, 0.00020646280609071255, 0.10777358710765839, 0.0006193884182721376, 0.0004129256121814251, 0.00020646280609071255, 0.004955107346177101, 0.0344792902469635, 0.9857267737388611, 0.005506853573024273, 0.0027534267865121365, 0.0027534267865121365, 0.0010652064811438322, 0.0010652064811438322, 0.9959680438041687, 0.9973515868186951, 0.9939072728157043, 0.00039882055716589093, 0.0025923335924744606, 0.00039882055716589093, 0.04706082493066788, 0.00039882055716589093, 0.012562847696244717, 0.8837863802909851, 0.0007976411143317819, 0.022732771933078766, 0.00039882055716589093, 0.00039882055716589093, 0.0005982308648526669, 0.0029911540914326906, 0.00019941027858294547, 0.024726875126361847, 0.00018713122699409723, 0.00018713122699409723, 0.0016841809265315533, 0.00018713122699409723, 0.00018713122699409723, 0.0020584433805197477, 0.00018713122699409723, 0.9783220291137695, 0.00018713122699409723, 0.0005613936809822917, 0.01534476038068533, 0.0011227873619645834, 0.0006412214715965092, 0.997740626335144, 0.0003763430577237159, 0.0007526861154474318, 0.0003763430577237159, 0.0003763430577237159, 0.9969327449798584, 0.0003763430577237159, 0.0006788697210140526, 8.485871512675658e-05, 0.00016971743025351316, 0.0019517504842951894, 8.485871512675658e-05, 0.00025457615265622735, 0.00021214679873082787, 0.00012728807632811368, 0.9940774440765381, 0.00016971743025351316, 0.00012728807632811368, 0.000763728457968682, 0.0005940110422670841, 4.242935756337829e-05, 0.0008061578264459968, 0.001172917545773089, 0.0005864587728865445, 0.9963935017585754, 0.0004439798358362168, 0.0004439798358362168, 0.9962908029556274, 0.0004439798358362168, 0.0008879596716724336, 0.0008879596716724336, 0.0004147131694480777, 0.9961410164833069, 0.0004147131694480777, 0.0004147131694480777, 0.001244139508344233, 0.0004147131694480777, 7.118272333173081e-05, 7.118272333173081e-05, 0.0005694617866538465, 7.118272333173081e-05, 0.00028473089332692325, 0.0012101063039153814, 0.00014236544666346163, 0.9957751035690308, 7.118272333173081e-05, 0.00014236544666346163, 0.0004270963545423001, 0.0004982790560461581, 7.118272333173081e-05, 0.0006406445172615349, 0.00015023062587715685, 0.000901383813470602, 0.000450691906735301, 0.00015023062587715685, 0.000450691906735301, 0.9967802166938782, 0.00015023062587715685, 0.00015023062587715685, 0.00015023062587715685, 0.00015023062587715685, 0.00015023062587715685, 0.0003004612517543137, 0.00022516613535117358, 0.00045033227070234716, 0.00022516613535117358, 0.00022516613535117358, 0.00045033227070234716, 0.9970356822013855, 0.00022516613535117358, 0.00022516613535117358, 0.00022516613535117358, 0.00045033227070234716, 9.281254460802302e-05, 9.281254460802302e-05, 0.0004640627303160727, 0.00018562508921604604, 0.0004640627303160727, 0.0002784376556519419, 0.9971780180931091, 9.281254460802302e-05, 0.00037125017843209207, 9.281254460802302e-05, 0.0004640627303160727, 0.00037939895992167294, 0.0011381969088688493, 0.9959222674369812, 0.00037939895992167294, 0.0007587979198433459, 0.00037939895992167294, 0.00037939895992167294, 0.9973934888839722, 0.0007672257488593459, 0.0007672257488593459, 0.0014423680258914828, 0.0003605920064728707, 0.9959551095962524, 0.0003605920064728707, 0.0010817759903147817, 0.03263668715953827, 0.006488952320069075, 0.06954146176576614, 0.6373887658119202, 0.04021322354674339, 0.0237714983522892, 0.013773029670119286, 0.02853311039507389, 0.015390697866678238, 0.022967234253883362, 0.029035774990916252, 0.0027052531950175762, 0.06196492165327072, 0.009779125452041626, 0.005794359836727381, 0.00046359075349755585, 0.9967201352119446, 0.0009271815069951117, 0.00046359075349755585, 0.00046359075349755585, 0.00072218052810058, 0.02672068029642105, 0.00048145369510166347, 0.9689255356788635, 0.00024072684755083174, 0.00024072684755083174, 0.0009629073902033269, 0.0009629073902033269, 0.00024072684755083174, 0.00024072684755083174, 0.9904184341430664, 0.001479433267377317, 0.001479433267377317, 0.001479433267377317, 0.9882614016532898, 0.007397166453301907, 0.000533892132807523, 0.0002669460664037615, 0.00010677842510631308, 0.0007474489975720644, 0.0008542274008505046, 0.9933063387870789, 5.338921255315654e-05, 0.0006139759789220989, 0.00013347303320188075, 0.00016016764857340604, 0.0008809220162220299, 0.00013347303320188075, 0.00013347303320188075, 0.0016550656873732805, 0.0003737244987860322, 0.9960261583328247, 0.0026667367201298475, 0.9933220148086548, 0.0031179774086922407, 0.9743679165840149, 0.0015589887043461204, 0.0015589887043461204, 0.0031179774086922407, 0.0015589887043461204, 0.014030897989869118, 0.00083084189100191, 0.8656792640686035, 9.660951764089987e-05, 0.00017389713320881128, 0.0016423618653789163, 0.11513922363519669, 0.00027050665812566876, 0.0008501637494191527, 7.728761556791142e-05, 0.00011593141971388832, 0.000289828545646742, 0.0006183009245432913, 9.660951764089987e-05, 0.004038278013467789, 0.010105355642735958, 0.0002561082364991307, 0.9948097467422485, 8.53694073157385e-05, 0.000170738814631477, 8.53694073157385e-05, 0.000170738814631477, 0.000341477629262954, 8.53694073157385e-05, 0.000170738814631477, 0.0016220187535509467, 8.53694073157385e-05, 8.53694073157385e-05, 0.000341477629262954, 0.001707388204522431, 0.9912488460540771, 0.0027766074053943157, 0.0006334475474432111, 0.9945126175880432, 0.0012668950948864222, 0.0006334475474432111, 0.0012668950948864222, 0.0006334475474432111, 0.00033502228325232863, 0.990325927734375, 0.0010050669079646468, 0.0020101338159292936, 0.0010050669079646468, 0.00033502228325232863, 0.00033502228325232863, 0.004355289973318577, 0.0005170319345779717, 8.617198909632862e-05, 0.00034468795638531446, 0.0010340638691559434, 0.01034063845872879, 0.0004308599454816431, 0.0011202358873561025, 0.0005170319345779717, 8.617198909632862e-05, 0.9801201820373535, 8.617198909632862e-05, 0.0006032038945704699, 0.0037915674038231373, 0.0008617198909632862, 0.6767709851264954, 0.00487477146089077, 0.008163773454725742, 0.09908119589090347, 0.00017619655409362167, 0.0007047862163744867, 0.12045971304178238, 0.00023492873879149556, 0.000646054046228528, 0.00041112530743703246, 0.00035239310818724334, 0.00041112530743703246, 0.08739349246025085, 5.873218469787389e-05, 0.0002936609380412847, 0.9990870356559753, 0.9899345636367798, 0.002999801654368639, 0.9923080205917358, 0.0021618911996483803, 0.9922658205032349, 0.0044943662360310555, 0.9887605905532837, 0.0029962442349642515, 0.9903050065040588, 0.004671250004321337, 0.001521892612800002, 0.00010145951091544703, 0.00020291902183089405, 0.00010145951091544703, 0.011566384695470333, 0.0023335686419159174, 0.00010145951091544703, 0.001014595152810216, 0.020190441980957985, 0.0016233521746471524, 0.003246704349294305, 0.00020291902183089405, 0.001014595152810216, 0.03520645201206207, 0.9214552640914917, 0.0006796673987992108, 0.9957127571105957, 0.0006796673987992108, 0.0006796673987992108, 0.0013593347975984216, 0.0006796673987992108, 0.010744082741439342, 0.00024418369866907597, 0.00024418369866907597, 0.0014651021920144558, 0.00012209184933453798, 0.00012209184933453798, 0.9392525553703308, 0.0032964798156172037, 0.03516245260834694, 0.0007325510960072279, 0.0013430103426799178, 0.00012209184933453798, 0.0010988266440108418, 0.005982500500977039, 0.610870361328125, 0.005005424842238426, 0.0006256781052798033, 0.00020855935872532427, 0.00041711871745064855, 0.3797866106033325, 0.0006256781052798033, 0.00020855935872532427, 0.0008342374349012971, 0.00020855935872532427, 0.0010427968809381127, 0.9930955767631531, 0.0005583991296589375, 0.0003140995104331523, 0.0003838994016405195, 0.04051883518695831, 0.17317353188991547, 0.04477662965655327, 0.02024196833372116, 0.0017798972548916936, 0.012773379683494568, 0.001919497037306428, 0.0025476959999650717, 0.5140063762664795, 0.001954396953806281, 0.010818983428180218, 0.1741856336593628, 0.0002020659449044615, 0.00017319938342552632, 0.0002020659449044615, 8.659969171276316e-05, 0.5223116278648376, 0.007620772812515497, 8.659969171276316e-05, 0.0009814631193876266, 8.659969171276316e-05, 0.00031753219082020223, 0.00031753219082020223, 5.773312659584917e-05, 5.773312659584917e-05, 0.0662776306271553, 0.4012163579463959, 0.007619346491992474, 0.00012329038872849196, 0.00027123885229229927, 0.00012329038872849196, 0.6619214415550232, 0.031143153086304665, 0.0006657681078650057, 0.002021962543949485, 4.931615694658831e-05, 0.0010602973634377122, 0.00022192270262166858, 0.00014794847811572254, 0.00012329038872849196, 0.00044384540524333715, 0.29404759407043457, 0.0001516828779131174, 0.0005633935215882957, 0.0005633935215882957, 0.0026436159387230873, 0.0008667592774145305, 0.00017335185839328915, 8.667592919664457e-05, 0.0001516828779131174, 0.00301198847591877, 0.5160901546478271, 0.0016251737251877785, 0.46356454491615295, 0.0011917940573766828, 0.009144310839474201, 0.0001950208388734609, 0.002606625435873866, 0.0006516563589684665, 0.0006516563589684665, 0.001303312717936933, 0.9879110455513, 0.0006516563589684665, 0.003909938037395477, 0.001303312717936933, 0.0006516563589684665, 0.001128057250753045, 0.984794020652771, 0.00225611450150609, 0.00225611450150609, 0.005640286486595869, 0.003384171985089779, 0.9957096576690674, 0.013272413052618504, 0.005152819212526083, 0.9268828630447388, 0.0001561460376251489, 0.0003122920752502978, 0.0009368762257508934, 0.0006245841505005956, 0.015770750120282173, 0.0001561460376251489, 0.026076387614011765, 0.0003122920752502978, 0.009056470356881618, 0.0003122920752502978, 0.0009368762257508934, 0.9956364631652832, 0.0012830366613343358, 0.020502736791968346, 0.01236672978848219, 0.014319371432065964, 0.021641777828335762, 0.0017899214290082455, 0.010739528574049473, 0.009275048039853573, 0.008461447432637215, 0.005857924930751324, 0.008461447432637215, 0.006020645145326853, 0.002278081839904189, 0.7765004634857178, 0.0929131954908371, 0.008786886930465698, 0.1436312347650528, 0.002463720506057143, 0.0004044914385303855, 0.0005148072959855199, 0.0002941755810752511, 0.002978527918457985, 0.0011399304494261742, 0.6600196957588196, 0.00022063168580643833, 0.042398057878017426, 0.0012134743155911565, 0.00011031584290321916, 0.0037139668129384518, 7.354389526881278e-05, 0.14087332785129547, 0.0001572372711962089, 0.0003144745423924178, 0.0007861863123252988, 0.0003144745423924178, 0.0001572372711962089, 0.0001572372711962089, 0.002358559053391218, 0.0006289490847848356, 0.011950032785534859, 0.0026730336248874664, 0.0006289490847848356, 0.9797453880310059, 0.0001572372711962089, 0.04948596656322479, 0.007866635918617249, 0.017877425998449326, 0.03206292912364006, 0.0044871061109006405, 0.24642165005207062, 0.09506700932979584, 0.38144662976264954, 0.005566283594816923, 0.0046433028765022755, 0.02112915739417076, 0.00874701701104641, 0.02592865750193596, 0.09229806810617447, 0.006957854609936476, 0.0005649582599289715, 0.0009886769112199545, 0.5799296498298645, 0.00014123956498224288, 0.01370023749768734, 0.00028247912996448576, 0.00014123956498224288, 0.00014123956498224288, 0.00014123956498224288, 0.40182653069496155, 0.0005649582599289715, 0.001553635112941265, 0.00014123956498224288, 0.00014123956498224288, 0.021051576361060143, 0.00025989601272158325, 0.0007796880090609193, 0.00025989601272158325, 0.0005197920254431665, 0.0051979199051856995, 0.00025989601272158325, 0.008316672407090664, 0.00025989601272158325, 0.00025989601272158325, 0.0005197920254431665, 0.00025989601272158325, 0.9579766392707825, 0.004158336203545332, 0.0012741188984364271, 0.006370594259351492, 0.0012741188984364271, 0.0050964755937457085, 0.0025482377968728542, 0.0012741188984364271, 0.9721526503562927, 0.008918832056224346, 0.0007877071620896459, 0.0038510130252689123, 0.00013128452701494098, 0.0009189916891045868, 0.0001750460360199213, 0.007264410611242056, 0.6885436177253723, 0.0025819290895015, 0.004069820512086153, 0.0015754143241792917, 0.0007877071620896459, 0.00043761509004980326, 0.00026256905402988195, 0.0001750460360199213, 0.28838834166526794, 0.011682338081300259, 0.4786030352115631, 0.0013670821208506823, 0.002485603792592883, 0.03343137353658676, 0.3085877299308777, 0.0008699613390490413, 0.028087323531508446, 0.0007456811727024615, 0.04287666827440262, 0.008823893964290619, 0.016280705109238625, 0.0008699613390490413, 0.04846927523612976, 0.016777826473116875, 0.0005527644534595311, 0.0004264182935003191, 7.896635361248627e-05, 0.00022110578720457852, 7.896635361248627e-05, 0.9935072660446167, 0.0002053125062957406, 0.0009791827760636806, 0.0007422837079502642, 3.1586539989802986e-05, 0.00028427885263226926, 0.001989952055737376, 0.00034745194716379046, 0.00012634615995921195, 0.0004264182935003191, 0.00038308085640892386, 0.00038308085640892386, 0.00038308085640892386, 0.0015323234256356955, 0.003447727533057332, 0.00038308085640892386, 0.8335838913917542, 0.15897855162620544, 0.6076620221138, 0.06038655340671539, 0.0019257670501247048, 0.002806117758154869, 0.0001925766991917044, 0.21527324616909027, 0.05884593725204468, 0.020935839042067528, 0.01675417274236679, 0.0011554602533578873, 0.0027786067221313715, 0.0018707450944930315, 0.00272358488291502, 0.0012655040482059121, 0.005392147693783045, 0.00032096999348141253, 0.002086305059492588, 0.05183665454387665, 0.009147644974291325, 0.00016048499674070626, 0.0006419399869628251, 0.00032096999348141253, 0.27635517716407776, 0.00032096999348141253, 0.6445077657699585, 0.00016048499674070626, 0.013801710680127144, 0.00032096999348141253, 0.00032096999348141253, 0.0002906576555687934, 0.7964019775390625, 0.0002906576555687934, 0.0023252612445503473, 0.0029065764974802732, 0.008429071865975857, 0.0005813153111375868, 0.0002906576555687934, 0.0002906576555687934, 0.06249139457941055, 0.1255641132593155, 0.9953300356864929, 0.0009595937444828451, 0.9960582852363586, 0.0009595937444828451, 0.0009595937444828451, 0.05751623213291168, 0.1433606743812561, 0.18992574512958527, 0.015023909509181976, 0.018101096153259277, 0.10123038291931152, 0.04520748555660248, 0.1341743767261505, 0.09539277851581573, 0.010792778804898262, 0.013711580075323582, 0.06760759651660919, 0.027151644229888916, 0.02436860091984272, 0.05643016844987869, 0.9960587024688721, 0.000589638133533299, 0.0008844572002999485, 0.016657277941703796, 0.000589638133533299, 0.0020637335255742073, 0.00014740953338332474, 0.00014740953338332474, 0.9758511185646057, 0.0002948190667666495, 0.0010318667627871037, 0.0002948190667666495, 0.0013266857713460922, 0.0002948190667666495, 0.00016703280562069267, 0.00011930915206903592, 0.9750898480415344, 0.001240815152414143, 0.017872510477900505, 0.00016703280562069267, 0.0004056511097587645, 0.00038178928662091494, 0.000978335039690137, 0.00016703280562069267, 0.00019089464331045747, 0.0002863419649656862, 0.002744110533967614, 7.158549124142155e-05, 0.00011930915206903592, 0.0003099961904808879, 0.9950878024101257, 0.0006199923809617758, 0.0006199923809617758, 0.0003099961904808879, 0.002479969523847103, 0.0003099961904808879, 0.0032358078751713037, 0.9901571869850159, 0.9930509924888611, 0.9919068217277527, 0.0005578778800554574, 0.004463023040443659, 0.0005578778800554574, 0.0002789389400277287, 0.0005578778800554574, 0.0016736335819587111, 0.0002789389400277287, 0.00014101775013841689, 7.050887506920844e-05, 0.9847974181175232, 0.0004230532213114202, 0.0019742483273148537, 0.0007755975821055472, 0.0012691597221419215, 0.0004935620818287134, 0.00014101775013841689, 0.00014101775013841689, 0.0002115266106557101, 0.0005640710005536675, 0.008743099868297577, 7.050887506920844e-05, 0.00014101775013841689, 0.00016542636149097234, 0.00016542636149097234, 0.9005811214447021, 0.007940465584397316, 0.024648526683449745, 0.0004962790990248322, 0.0009925581980496645, 0.045657675713300705, 0.00016542636149097234, 0.0008271317929029465, 0.016873488202691078, 0.0009925581980496645, 0.00016542636149097234, 0.9955921173095703, 0.0021991620305925608, 0.00031416601268574595, 0.00031416601268574595, 0.00031416601268574595, 0.00031416601268574595, 0.00031416601268574595, 0.04584772512316704, 0.029090626165270805, 0.11028946191072464, 0.24946336448192596, 0.05334531143307686, 0.053813908249139786, 0.01866898499429226, 0.05992443859577179, 0.0562131367623806, 0.0362883061170578, 0.056831687688827515, 0.029990335926413536, 0.14942684769630432, 0.02313004620373249, 0.027684828266501427, 0.0014557259855791926, 0.00036393149639479816, 0.9822511672973633, 0.00036393149639479816, 0.010917944833636284, 0.0010917945764958858, 0.0018196575110778213, 0.00036393149639479816, 0.00036393149639479816, 0.00036393149639479816, 0.9793700575828552, 0.01691810041666031, 0.028523718938231468, 0.03040171228349209, 0.0859794169664383, 0.06344350427389145, 0.1726120561361313, 0.041098106652498245, 0.06227315962314606, 0.055768225342035294, 0.021937135607004166, 0.12519952654838562, 0.01856219209730625, 0.08832010626792908, 0.11131871491670609, 0.05658474564552307, 0.037968117743730545, 0.00013345267507247627, 0.00013345267507247627, 0.03549841046333313, 0.00026690535014495254, 0.7310537695884705, 0.00013345267507247627, 0.0004003580252174288, 0.0005338107002899051, 0.00013345267507247627, 0.0021352428011596203, 0.22940514981746674, 0.00013345267507247627, 0.00026690535014495254, 0.000323574902722612, 0.001294299610890448, 0.000323574902722612, 0.9561638236045837, 0.004206473473459482, 0.0016178744845092297, 0.020385218784213066, 0.004853623453527689, 0.0009707246790640056, 0.002588599221780896, 0.0019414493581280112, 0.005500773433595896, 0.0018583397613838315, 0.9923533797264099, 0.0018583397613838315, 0.9938970804214478, 0.0013712717918679118, 0.0013712717918679118, 0.0013712717918679118, 0.0013712717918679118, 0.985944390296936, 0.0013712717918679118, 0.0027425435837358236, 0.005485087167471647, 0.0015143945347517729, 0.0015143945347517729, 0.9676980972290039, 0.024230312556028366, 0.0015143945347517729, 0.0015143945347517729, 0.0010040417546406388, 0.0002868690644390881, 9.562302147969604e-05, 0.00019124604295939207, 3.187434049323201e-05, 0.9312885403633118, 0.00017530887271277606, 0.06261714547872543, 0.0007012354908511043, 7.968585123308003e-05, 0.00019124604295939207, 0.0020080835092812777, 0.00044624076690524817, 0.0002709318941924721, 0.0006215496687218547, 0.9966591596603394, 0.0009185798699036241, 0.0008436269708909094, 0.9971670508384705, 0.0008436269708909094, 0.007887628860771656, 0.0022536083124578, 0.00056340207811445, 0.988207221031189, 0.00056340207811445, 0.0006294011836871505, 0.0010070418938994408, 0.0007552814204245806, 0.0002517604734748602, 0.0001258802367374301, 0.9441018104553223, 0.0002517604734748602, 0.03373590484261513, 0.0001258802367374301, 0.0001258802367374301, 0.0010070418938994408, 0.0001258802367374301, 0.0001258802367374301, 0.0006294011836871505, 0.017119713127613068, 0.0006053705583326519, 0.0015134263085201383, 0.05508871749043465, 0.0003026852791663259, 0.09020020812749863, 0.0003026852791663259, 0.0006053705583326519, 0.0015134263085201383, 0.0006053705583326519, 0.8326871991157532, 0.014831578359007835, 0.0012107411166653037, 0.0006053705583326519, 0.0003026852791663259, 0.0003026852791663259, 0.0002354303578613326, 0.0002354303578613326, 0.8556716442108154, 0.0440254770219326, 0.014478967525064945, 0.0001177151789306663, 0.0049440376460552216, 0.0024720188230276108, 0.06662679463624954, 0.0001177151789306663, 0.0021188731770962477, 0.003884600941091776, 0.004355461802333593, 0.0004708607157226652, 0.0001177151789306663, 0.9892241358757019, 0.003275576513260603, 0.0004903925582766533, 0.005884710233658552, 0.0004903925582766533, 0.001471177558414638, 0.9901025295257568, 0.0004903925582766533, 0.006852755788713694, 0.989081084728241, 0.9919514656066895, 0.0023674259427934885, 0.9911008477210999, 0.00230488576926291, 0.00014970573829486966, 0.000449117214884609, 0.9115582704544067, 0.00026198505656793714, 0.0010853665880858898, 0.0005613965331576765, 0.06796640902757645, 0.0006736758514307439, 0.00033683792571537197, 0.00014970573829486966, 0.0008608080097474158, 0.003181247040629387, 0.012500429525971413, 0.00011227930372115225, 0.00011227930372115225, 0.00010713861411204562, 0.00032141583506017923, 0.9850323796272278, 0.00010713861411204562, 0.001285663340240717, 0.0013928019907325506, 0.00042855445644818246, 0.007606841623783112, 0.00010713861411204562, 0.00021427722822409123, 0.0016070791753008962, 0.0008571089128963649, 0.00032141583506017923, 0.00042855445644818246, 0.0001295282127102837, 6.476410635514185e-05, 0.9899193048477173, 0.0001295282127102837, 0.005569712724536657, 0.0001295282127102837, 0.0007771692471578717, 6.476410635514185e-05, 6.476410635514185e-05, 6.476410635514185e-05, 0.0007124051335267723, 0.0020076872315257788, 0.0001295282127102837, 0.0002590564254205674, 6.476410635514185e-05, 0.0018773652845993638, 0.0008238227455876768, 0.0002534839150030166, 0.0009109578095376492, 0.00040399000863544643, 0.0020516354124993086, 0.009157106280326843, 0.0002297198079759255, 0.002954671857878566, 0.016460612416267395, 0.10030041635036469, 0.00029309079400263727, 0.00011089921463280916, 0.6441818475723267, 0.21999235451221466, 0.0012551365653052926, 0.00015689207066316158, 0.00031378414132632315, 0.9611207842826843, 0.0006275682826526463, 0.00031378414132632315, 0.00015689207066316158, 0.0004706761974375695, 0.034830037504434586, 0.0006275682826526463, 0.9950573444366455, 0.9956225752830505, 0.0003041899180971086, 0.9799478054046631, 0.0001520949590485543, 0.017186731100082397, 0.0003041899180971086, 0.0003041899180971086, 0.0003041899180971086, 0.0003041899180971086, 0.0003041899180971086, 0.000760474824346602, 0.0001520949590485543, 0.0001162062690127641, 0.0006972376140765846, 0.9009472131729126, 0.0011620627483353019, 0.00976132694631815, 0.0004648250760510564, 0.013944752514362335, 0.0002324125380255282, 0.0002324125380255282, 0.016849908977746964, 0.0450880341231823, 0.007785819936543703, 0.0003486188070382923, 0.0022079190239310265, 0.0001162062690127641, 0.1894291639328003, 0.04313010349869728, 0.02326323091983795, 0.021291151642799377, 0.002483359072357416, 0.22817686200141907, 0.08059960603713989, 0.21072031557559967, 0.08238908648490906, 0.0023372790310531855, 0.007632676977664232, 0.02176591195166111, 0.016835713759064674, 0.00891087669879198, 0.06102489307522774, 0.28817200660705566, 0.001188688213005662, 0.0001698126143310219, 0.0013585009146481752, 0.006962317042052746, 0.11869901418685913, 0.0003396252286620438, 0.004075502511113882, 0.0005094378138892353, 0.0001698126143310219, 0.0006792504573240876, 0.0005094378138892353, 0.5744760632514954, 0.0027170018292963505, 0.004048807546496391, 0.00015877677651587874, 0.9173328280448914, 0.0008732722490094602, 0.0030167587101459503, 0.0002381651574978605, 0.000635107106063515, 0.00015877677651587874, 0.00817700382322073, 0.00015877677651587874, 0.011590704321861267, 0.0061922939494252205, 0.04675975814461708, 7.938838825793937e-05, 0.0005557187250815332, 0.9926148653030396, 0.001135714934207499, 0.00037857165443710983, 0.00037857165443710983, 0.00037857165443710983, 0.004542859736829996, 0.8676005601882935, 0.00040963198989629745, 0.00020481599494814873, 0.00020481599494814873, 0.12903407216072083, 0.00020481599494814873, 0.00020481599494814873, 0.00040963198989629745, 0.00020481599494814873, 0.00020481599494814873, 0.00040963198989629745, 0.00020481599494814873, 0.0006144479848444462, 0.9933274388313293, 0.0039005004800856113, 0.03192366659641266, 0.01734321005642414, 0.06691411882638931, 0.013152815401554108, 0.003463354427367449, 0.27364465594291687, 0.021837638691067696, 0.44106215238571167, 0.03915441036224365, 0.0012293586041778326, 0.00576344458386302, 0.003899578470736742, 0.002101806690916419, 0.06934639811515808, 0.009147485718131065, 0.0001704836031422019, 0.0005114508094266057, 0.0011933852219954133, 0.0001704836031422019, 0.0003409672062844038, 0.0001704836031422019, 0.0001704836031422019, 0.9949423670768738, 0.0011933852219954133, 0.0003409672062844038, 0.0001704836031422019, 0.0005114508094266057, 0.0001704836031422019, 0.9926604628562927, 0.005460251122713089, 0.0009707113495096564, 0.00012133891868870705, 0.0038828453980386257, 0.0002426778373774141, 0.007523012813180685, 0.0013347280910238624, 0.00036401674151420593, 0.0004853556747548282, 0.0014560669660568237, 0.0021841004490852356, 0.0002426778373774141, 0.709954023361206, 0.2656109035015106, 0.9938954710960388, 0.962792158126831, 0.017505312338471413, 0.017505312338471413, 0.0014208069769665599, 0.9903024435043335, 0.0028416139539331198, 0.0014208069769665599, 0.00014462298713624477, 0.00028924597427248955, 0.00014462298713624477, 0.00014462298713624477, 0.00014462298713624477, 0.9939938187599182, 0.0031817059498280287, 0.00014462298713624477, 0.00028924597427248955, 0.00028924597427248955, 0.0005784919485449791, 0.0008677379810251296, 0.00025023656780831516, 0.0010009462712332606, 0.00025023656780831516, 0.0005004731356166303, 0.0005004731356166303, 0.0005838853539898992, 0.8991000056266785, 0.02735919877886772, 0.00033364875707775354, 0.00041706094634719193, 0.008007570169866085, 8.341218926943839e-05, 0.00033364875707775354, 8.341218926943839e-05, 0.06114113703370094, 0.0018663443624973297, 0.0006221148069016635, 0.0006221148069016635, 0.03297208249568939, 0.00435480335727334, 0.01057595107704401, 0.001244229613803327, 0.003110573859885335, 0.007465377449989319, 0.0006221148069016635, 0.9151308536529541, 0.001244229613803327, 0.016797099262475967, 0.002488459227606654, 0.0019248737953603268, 0.0004812184488400817, 0.0019248737953603268, 0.08276957273483276, 0.004330966155976057, 0.00336852902546525, 0.00336852902546525, 0.0038497475907206535, 0.0009624368976801634, 0.8926602005958557, 0.0004812184488400817, 0.0038497475907206535, 0.9034810066223145, 0.0030967644415795803, 0.0023225732147693634, 0.0007741911103948951, 0.009290292859077454, 0.0007741911103948951, 0.0007741911103948951, 0.0007741911103948951, 0.0007741911103948951, 0.078193299472332, 0.6002049446105957, 0.0014077144442126155, 0.13491831719875336, 0.028722312301397324, 0.004470110405236483, 0.005285103339701891, 0.049912117421627045, 0.0012101404136046767, 0.04440474510192871, 0.03921842947602272, 0.028302468359470367, 0.029463214799761772, 0.01736181043088436, 0.0008643860346637666, 0.01427471823990345, 0.9964507222175598, 0.008685529232025146, 0.09409323334693909, 0.005790352821350098, 0.8852002024650574, 0.0014475882053375244, 0.002895176410675049, 0.07338070124387741, 0.01270215678960085, 0.008754189126193523, 0.014161188155412674, 0.04703231155872345, 0.09071743488311768, 0.029352281242609024, 0.10393454134464264, 0.04445754736661911, 0.3578060269355774, 0.015877695754170418, 0.023344503715634346, 0.11174464970827103, 0.029523931443691254, 0.037248216569423676, 0.008453654125332832, 0.030346449464559555, 0.000650281086564064, 0.22586429119110107, 0.0004335207340773195, 0.000650281086564064, 0.00021676036703865975, 0.0060692899860441685, 0.7261472344398499, 0.00021676036703865975, 0.0004335207340773195, 0.00021676036703865975, 0.00021676036703865975, 0.5314420461654663, 0.006021489389240742, 0.01587483659386635, 0.017790764570236206, 0.00027370406314730644, 9.123468771576881e-05, 0.14378586411476135, 9.123468771576881e-05, 0.22936400771141052, 0.00027370406314730644, 0.038592275232076645, 0.014141377061605453, 0.0016422243788838387, 0.00018246937543153763, 0.0005474081262946129, 0.00025601021479815245, 0.00010240408300887793, 0.00040961633203551173, 0.00010240408300887793, 0.9604479074478149, 0.0009216367616318166, 0.00030721226357854903, 0.0006656265468336642, 5.1202041504438967e-05, 0.0007168286247178912, 0.00015360613178927451, 5.1202041504438967e-05, 5.1202041504438967e-05, 0.0006144245271570981, 0.03517580404877663, 0.00011456041829660535, 0.00011456041829660535, 0.00011456041829660535, 0.9967901706695557, 0.0004582416731864214, 0.00034368125488981605, 0.00034368125488981605, 0.00011456041829660535, 0.0002291208365932107, 0.0002291208365932107, 0.00011456041829660535, 0.0009164833463728428, 0.0002291208365932107, 0.9950458407402039, 0.9916520118713379, 0.0009982238989323378, 0.0009982238989323378, 0.982252299785614, 0.003992895595729351, 0.008984014391899109, 0.0010267358738929033, 0.005314867943525314, 0.011958452872931957, 0.0133475661277771, 0.8636659979820251, 0.0033821885008364916, 0.017998075112700462, 0.0005435660132206976, 0.04849816858768463, 0.004469320643693209, 0.002295056590810418, 0.0036237735766917467, 0.006039622705429792, 0.012200037948787212, 0.005798037629574537, 0.9933615922927856, 0.9937905669212341, 0.0005729760159738362, 0.9935404658317566, 0.0011459520319476724, 0.0005729760159738362, 0.0011459520319476724, 0.0011459520319476724, 0.0011459520319476724, 0.0005729760159738362, 0.9888414144515991, 0.9944366812705994, 0.016839908435940742, 9.96444359770976e-05, 9.96444359770976e-05, 0.9750207662582397, 0.0001992888719541952, 9.96444359770976e-05, 9.96444359770976e-05, 0.0019928887486457825, 0.002590755233541131, 9.96444359770976e-05, 0.002291821874678135, 9.96444359770976e-05, 0.0003985777439083904, 0.0013124077813699841, 0.9974299073219299, 0.991997480392456, 0.9923338890075684, 0.989346444606781, 0.001494634780101478, 0.968523383140564, 0.001494634780101478, 0.02391415648162365, 0.9957915544509888, 0.997468113899231, 0.00023634181707166135, 7.878060569055378e-05, 0.0023634182289242744, 7.878060569055378e-05, 0.00023634181707166135, 7.878060569055378e-05, 7.878060569055378e-05, 7.878060569055378e-05, 0.00015756121138110757, 0.0003939030284527689, 0.0007878060569055378, 0.9880663752555847, 0.007169035263359547, 7.878060569055378e-05, 0.00032728147925809026, 0.20978742837905884, 0.00032728147925809026, 0.0006545629585161805, 0.0006545629585161805, 0.00032728147925809026, 0.00032728147925809026, 0.0006545629585161805, 0.7684568762779236, 0.01800048165023327, 0.00032728147925809026, 0.00027283525560051203, 0.9876636862754822, 0.006548046600073576, 0.000818505825009197, 0.0010913410224020481, 0.00027283525560051203, 0.00027283525560051203, 0.00027283525560051203, 0.00027283525560051203, 0.00027283525560051203, 0.0005456705112010241, 0.0013641762780025601, 0.00027283525560051203, 0.07953394949436188, 0.03370082378387451, 0.662305474281311, 0.008088198490440845, 0.06849692761898041, 0.0007582685793749988, 0.009688987396657467, 0.0005897644441574812, 0.0030330743174999952, 0.061166997998952866, 0.04389532655477524, 0.01103702001273632, 0.00758268591016531, 0.0002527562028262764, 0.009773239493370056, 0.3785015642642975, 0.00038633120129816234, 0.00018028789781965315, 0.0021376993972808123, 2.575541293481365e-05, 7.726623880444095e-05, 0.616533100605011, 0.000592374533880502, 0.0003605757956393063, 0.0003090649552177638, 0.0001030216517392546, 0.00023179872368928045, 0.00018028789781965315, 0.00018028789781965315, 0.0001545324776088819, 0.0018247978296130896, 0.0018247978296130896, 0.9945148229598999, 0.997925341129303, 0.03326535224914551, 0.03218498453497887, 0.1405516415834427, 0.07020124047994614, 0.06804502755403519, 0.08560211211442947, 0.06601086258888245, 0.08059807121753693, 0.08750518411397934, 0.03979727625846863, 0.0657893642783165, 0.05320919305086136, 0.03563854098320007, 0.04399669170379639, 0.0976036787033081, 0.08615162968635559, 0.029315680265426636, 0.005835082847625017, 0.008948996663093567, 0.048027217388153076, 0.24375496804714203, 0.09136953949928284, 0.3495999872684479, 0.0007574385963380337, 0.004684898070991039, 0.034729961305856705, 0.005330123472958803, 0.014082747511565685, 0.04107000306248665, 0.036357052624225616, 0.9935845732688904, 0.00033185858046635985, 0.0013274343218654394, 0.00033185858046635985, 0.00033185858046635985, 0.002654868643730879, 0.00033185858046635985, 0.00033185858046635985, 0.0003874018439091742, 0.010847251862287521, 0.0003874018439091742, 0.005423625931143761, 0.0007748036878183484, 0.0007748036878183484, 0.0011622054735198617, 0.003486616536974907, 0.825940728187561, 0.003486616536974907, 0.1472126990556717, 0.0003874018439091742, 0.00014708840171806514, 5.883535777684301e-05, 0.00032359446049667895, 0.0009413657244294882, 0.001323795528151095, 0.28935229778289795, 0.0002941768034361303, 0.014149903319776058, 0.0030594386626034975, 0.0003824298328254372, 0.0017062254482880235, 0.00011767071555368602, 0.0019415668211877346, 0.08657623082399368, 0.5995911359786987, 0.996715247631073, 0.00029241107404232025, 0.00029241107404232025, 0.0005848221480846405, 0.00029241107404232025, 0.0005848221480846405, 0.0014620553702116013, 0.00029241107404232025, 0.0005848221480846405, 0.9953673481941223, 0.014639242552220821, 0.9827811121940613, 0.01092175766825676, 0.0009928870713338256, 0.0009928870713338256, 0.9859368801116943, 0.0022560828365385532, 0.9949325323104858, 0.9951076507568359, 0.00029970004106871784, 0.00029970004106871784, 0.00014985002053435892, 0.00014985002053435892, 0.00014985002053435892, 0.9761230945587158, 0.0055444510653615, 0.011838152073323727, 0.000449550076154992, 0.000449550076154992, 0.00014985002053435892, 0.002847150433808565, 0.0014985002344474196, 0.98261958360672, 0.005369505845010281, 0.009665110148489475, 0.0010739011922851205, 0.002535515697672963, 0.028735844418406487, 0.0008451718604192138, 0.9651862978935242, 0.0008451718604192138, 0.04049261659383774, 0.025052251294255257, 0.002607479225844145, 0.0035277658607810736, 0.053555577993392944, 0.35576751828193665, 0.08287693560123444, 0.33577683568000793, 0.001099231420084834, 0.0028119871858507395, 0.03238898143172264, 0.006211935542523861, 0.009151740930974483, 0.014775714837014675, 0.03389722853899002, 0.9881381988525391, 0.9929189682006836, 0.00032384833320975304, 0.0035623316653072834, 0.0006476966664195061, 0.0006476966664195061, 0.0012953933328390121, 0.00029142829589545727, 0.00014571414794772863, 0.034825682640075684, 0.01602855697274208, 0.00029142829589545727, 0.00014571414794772863, 0.0004371424438431859, 0.00014571414794772863, 0.09587991237640381, 0.00014571414794772863, 0.00029142829589545727, 0.09602562338113785, 0.7545078992843628, 0.0013114273315295577, 0.990132749080658, 0.0006700757658109069, 0.00022335859830491245, 0.00022335859830491245, 0.9809909462928772, 0.0020102274138480425, 0.0004467171966098249, 0.011167929507791996, 0.00022335859830491245, 0.00022335859830491245, 0.0006700757658109069, 0.00022335859830491245, 0.0029036616906523705, 0.00022335859830491245, 0.992825984954834, 0.004419106524437666, 0.001473035546950996, 0.9915484189987183, 0.06891504675149918, 0.019371263682842255, 0.09710986912250519, 0.41333815455436707, 0.003549708053469658, 0.0028397664427757263, 0.0034989980049431324, 0.01049699354916811, 0.07074061036109924, 0.010953384451568127, 0.12500043213367462, 0.018154220655560493, 0.14771856367588043, 0.00674444530159235, 0.0015720135997980833, 0.9817585349082947, 0.007227670401334763, 0.007227670401334763, 0.0012046117335557938, 0.018264289945364, 0.001712277065962553, 0.0008561385329812765, 0.00114151812158525, 0.11729098111391068, 0.0002853795303963125, 0.0014268975937739015, 0.8567093014717102, 0.0002853795303963125, 0.001712277065962553, 0.9863609671592712, 0.0005901710246689618, 0.0005901710246689618, 0.0005901710246689618, 0.0005901710246689618, 0.0005901710246689618, 0.0005901710246689618, 0.0011803420493379235, 0.0011803420493379235, 0.0005901710246689618, 0.9655197858810425, 0.027738038450479507, 0.9958209991455078, 0.001110168406739831, 0.0009607574320398271, 0.9972662329673767, 0.9973782896995544, 0.03474035486578941, 0.003330230014398694, 0.00037843521567992866, 0.0002270611294079572, 0.5331395268440247, 0.18777956068515778, 0.021949242800474167, 0.08098513633012772, 0.0009082445176318288, 0.10073945671319962, 0.001362366834655404, 0.0008325575035996735, 0.0005298093310557306, 0.00264904648065567, 0.03042619116604328, 0.9934008717536926, 0.0009155481238849461, 0.015962382778525352, 0.2181791067123413, 0.06512333452701569, 0.3926507234573364, 0.00023883864923845977, 0.002826257376000285, 0.00011941932461922988, 0.008478771895170212, 0.04924056679010391, 0.12272325903177261, 0.11902125924825668, 0.0041000633500516415, 0.00019903220527339727, 0.00019903220527339727, 0.018741009756922722, 0.0336456261575222, 0.11407763510942459, 0.2685917615890503, 0.029280073940753937, 0.06482184678316116, 0.02526729181408882, 0.04140660911798477, 0.16029076278209686, 0.03448345884680748, 0.028133563697338104, 0.09176480770111084, 0.07791850715875626, 0.009568939916789532, 0.00198434223420918, 0.07104828208684921, 0.32330024242401123, 0.014720715582370758, 0.009499028325080872, 0.0721592828631401, 0.09071294218301773, 0.0010832224506884813, 0.0132764196023345, 0.0031385677866637707, 0.0009998977184295654, 0.12995892763137817, 0.008526905439794064, 0.0337187722325325, 0.011165523901581764, 0.21670004725456238, 0.9959030747413635, 0.9939790964126587, 0.00021020662097726017, 0.00010510331048863009, 0.9936992526054382, 0.00010510331048863009, 0.0022071695420891047, 0.00015765496937092394, 0.000788274803198874, 5.255165524431504e-05, 5.255165524431504e-05, 5.255165524431504e-05, 0.0007357231806963682, 0.001366342999972403, 0.00021020662097726017, 0.00021020662097726017, 0.00010510331048863009, 0.08944813907146454, 0.9048922657966614, 0.0002124658931279555, 0.002549590775743127, 0.0006373976939357817, 0.000424931786255911, 0.000424931786255911, 0.0002124658931279555, 0.000424931786255911, 0.0006373976939357817, 0.0002124658931279555, 0.0002124658931279555, 0.9978897571563721, 0.0017354029696434736, 0.03937336429953575, 0.00022537700715474784, 0.0001352262042928487, 0.07912986725568771, 0.6879182457923889, 0.00018030160572379827, 0.009353145956993103, 4.507540143094957e-05, 9.015080286189914e-05, 0.0009015080286189914, 9.015080286189914e-05, 6.761310214642435e-05, 0.16783826053142548, 0.012891565449535847, 0.003708107164129615, 0.0006180178606882691, 0.0010594591731205583, 0.0016774771502241492, 8.828826685203239e-05, 0.8520700931549072, 8.828826685203239e-05, 8.828826685203239e-05, 8.828826685203239e-05, 0.1401134878396988, 8.828826685203239e-05, 0.00017657653370406479, 0.0024432071950286627, 0.014414922334253788, 0.0007329621585085988, 0.000977282878011465, 0.0017102450365200639, 0.00024432071950286627, 0.00024432071950286627, 0.9337937831878662, 0.00024432071950286627, 0.0007329621585085988, 0.044466372579336166, 0.00024432071950286627, 0.004367238376289606, 0.0014557461254298687, 0.9928188323974609, 0.002865791553631425, 0.0014328957768157125, 0.9944297075271606, 0.06359994411468506, 0.0362662598490715, 0.18651218712329865, 0.01983032003045082, 0.015542683191597462, 0.00035730304080061615, 0.0010719092097133398, 0.00017865152040030807, 0.04627074673771858, 0.0030370759777724743, 0.0008932576165534556, 0.06038421764969826, 0.5650748014450073, 0.00035730304080061615, 0.00035730304080061615, 0.6303659081459045, 0.004111975897103548, 0.1753757745027542, 0.001233592745848, 0.004728772211819887, 0.00020559878612402827, 0.15913346409797668, 0.00041119757224805653, 0.002261586720123887, 0.000616796372924, 0.001027993974275887, 0.012335927225649357, 0.000616796372924, 0.007195957936346531, 0.00020559878612402827, 0.9958504438400269, 0.07907295972108841, 0.04436296597123146, 0.0880303755402565, 0.029213394969701767, 0.013436125591397285, 0.13558273017406464, 0.0896081030368805, 0.1950443685054779, 0.06952176988124847, 0.03196169435977936, 0.015624585561454296, 0.05905448645353317, 0.04417635500431061, 0.03280993551015854, 0.0725245401263237, 8.04926676210016e-05, 0.0002414780028630048, 0.003219706704840064, 8.04926676210016e-05, 0.000724433979485184, 8.04926676210016e-05, 0.0004829560057260096, 0.000402463338105008, 8.04926676210016e-05, 0.0001609853352420032, 0.9939234256744385, 8.04926676210016e-05, 0.000402463338105008, 0.0001609853352420032, 0.000940288242418319, 0.0003134294238407165, 0.001880576484836638, 0.000626858847681433, 0.0003134294238407165, 0.0003134294238407165, 0.0003134294238407165, 0.0003134294238407165, 0.000626858847681433, 0.0003134294238407165, 0.9935712814331055, 0.0001552898611407727, 0.00011646739585557953, 0.0001941123337019235, 0.004813985899090767, 7.764493057038635e-05, 0.0002717572497203946, 0.01649954728782177, 0.0004658695834223181, 0.9719980955123901, 0.0005046920850872993, 0.0002717572497203946, 0.0006988043896853924, 0.003066974924877286, 7.764493057038635e-05, 0.000776449334807694, 4.746027843793854e-05, 4.746027843793854e-05, 9.492055687587708e-05, 0.000996665796265006, 7.1190414018929e-05, 0.00018984111375175416, 0.00018984111375175416, 0.0001661109708948061, 0.996452271938324, 0.00018984111375175416, 7.1190414018929e-05, 0.00042714248411357403, 0.0007356342975981534, 7.1190414018929e-05, 0.00026103152777068317, 0.073102205991745, 0.013293822295963764, 0.04651455953717232, 0.13733264803886414, 0.15590530633926392, 0.005085370969027281, 0.03847193717956543, 0.3051498830318451, 0.02600724995136261, 0.044939201325178146, 0.018158091232180595, 0.08982312679290771, 0.04239651560783386, 0.0020728413946926594, 0.0017688246443867683, 0.993479311466217, 0.001024205470457673, 0.0025605137925595045, 0.0005121027352288365, 0.001024205470457673, 0.9943015575408936, 0.9952986836433411, 0.0007586117717437446, 0.0007586117717437446, 0.0007586117717437446, 0.0007586117717437446, 0.029045293107628822, 0.01729709282517433, 0.034492556005716324, 0.004430982284247875, 0.014532809145748615, 0.20371001958847046, 0.03947233036160469, 0.43482571840286255, 0.03299523517489433, 0.0020732120610773563, 0.03374728560447693, 0.015196779742836952, 0.09954127669334412, 0.0005352409789338708, 0.03810374066233635, 0.0036781043745577335, 0.9953870177268982, 0.0004597630468197167, 0.000601956679020077, 0.009631306864321232, 0.000601956679020077, 0.000601956679020077, 0.000601956679020077, 0.000601956679020077, 0.002407826716080308, 0.021068483591079712, 0.9637326002120972, 0.000601956679020077, 0.003068318823352456, 0.016875753179192543, 0.9757253527641296, 0.001534159411676228, 0.9902957081794739, 0.0013116499176248908, 0.006558249704539776, 0.0003273564507253468, 0.013748970814049244, 0.0003273564507253468, 0.0009820692939683795, 0.0003273564507253468, 0.9837061166763306, 0.9938716888427734, 0.9958866238594055, 0.0002171169762732461, 0.015632422640919685, 0.0160666573792696, 0.007164860609918833, 0.0034738716203719378, 0.0006513509433716536, 0.0010855848668143153, 0.0019540528301149607, 0.04516033083200455, 0.004342339467257261, 0.8953904509544373, 0.001519818906672299, 0.003039637813344598, 0.004125222563743591, 0.024938518181443214, 0.044308241456747055, 0.001785646309144795, 0.003995005507022142, 0.001513259601779282, 0.25746598839759827, 0.5376611351966858, 0.07505767792463303, 0.004176596645265818, 0.0009382209391333163, 0.0007868949905969203, 0.003298905910924077, 0.0013014032738283277, 0.021004043519496918, 0.021851468831300735, 0.0009858607081696391, 0.0009858607081696391, 0.0029575820080935955, 0.9907900094985962, 0.0029575820080935955, 0.013204461894929409, 0.19095683097839355, 0.0003385759482625872, 0.0006771518965251744, 0.006094366777688265, 0.003385759424418211, 0.001015727873891592, 0.0006771518965251744, 0.7221825122833252, 0.059927940368652344, 0.001015727873891592, 0.000849907286465168, 0.00012141533079557121, 0.0003642459923867136, 0.00012141533079557121, 0.009106149896979332, 0.0030353832989931107, 0.00024283066159114242, 0.001699814572930336, 0.00024283066159114242, 0.00012141533079557121, 0.0021854760125279427, 0.00012141533079557121, 0.00012141533079557121, 0.9809144139289856, 0.0009713226463645697, 0.0020312690176069736, 0.0006770896725356579, 0.00033854483626782894, 0.004062538035213947, 0.0006770896725356579, 0.00033854483626782894, 0.00033854483626782894, 0.00033854483626782894, 0.01252615824341774, 0.9777174592018127, 0.0010156345088034868, 0.0005736927269026637, 0.0005736927269026637, 0.0005736927269026637, 0.0005736927269026637, 0.9970779418945312, 0.007845892570912838, 0.00043588291737250984, 0.00043588291737250984, 0.00043588291737250984, 0.9898900985717773, 0.00373466731980443, 0.0014938669046387076, 0.9926745295524597, 0.0007469334523193538, 0.0023395740427076817, 0.9919793605804443, 0.001354404492303729, 0.0006019575521349907, 0.12942087650299072, 0.0030097877606749535, 0.0016553832683712244, 0.000451468164101243, 0.001805872656404972, 0.00030097877606749535, 0.04529730603098869, 0.8028608560562134, 0.0016553832683712244, 0.008577895350754261, 0.0021068514324724674, 0.000451468164101243, 0.000451468164101243, 0.991897463798523, 0.0005542762228287756, 0.9960343241691589, 0.0011085524456575513, 0.0011085524456575513, 0.997479259967804, 0.0011478472733870149, 0.0003826157480943948, 0.0003826157480943948, 0.9949614405632019, 0.0709141194820404, 0.0011918339878320694, 0.007746920920908451, 0.8777857422828674, 0.0005959169939160347, 0.0011918339878320694, 0.0005959169939160347, 0.0011918339878320694, 0.001787750981748104, 0.0005959169939160347, 0.001787750981748104, 0.0011918339878320694, 0.032179515808820724, 0.0005959169939160347, 0.0005959169939160347, 0.9950010180473328, 0.00023675702686887234, 0.0006313520716503263, 0.9606021642684937, 0.0014205421321094036, 0.02596435323357582, 0.00023675702686887234, 0.0016572991153225303, 0.00015783801791258156, 0.0005524330772459507, 0.0003156760358251631, 0.0004735140537377447, 0.006471358705312014, 0.0003945950302295387, 0.0007102710660547018, 0.00015783801791258156, 0.9965064525604248, 0.002256178529933095, 0.002256178529933095, 0.9927185773849487, 0.04955258220434189, 0.02562914974987507, 0.11783916503190994, 0.22063040733337402, 0.034996148198843, 0.06546780467033386, 0.05630318075418472, 0.06113123521208763, 0.03856659680604935, 0.03307360038161278, 0.07575993984937668, 0.027132496237754822, 0.10179384052753448, 0.026987943798303604, 0.06513533741235733, 0.04114457592368126, 0.025907834991812706, 0.15902790427207947, 0.12235946208238602, 0.052012618631124496, 0.08902125060558319, 0.060445643961429596, 0.10851927101612091, 0.060230791568756104, 0.03050929494202137, 0.05002521723508835, 0.021503323689103127, 0.09833160042762756, 0.025299079716205597, 0.05562933161854744, 0.00017517060041427612, 0.9909400343894958, 0.0007006824016571045, 0.005079947412014008, 0.00017517060041427612, 0.00017517060041427612, 0.00017517060041427612, 0.0005255118012428284, 0.0010510236024856567, 0.0008758530020713806, 0.00017517060041427612, 0.057644303888082504, 0.0019153961911797523, 0.0008208840736187994, 0.01463909912854433, 0.004423653241246939, 0.4365735352039337, 0.060334980487823486, 0.19623690843582153, 0.01851549744606018, 0.0008208840736187994, 0.003511559683829546, 0.070185586810112, 0.08870108425617218, 0.007068723905831575, 0.03858155384659767, 0.0020551499910652637, 0.007004287093877792, 0.5455374717712402, 0.1356818526983261, 0.03556668013334274, 0.0015937899006530643, 0.014511875808238983, 0.001048545935191214, 0.10649032890796661, 0.043871164321899414, 0.046219903975725174, 0.025416754186153412, 0.029820647090673447, 0.003313405206426978, 0.0018454408273100853, 0.9865167737007141, 0.005573540925979614, 0.005573540925979614, 0.9922975897789001, 0.0021587328519672155, 0.0007195776561275125, 0.0007195776561275125, 0.0007195776561275125, 0.0007195776561275125, 0.001439155312255025, 0.015976347029209137, 0.9798826575279236, 0.9953402280807495, 0.000910879869479686, 0.9965025782585144, 0.0015484513714909554, 0.0005161504377610981, 0.0015484513714909554, 0.9956542253494263, 0.00027948393835686147, 0.0011179357534274459, 0.00027948393835686147, 0.0005589678767137229, 0.003912775311619043, 0.00027948393835686147, 0.00027948393835686147, 0.9930064082145691, 0.0007517105550505221, 0.9960164427757263, 0.0015034211101010442, 0.0007517105550505221, 0.0022450077813118696, 0.00014031298633199185, 0.00014031298633199185, 0.0007015649462118745, 0.00014031298633199185, 0.9090878963470459, 0.0002806259726639837, 0.08531029522418976, 0.0004209389735478908, 0.0005612519453279674, 0.00014031298633199185, 0.00014031298633199185, 0.0002806259726639837, 0.0002806259726639837, 0.056253064423799515, 0.011991554871201515, 0.09632240980863571, 0.5685752034187317, 0.006044523324817419, 0.009846723638474941, 0.009944216348230839, 0.020473385229706764, 0.04923361912369728, 0.009310516528785229, 0.03563344106078148, 0.007019446697086096, 0.10022210329771042, 0.00989546999335289, 0.00926177017390728, 0.9865217208862305, 0.005573568865656853, 0.005573568865656853, 0.9943755269050598, 0.9943128228187561, 0.0032600420527160168, 0.00023622730805072933, 7.874243601690978e-05, 0.002441015560179949, 0.0003149697440676391, 7.874243601690978e-05, 0.00015748487203381956, 0.9863671064376831, 0.00511825829744339, 0.00015748487203381956, 0.0010236516827717423, 0.0016535911709070206, 0.0009449092322029173, 0.00019685608276631683, 3.937121800845489e-05, 0.0012205077800899744, 0.0006075565470382571, 0.003037782618775964, 0.0072906785644590855, 0.0024302261881530285, 0.0012151130940765142, 0.0072906785644590855, 0.9714828729629517, 0.005468008574098349, 0.9957274198532104, 0.04170497506856918, 0.027057673782110214, 0.07348165661096573, 0.2937121093273163, 0.01832444965839386, 0.06484036147594452, 0.0589875653386116, 0.07198014855384827, 0.05485077574849129, 0.06468714028596878, 0.03468775376677513, 0.028957532718777657, 0.039897043257951736, 0.04335969313979149, 0.08350188285112381, 0.00019541883375495672, 6.513961125165224e-05, 6.513961125165224e-05, 0.9701893925666809, 0.00019541883375495672, 0.00013027922250330448, 0.00013027922250330448, 0.00013027922250330448, 6.513961125165224e-05, 0.00045597730786539614, 0.00013027922250330448, 0.028010033071041107, 6.513961125165224e-05, 0.00019541883375495672, 0.9910156726837158, 0.0003312718472443521, 0.965326189994812, 0.0003312718472443521, 0.0003312718472443521, 0.0006625436944887042, 0.03279591351747513, 0.991065263748169, 0.0011681951582431793, 0.0011681951582431793, 0.0011681951582431793, 0.9953022599220276, 0.001715275226160884, 0.9931443333625793, 0.001715275226160884, 0.010141492821276188, 0.0001913489104481414, 0.0001913489104481414, 0.0001913489104481414, 0.22368688881397247, 0.011480934917926788, 0.004209676291793585, 0.09031669050455093, 0.0001913489104481414, 0.0003826978208962828, 0.05032476410269737, 0.0001913489104481414, 0.5977740287780762, 0.010906888172030449, 0.9952055215835571, 0.0010535719338804483, 0.0021071438677608967, 0.06490003317594528, 0.7039967775344849, 0.0021071438677608967, 0.026339299976825714, 0.004846430849283934, 0.0014750007539987564, 0.018964296206831932, 0.0050571453757584095, 0.012010720558464527, 0.012221435084939003, 0.1392822116613388, 0.0021071438677608967, 0.003792859148234129, 0.003480640472844243, 0.001160213490948081, 0.0005801067454740405, 0.9931427240371704, 0.0005801067454740405, 0.0005801067454740405, 0.9934607744216919, 0.9678267240524292, 0.02897684834897518, 0.9962721467018127, 0.9967915415763855, 0.9904733300209045, 0.9957524538040161, 0.993120014667511, 0.9934377074241638, 0.0007730996585451066, 0.0011596495751291513, 0.021646790206432343, 0.9563242793083191, 0.0023192991502583027, 0.0003865498292725533, 0.0007730996585451066, 0.001932749175466597, 0.0003865498292725533, 0.004638598300516605, 0.0034789484925568104, 0.0050251479260623455, 0.0011596495751291513, 0.9959985017776489, 0.0020970473997294903, 0.9824667572975159, 0.0031455710995942354, 0.00733966613188386, 0.0010485236998647451, 0.9938956499099731, 0.9845414161682129, 9.946872160071507e-05, 0.0003978748864028603, 0.009681622497737408, 6.631248106714338e-05, 9.946872160071507e-05, 0.003580874064937234, 3.315624053357169e-05, 9.946872160071507e-05, 0.00013262496213428676, 0.00019893744320143014, 6.631248106714338e-05, 0.0007625935249961913, 0.00023209369101095945, 0.002826493699103594, 0.9920992851257324, 0.0028934183064848185, 0.9895490407943726, 0.0028934183064848185, 0.05624379217624664, 0.016540685668587685, 0.11249440908432007, 0.0967387706041336, 0.022841572761535645, 0.025701889768242836, 0.030698910355567932, 0.02613196149468422, 0.06799907982349396, 0.015141245909035206, 0.0399283803999424, 0.026015911251306534, 0.4117012619972229, 0.013796419836580753, 0.038030605763196945, 0.00037790866917930543, 0.9757601618766785, 0.00037790866917930543, 0.0034011779353022575, 0.007558173034340143, 0.005668629892170429, 0.0018895432585850358, 0.004156995099037886, 0.00838295929133892, 0.0013971597654744983, 0.9877920150756836, 0.996505618095398, 0.993739664554596, 0.003391603007912636, 0.009089333936572075, 0.009089333936572075, 0.03720945864915848, 0.8538292646408081, 0.0028404167387634516, 0.009657417424023151, 0.009373375214636326, 0.028120126575231552, 0.000852125056553632, 0.00312445848248899, 0.007669125217944384, 0.00028404168551787734, 0.015054209157824516, 0.00880529172718525, 0.004828708712011576, 0.9916476011276245, 0.0005650866078212857, 0.0005650866078212857, 0.00028254330391064286, 0.00018836220260709524, 0.001035992056131363, 0.00047090547741390765, 0.00018836220260709524, 0.0009418109548278153, 9.418110130354762e-05, 0.00018836220260709524, 0.0006592677091248333, 0.00047090547741390765, 9.418110130354762e-05, 0.9944581985473633, 0.00028345425380393863, 0.00028345425380393863, 0.00028345425380393863, 0.00028345425380393863, 0.9935072064399719, 0.0005669085076078773, 0.00028345425380393863, 0.00028345425380393863, 0.00028345425380393863, 0.0031179969664663076, 0.0003513151896186173, 0.0003513151896186173, 0.0007026303792372346, 0.0007026303792372346, 0.0003513151896186173, 0.007904591970145702, 0.0003513151896186173, 0.9866687059402466, 0.00017565759480930865, 0.0003513151896186173, 0.00017565759480930865, 0.00017565759480930865, 0.0015809183241799474, 0.48458683490753174, 0.024526910856366158, 0.0008115522214211524, 0.26835325360298157, 0.00018034494132734835, 0.0003606898826546967, 0.00018034494132734835, 0.0008115522214211524, 0.027322258800268173, 0.19116564095020294, 0.0003606898826546967, 9.017247066367418e-05, 9.017247066367418e-05, 0.0010820695897564292, 0.9958968758583069, 0.9835802912712097, 0.004967577289789915, 0.0024837886448949575, 0.004967577289789915, 0.9918099641799927, 0.004678349010646343, 0.0004647042660508305, 0.0004647042660508305, 0.000697056413628161, 0.059946849942207336, 0.00023235213302541524, 0.0005808803252875805, 0.01742640882730484, 0.001161760650575161, 0.8642337322235107, 0.0005808803252875805, 0.0003485282068140805, 0.005460274871438742, 0.008829381316900253, 0.03973221406340599, 0.00011617606651270762, 0.0001910790742840618, 0.0005732371937483549, 0.0003821581485681236, 0.0003821581485681236, 0.9932290315628052, 0.0001910790742840618, 0.0030572651885449886, 0.0017197115812450647, 0.0001910790742840618, 0.0002858786901924759, 0.0005717573803849518, 0.009433996863663197, 0.0004288180498406291, 0.9335368871688843, 0.004145241342484951, 0.00014293934509623796, 0.005002877209335566, 0.00014293934509623796, 0.012435723096132278, 0.0005717573803849518, 0.00014293934509623796, 0.00014293934509623796, 0.032590173184871674, 0.0004288180498406291, 0.0024076667614281178, 0.9943663477897644, 0.0024278475902974606, 0.0012139237951487303, 0.009711390361189842, 0.0006069618975743651, 0.0012139237951487303, 0.014567086473107338, 0.01213923841714859, 0.0006069618975743651, 0.95535808801651, 0.0012139237951487303, 0.9966295957565308, 0.03925641253590584, 0.024442672729492188, 0.0007406870136037469, 0.0007406870136037469, 0.019257862120866776, 0.0037034351844340563, 0.9117857217788696, 0.009528495371341705, 0.001588082523085177, 0.9877873659133911, 0.9931380748748779, 0.004821058362722397, 0.035379912704229355, 0.020264551043510437, 0.26773014664649963, 0.05439869314432144, 0.142654687166214, 0.015281464904546738, 0.029206423088908195, 0.07424798607826233, 0.008997239172458649, 0.10550301522016525, 0.04573366045951843, 0.13473711907863617, 0.06289762258529663, 0.0022977564949542284, 0.0006644115201197565, 0.11261457949876785, 0.004648103844374418, 0.032944455742836, 0.0010600938694551587, 0.0021201877389103174, 0.0026094617787748575, 0.0018755506025627255, 0.6867777109146118, 0.000489274098072201, 0.005137377884238958, 0.12517261505126953, 0.0001630913611734286, 0.008154568262398243, 8.15456805867143e-05, 0.01606449857354164, 0.001337766763754189, 0.001337766763754189, 0.016053201630711555, 0.005351067055016756, 0.9738941788673401, 0.0006193245644681156, 0.0019705782178789377, 0.9669908285140991, 0.0007319290307350457, 0.012048677541315556, 0.004616783000528812, 0.0015764625277370214, 0.001069742371328175, 0.00016890669940039515, 5.630223313346505e-05, 0.0018016714602708817, 0.006925174500793219, 0.0009008357301354408, 0.0003378133988007903, 0.0002252089325338602, 0.00024684457457624376, 0.9740487337112427, 0.00024684457457624376, 0.000987378298304975, 0.00024684457457624376, 0.0017279121093451977, 0.00024684457457624376, 0.0014810675056651235, 0.0004936891491524875, 0.0039495131932199, 0.004690046887844801, 0.010861161164939404, 0.00024684457457624376, 0.0006289500743150711, 0.02798827923834324, 0.0006289500743150711, 0.07830428332090378, 0.00031447503715753555, 0.8902788162231445, 0.0009434251114726067, 0.00031447503715753555, 0.0003977219166699797, 0.0003977219166699797, 0.0003977219166699797, 0.0007954438333399594, 0.0003977219166699797, 0.996691107749939, 0.0003977219166699797, 0.00014134356752038002, 0.00014134356752038002, 0.13187354803085327, 0.00042403070256114006, 0.0018374663777649403, 0.00014134356752038002, 0.00028268713504076004, 0.00028268713504076004, 0.0007067178376019001, 0.00028268713504076004, 0.5762577056884766, 0.286503404378891, 0.00014134356752038002, 0.0007067178376019001, 0.9938207268714905, 0.9942290186882019, 0.9934769868850708, 0.017404967918992043, 0.012642240151762962, 0.08649953454732895, 0.4067579507827759, 0.01940111070871353, 0.0576079897582531, 0.015058623626828194, 0.035405274480581284, 0.09528957307338715, 0.031097808852791786, 0.058203332126140594, 0.018525609746575356, 0.09924683719873428, 0.022482875734567642, 0.024338938295841217, 0.9927520155906677, 0.9920592904090881, 0.022229734808206558, 0.010455145500600338, 0.07272708415985107, 0.413845956325531, 0.03855066001415253, 0.030648348852992058, 0.011157892644405365, 0.026948174461722374, 0.0479588583111763, 0.026231085881590843, 0.02802380733191967, 0.009938842616975307, 0.23031438887119293, 0.014599915593862534, 0.016363952308893204, 0.9955389499664307, 0.0013796413550153375, 7.261270366143435e-05, 0.00018153175187762827, 0.0001670092169661075, 0.05758913233876228, 0.06144486740231514, 0.00024688319535925984, 0.5960631370544434, 0.00015974794223438948, 0.0001670092169661075, 0.0001452254073228687, 0.00015248666750267148, 4.3567619286477566e-05, 0.2820059359073639, 0.00018153175187762827, 0.9971499443054199, 0.00013371260138228536, 0.00013371260138228536, 0.0003075389831792563, 0.00016045512165874243, 0.001123185851611197, 0.0002941677230410278, 0.0002540539426263422, 0.0006685630069114268, 4.011378041468561e-05, 0.002259742934256792, 0.00016045512165874243, 0.0003075389831792563, 0.00013371260138228536, 9.359882096759975e-05, 0.9939392805099487, 6.533486885018647e-05, 0.0007840184262022376, 0.00045734408195130527, 0.00013066973770037293, 0.00026133947540074587, 0.00013066973770037293, 0.0007186835864558816, 0.00032667434425093234, 0.00013066973770037293, 0.0003920092131011188, 0.00013066973770037293, 0.00013066973770037293, 0.00013066973770037293, 0.9960954189300537, 0.000707648869138211, 0.00017691221728455275, 8.845610864227638e-05, 8.845610864227638e-05, 8.845610864227638e-05, 8.845610864227638e-05, 0.00017691221728455275, 0.0006191927241161466, 0.00017691221728455275, 0.00521891051903367, 0.00017691221728455275, 0.0002653683186508715, 8.845610864227638e-05, 0.0002653683186508715, 0.9918583035469055, 0.00024285590916406363, 0.00024285590916406363, 0.00024285590916406363, 0.00024285590916406363, 0.00048571181832812726, 0.00048571181832812726, 0.007771389093250036, 0.00048571181832812726, 0.00024285590916406363, 0.9896378517150879, 0.0003558576572686434, 0.0003558576572686434, 0.0003558576572686434, 0.0005337864859029651, 0.0001779288286343217, 0.0007117153145372868, 0.0001779288286343217, 0.0016013594577088952, 0.0001779288286343217, 0.0001779288286343217, 0.9953339099884033, 0.0001684017333900556, 0.0007578078075312078, 0.0002526025928091258, 0.0001684017333900556, 0.0008420086815021932, 0.0005052051856182516, 0.0010946112452074885, 8.42008666950278e-05, 0.0003368034667801112, 0.0006736069335602224, 0.0003368034667801112, 0.9946648478507996, 0.001254636445082724, 0.001254636445082724, 0.9873988628387451, 0.005018545780330896, 0.001254636445082724, 0.0019347610650584102, 0.990597665309906, 0.0038695221301168203, 0.0019347610650584102, 0.00037222469109110534, 0.0037222469691187143, 0.0007444493821822107, 0.0007444493821822107, 0.00037222469109110534, 0.0014888987643644214, 0.0007444493821822107, 0.9852787852287292, 0.00037222469109110534, 0.00409447168931365, 0.0018611234845593572, 0.00037222469109110534, 0.0006769543397240341, 0.008123451843857765, 0.0006769543397240341, 0.0006769543397240341, 0.007446498144418001, 0.9633060693740845, 0.0006769543397240341, 0.017600813880562782, 0.024861402809619904, 0.049443043768405914, 0.0008579328423365951, 0.000671425717882812, 0.04101292043924332, 0.03041931428015232, 0.00046626784023828804, 0.14888864755630493, 0.0002984114398714155, 0.00013055499584879726, 0.0012309470912441611, 0.000671425717882812, 0.000111904286313802, 0.03426136076450348, 0.6666884422302246, 0.5224702954292297, 0.08933044970035553, 0.012319217436015606, 0.02847609668970108, 0.0002579940774012357, 0.003644166514277458, 0.15631216764450073, 0.01673736609518528, 0.06662697345018387, 0.001644712290726602, 0.026960382238030434, 0.008417056873440742, 0.01186772808432579, 0.0029669320210814476, 0.05201805755496025, 0.09297191351652145, 0.06788577139377594, 0.03223663568496704, 0.005621197167783976, 0.04810308292508125, 0.17855826020240784, 0.007284823339432478, 0.2442275732755661, 0.007886725477874279, 0.0016455434961244464, 0.01818622648715973, 0.0035158314276486635, 0.016933338716626167, 0.030309515073895454, 0.2446383237838745, 0.9946607351303101, 0.996531069278717, 0.007667590398341417, 0.0010628342861309648, 0.95780348777771, 0.00034162530209869146, 0.01210871897637844, 7.591673784190789e-05, 0.0008730424451641738, 0.00030366695136763155, 0.015259264037013054, 0.0008730424451641738, 0.0005314171430654824, 0.00030366695136763155, 0.0017460848903283477, 0.000797125743702054, 0.0002657085715327412, 0.0005281651974655688, 0.9919529557228088, 0.00023474008776247501, 0.00017605506582185626, 0.00017605506582185626, 0.0010563303949311376, 0.0004107951535843313, 0.00017605506582185626, 5.8685021940618753e-05, 0.00017605506582185626, 0.0018779207020998001, 5.8685021940618753e-05, 0.00011737004388123751, 0.0006455352413468063, 0.00234740087762475, 0.0003176144964527339, 0.9858753681182861, 0.00015880724822636694, 0.0003176144964527339, 0.0003176144964527339, 0.0003176144964527339, 0.0003176144964527339, 0.0004764217301271856, 0.00015880724822636694, 0.0038113738410174847, 0.00015880724822636694, 0.0003176144964527339, 0.007305133156478405, 0.5284634828567505, 0.0043123504146933556, 0.00010028721590060741, 0.0004512924933806062, 0.00832383893430233, 0.15980768203735352, 0.00030086166225373745, 0.20037385821342468, 0.001905457116663456, 0.00035100526292808354, 0.0012034466490149498, 0.00035100526292808354, 0.00015043083112686872, 0.09396912157535553, 0.0010277130641043186, 0.9948262572288513, 0.0020554261282086372, 0.0004278726992197335, 0.0004278726992197335, 0.9956597685813904, 0.0004278726992197335, 0.0021393634378910065, 0.9923246502876282, 0.0006438688724301755, 0.0006438688724301755, 0.9954212307929993, 0.0006438688724301755, 0.001287737744860351, 0.0006438688724301755, 8.884011913323775e-05, 0.000355360476532951, 0.0001776802382664755, 0.00026652036467567086, 0.0001776802382664755, 8.884011913323775e-05, 0.0006218808121047914, 8.884011913323775e-05, 0.9978522062301636, 0.0001776802382664755, 8.884011913323775e-05, 0.9973992705345154, 0.9960400462150574, 0.00047350121894851327, 0.00023675060947425663, 0.00023675060947425663, 0.00023675060947425663, 0.00047350121894851327, 0.00047350121894851327, 0.007812770083546638, 0.00047350121894851327, 0.00023675060947425663, 0.9896175265312195, 0.0004763096512760967, 0.0019052386051043868, 0.0019052386051043868, 0.014765598811209202, 0.024768101051449776, 0.00023815482563804835, 0.00023815482563804835, 0.00023815482563804835, 0.9547626972198486, 0.05002395808696747, 0.013705317862331867, 0.01142862718552351, 0.029298843815922737, 0.010308350436389446, 0.24366013705730438, 0.07084845006465912, 0.41670671105384827, 0.013858904130756855, 0.009170005097985268, 0.006396417506039143, 0.0249622892588377, 0.025938015431165695, 0.025919945910573006, 0.0477653369307518, 0.00011427717981860042, 0.002171266358345747, 0.00011427717981860042, 0.00011427717981860042, 0.0005713858990930021, 0.00011427717981860042, 0.9915831089019775, 0.00022855435963720083, 0.00011427717981860042, 0.0006856630789116025, 0.0037711469922214746, 0.00034283153945580125, 0.9958757162094116, 0.0004920334904454648, 0.0009840669808909297, 0.0004920334904454648, 0.0004920334904454648, 0.0004920334904454648, 0.0011684002820402384, 0.9662669897079468, 0.03037840686738491, 0.9956297278404236, 0.0007356012356467545, 0.00011316941527184099, 0.00011316941527184099, 0.00011316941527184099, 0.00011316941527184099, 0.00011316941527184099, 0.0001697541301837191, 0.0007921858923509717, 0.0001697541301837191, 0.0040740990079939365, 0.00011316941527184099, 0.0003395082603674382, 5.6584707635920495e-05, 0.0002829235454555601, 0.9927787184715271, 0.9928073883056641, 0.00609084265306592, 0.0002933928626589477, 0.9480257034301758, 0.0005134374951012433, 7.334821566473693e-05, 0.006821384187787771, 0.0006601339555345476, 0.00014669643132947385, 0.00014669643132947385, 0.0005867857253178954, 0.00036674109287559986, 0.005427767988294363, 0.00014669643132947385, 0.00022004464699421078, 0.010195402428507805, 0.02640535868704319, 0.0004578626248985529, 0.0018314504995942116, 0.0077836643904447556, 0.0004578626248985529, 0.0004578626248985529, 0.0004578626248985529, 0.0004578626248985529, 0.0013735878746956587, 0.0004578626248985529, 0.0009157252497971058, 0.0004578626248985529, 0.9848624467849731, 0.00043867339263670146, 0.00043867339263670146, 0.0008773467852734029, 0.00043867339263670146, 0.00043867339263670146, 0.00043867339263670146, 0.0008773467852734029, 0.00043867339263670146, 0.00043867339263670146, 0.00043867339263670146, 0.9944725632667542, 0.9891691207885742, 0.004453113302588463, 0.004453113302588463, 0.0004453113360796124, 0.012914028950035572, 0.0008906226721592247, 0.0004453113360796124, 0.0008906226721592247, 0.9511850476264954, 0.004007801879197359, 0.0013359340373426676, 0.018703076988458633, 0.0004453113360796124, 0.994547963142395, 0.003022941993549466, 0.9915571212768555, 0.0018568485975265503, 0.0018568485975265503, 0.9946455955505371, 0.0007723515736870468, 0.0015447031473740935, 0.0015447031473740935, 0.005406460724771023, 0.0023170546628534794, 0.9870652556419373, 0.001002380158752203, 0.000668253458570689, 0.001670633559115231, 0.003675393993034959, 0.000668253458570689, 0.0003341267292853445, 0.02405712381005287, 0.0003341267292853445, 0.824958860874176, 0.0003341267292853445, 0.1420038491487503, 0.0003341267292853445, 0.9966834783554077, 0.9940073490142822, 0.002864574547857046, 0.002753671957179904, 0.0022747726179659367, 0.00041903703822754323, 0.03274475410580635, 0.0032924339175224304, 0.1976058930158615, 0.006046106107532978, 0.008141291327774525, 0.08302919566631317, 0.014606433920562267, 0.043939027935266495, 0.01364863570779562, 0.16522032022476196, 0.4032932221889496, 0.02286745049059391, 0.00044723914470523596, 0.0003833478258457035, 6.389130430761725e-05, 0.0006389130721800029, 0.0001277826086152345, 0.981626033782959, 0.012011565268039703, 0.00044723914470523596, 6.389130430761725e-05, 0.00019167391292285174, 0.00140560872387141, 0.002172304317355156, 0.00031945653609000146, 0.0005903433775529265, 0.000393562251701951, 0.0001967811258509755, 0.002361373510211706, 0.0001967811258509755, 0.0013774678809568286, 0.000787124503403902, 0.001180686755105853, 0.001180686755105853, 0.000787124503403902, 0.981937825679779, 0.0001967811258509755, 0.0037388415075838566, 0.005116309504956007, 0.000393562251701951, 0.002330062910914421, 0.0005825157277286053, 0.008737735450267792, 0.0005825157277286053, 0.0005825157277286053, 0.0011650314554572105, 0.0034950943663716316, 0.0005825157277286053, 0.0005825157277286053, 0.9774613380432129, 0.0034950943663716316, 0.0006920034065842628, 0.2485676258802414, 0.00027680135099217296, 0.2368035614490509, 0.00013840067549608648, 0.0004152020555920899, 0.0004152020555920899, 0.0012456061085686088, 0.11113574355840683, 0.3980403542518616, 0.00027680135099217296, 0.0011072054039686918, 0.00013840067549608648, 0.0006920034065842628, 0.968441367149353, 0.0015928312204778194, 0.02548529952764511, 0.0015928312204778194, 0.9895297288894653, 0.010414696298539639, 0.9852302670478821, 0.0020829392597079277, 0.00019391783280298114, 0.0002908767492044717, 9.695891640149057e-05, 0.0003878356656059623, 0.00019391783280298114, 9.695891640149057e-05, 0.00475098704919219, 0.9918897151947021, 0.0007756713312119246, 0.00019391783280298114, 9.695891640149057e-05, 0.0006787124439142644, 0.00019391783280298114, 0.9179137945175171, 0.023546844720840454, 0.020242024213075638, 0.0008262050687335432, 0.008675153367221355, 0.0004131025343667716, 0.0004131025343667716, 0.007435845676809549, 0.01941581815481186, 0.0004131025343667716, 0.0004131025343667716, 0.9459551572799683, 0.031125178560614586, 0.0001564079284435138, 0.0017204872565343976, 0.0001564079284435138, 0.020020214840769768, 0.0001564079284435138, 0.0001564079284435138, 0.0001564079284435138, 0.0001564079284435138, 0.0001564079284435138, 7.048773113638163e-05, 7.048773113638163e-05, 0.0014097546227276325, 0.003947312943637371, 0.00042292638681828976, 0.00014097546227276325, 0.00014097546227276325, 0.0011982914293184876, 0.014449984766542912, 0.005216091871261597, 0.010220721364021301, 0.11715060472488403, 0.8355615735054016, 0.005357067566365004, 0.004722678102552891, 0.9948000311851501, 0.001234243274666369, 0.001234243274666369, 0.8843274712562561, 0.042570825666189194, 0.0034820465371012688, 0.00786268524825573, 0.00011232407996430993, 0.00011232407996430993, 0.05492647364735603, 0.0022464815992861986, 0.0008985926397144794, 0.0007862685597501695, 0.0005616203998215497, 0.0007862685597501695, 0.0003369722398929298, 0.0004492963198572397, 0.0003369722398929298, 0.00032307853689417243, 0.002907706890255213, 0.18835479021072388, 0.0009692356106825173, 0.0006461570737883449, 0.0009692356106825173, 0.012276984751224518, 0.7921885848045349, 0.00032307853689417243, 0.0009692356106825173, 0.9925151467323303, 0.0018586426740512252, 0.0004646606685128063, 0.002323303371667862, 0.0018586426740512252, 0.04081999137997627, 0.00027618397143669426, 0.0008285518852062523, 0.003645628457888961, 0.0004971311427652836, 0.0018780509708449244, 0.5522574782371521, 0.20868460834026337, 0.02894407883286476, 0.0009390254854224622, 0.0031484973151236773, 0.0033694442827254534, 0.025961292907595634, 0.00044189434265717864, 0.1282598376274109, 0.9950594902038574, 0.14050745964050293, 0.5046596527099609, 0.020701726898550987, 0.0006606933893635869, 0.010350863449275494, 0.18323230743408203, 0.0019820802845060825, 0.11396960914134979, 0.0002202311297878623, 0.0009910401422530413, 0.0013213867787271738, 0.0005505778244696558, 0.002202311297878623, 0.000770808954257518, 0.017838722094893456, 0.9956666231155396, 0.9663991332054138, 0.029229462146759033, 0.0009134206920862198, 0.0009134206920862198, 0.0005165767506696284, 0.0005165767506696284, 0.9969931840896606, 0.0005165767506696284, 0.0005165767506696284, 0.9925643801689148, 0.002063881838694215, 0.0006879606517031789, 0.9879114627838135, 0.00825552735477686, 0.0006879606517031789, 0.9547421336174011, 0.03908301144838333, 0.0027916436083614826, 0.0001534087205072865, 0.000306817441014573, 0.0001534087205072865, 0.0001534087205072865, 0.0015340872341766953, 0.0018409047042950988, 0.0001534087205072865, 0.0010738610289990902, 0.0013806784991174936, 0.9924010038375854, 0.0001534087205072865, 0.0001534087205072865, 0.0001534087205072865, 0.0004602261760737747, 0.9938997626304626, 0.0005982253351248801, 0.0005982253351248801, 0.0005982253351248801, 0.0005982253351248801, 0.0005982253351248801, 0.9966433644294739, 0.000640218029730022, 0.009843352250754833, 0.04225438833236694, 0.029049891978502274, 0.01464498694986105, 0.0001600545074325055, 0.002000681357458234, 0.0004001362540293485, 0.008562915958464146, 0.0007202452979981899, 0.002480844734236598, 0.055298831313848495, 0.8190789222717285, 0.014404905959963799, 0.00024008176114875823, 0.0006246140692383051, 0.002141533885151148, 0.000535383471287787, 0.01293843425810337, 0.0015169199323281646, 0.92523193359375, 0.0013384587364271283, 0.0039261458441615105, 0.022753799334168434, 8.92305833986029e-05, 0.007406138349324465, 0.000535383471287787, 0.0002676917356438935, 0.019719958305358887, 0.0009815364610403776, 0.003985114861279726, 0.000742055824957788, 0.0001923848467413336, 0.0006046380731277168, 0.005963930394500494, 0.3556371331214905, 0.0015115952119231224, 0.6279441118240356, 0.000467220350401476, 0.0004397367883939296, 0.0003572861314751208, 0.0005496710073202848, 0.0001649012992857024, 0.0014291445259004831, 0.0005925841396674514, 0.0002962920698337257, 0.00014814603491686285, 0.0010370222153142095, 0.11214654892683029, 0.00014814603491686285, 0.8835429549217224, 0.001333314343355596, 0.00014814603491686285, 0.00014814603491686285, 0.0002962920698337257, 0.00015228027768898755, 0.0003045605553779751, 0.00015228027768898755, 0.0003045605553779751, 0.00015228027768898755, 0.0007614013738930225, 0.0003045605553779751, 0.9960652589797974, 0.0009136816370300949, 0.0003045605553779751, 0.00015228027768898755, 0.00015228027768898755, 0.00045684081851504743, 0.0003098970337305218, 0.0003098970337305218, 0.0009296911302953959, 0.014565160498023033, 0.9578917622566223, 0.005268249660730362, 0.004028661642223597, 0.0006197940674610436, 0.014875058084726334, 0.0009296911302953959, 0.9944031834602356, 0.0014432556927204132, 0.0014432556927204132, 0.00042658360325731337, 0.00042658360325731337, 0.0012797508388757706, 0.009811422787606716, 0.00042658360325731337, 0.9764499068260193, 0.00042658360325731337, 0.0017063344130292535, 0.00042658360325731337, 0.007678505033254623, 0.0018593252170830965, 0.001062471535988152, 0.001062471535988152, 0.000531235767994076, 0.002124943071976304, 0.000531235767994076, 0.000265617883997038, 0.985973596572876, 0.000265617883997038, 0.0007968536810949445, 0.001593707362189889, 0.000265617883997038, 0.000531235767994076, 0.0029217968694865704, 0.00012361860717646778, 0.00024723721435293555, 0.00012361860717646778, 0.00012361860717646778, 0.00012361860717646778, 0.0004944744287058711, 0.00012361860717646778, 0.9959951043128967, 0.0008653302793391049, 0.0006180930649861693, 0.00012361860717646778, 0.00012361860717646778, 0.00012361860717646778, 0.0008653302793391049, 0.0024699638597667217, 0.0003033289103768766, 0.0003466616035439074, 0.0009533194242976606, 0.740035891532898, 0.05113258585333824, 0.0006933232070878148, 0.15387442708015442, 0.036832794547080994, 0.0002599962172098458, 0.0005633251275867224, 0.0003899942967109382, 0.0005633251275867224, 0.006629903335124254, 0.0049399277195334435, 0.006820800248533487, 6.496000423794612e-05, 0.000779520021751523, 0.0002598400169517845, 0.009938880801200867, 0.012797120958566666, 0.0002598400169517845, 0.485381156206131, 0.00012992000847589225, 0.00012992000847589225, 0.00045472002238966525, 6.496000423794612e-05, 0.0009094400447793305, 0.48206818103790283, 0.00012992000847589225, 0.9958343505859375, 0.0017022809479385614, 0.9974455237388611, 0.051948465406894684, 0.03716731071472168, 0.019098279997706413, 0.015324368141591549, 0.028032729402184486, 0.22945095598697662, 0.09177466481924057, 0.30860304832458496, 0.005332079716026783, 0.009534844197332859, 0.045887332409620285, 0.03312179073691368, 0.04005492478609085, 0.032750118523836136, 0.05191987380385399, 0.00034157480695284903, 0.00034157480695284903, 0.9926163554191589, 0.0017078740056604147, 0.00034157480695284903, 0.003757322672754526, 0.00034157480695284903, 0.00034157480695284903, 0.007905804552137852, 0.0002526969474274665, 0.00010829869279405102, 0.01270704623311758, 0.0009385886369273067, 0.07317381352186203, 0.00014439824735745788, 0.9027056694030762, 0.00018049782374873757, 0.00021659738558810204, 0.00032489607110619545, 0.00018049782374873757, 0.00010829869279405102, 0.0006136925658211112, 0.0004692943184636533, 0.03208085522055626, 0.0006416170508600771, 0.0006416170508600771, 0.8995471596717834, 0.0025664682034403086, 0.06352008879184723, 0.9921711683273315, 0.0017876958008855581, 0.0008938479004427791, 0.0035753916017711163, 0.0008938479004427791, 0.007354991510510445, 0.03630044311285019, 0.10652875155210495, 0.6790317893028259, 0.13784678280353546, 0.0009490311495028436, 0.0002372577873757109, 0.0004745155747514218, 0.0002372577873757109, 0.005219671409577131, 0.010676600970327854, 0.0023725780192762613, 0.01210014708340168, 0.0002372577873757109, 0.0007117733475752175, 0.000378308177459985, 0.00012610273552127182, 0.996842086315155, 0.00012610273552127182, 0.00025220547104254365, 0.00025220547104254365, 0.00012610273552127182, 0.00012610273552127182, 0.0010088218841701746, 0.000378308177459985, 0.00012610273552127182, 0.00012610273552127182, 0.007961862720549107, 0.0009952328400686383, 0.0009952328400686383, 0.9882662296295166, 0.0026771891862154007, 4.615843135979958e-05, 0.00013847529771737754, 0.00023079216771293432, 0.0009693271131254733, 0.004662001505494118, 0.00023079216771293432, 0.847284197807312, 9.231686271959916e-05, 0.00013847529771737754, 0.00013847529771737754, 0.00013847529771737754, 9.231686271959916e-05, 0.1429988294839859, 0.00013847529771737754, 0.988771378993988, 0.056401100009679794, 0.026080425828695297, 0.07829936593770981, 0.3356765806674957, 0.03270218148827553, 0.04902423173189163, 0.03659391775727272, 0.049488913267850876, 0.04507441073656082, 0.04902423173189163, 0.07190994918346405, 0.020620381459593773, 0.06389413774013519, 0.03415432199835777, 0.051057226955890656, 0.0011625578626990318, 0.009300462901592255, 0.0005812789313495159, 0.0005812789313495159, 0.9649230241775513, 0.0017438367940485477, 0.0005812789313495159, 0.0005812789313495159, 0.013950694352388382, 0.005812789313495159, 0.00019780476577579975, 0.0027692667208611965, 0.00019780476577579975, 0.00019780476577579975, 0.00019780476577579975, 0.0005934142973273993, 0.9737929105758667, 0.00019780476577579975, 0.00019780476577579975, 0.0003956095315515995, 0.0025714619550853968, 0.018989257514476776, 0.9919639229774475, 0.9935616254806519, 0.001880164141766727, 0.9946067929267883, 0.00010911879508057609, 0.0005237702280282974, 0.000152766311657615, 8.729503315407783e-05, 4.3647516577038914e-05, 0.9961017966270447, 0.00010911879508057609, 8.729503315407783e-05, 0.0007638315437361598, 0.0003928276419173926, 0.0006547127850353718, 0.00017459006630815566, 0.000152766311657615, 0.00061106524663046, 0.0007126592681743205, 0.001425318536348641, 0.001425318536348641, 0.0021379778627306223, 0.0007126592681743205, 0.9927343726158142, 0.022071141749620438, 0.001430536969564855, 0.0022479866165667772, 0.00020436242630239576, 0.020640604197978973, 0.0006130872643552721, 0.0004087248526047915, 0.000817449705209583, 0.003065436379984021, 0.001634899410419166, 0.014509731903672218, 0.00020436242630239576, 0.9320970177650452, 0.002573808655142784, 0.9934901595115662, 0.00029594238731078804, 0.0005918847746215761, 0.00029594238731078804, 0.00029594238731078804, 0.00029594238731078804, 0.0005918847746215761, 0.0017756542656570673, 0.00029594238731078804, 0.9952542185783386, 0.0013221025001257658, 0.9928989410400391, 0.003966307267546654, 0.9959302544593811, 0.0158999040722847, 0.00018760949023999274, 0.00042212134576402605, 0.0005628284998238087, 0.002579630585387349, 0.5179898142814636, 0.0003283166151959449, 0.10801616311073303, 0.00028141424991190434, 0.00023451187007594854, 0.00023451187007594854, 0.00018760949023999274, 9.380474511999637e-05, 0.33403870463371277, 0.01885475404560566, 0.00014929761528037488, 4.9765872972784564e-05, 0.00019906349189113826, 4.9765872972784564e-05, 0.34014973044395447, 0.001592507935129106, 0.0002488293685019016, 0.0016920396592468023, 0.00014929761528037488, 0.0002488293685019016, 9.953174594556913e-05, 9.953174594556913e-05, 0.6551676988601685, 9.953174594556913e-05, 0.0016007418744266033, 0.0016007418744266033, 0.0016007418744266033, 0.0016007418744266033, 0.9924599528312683, 0.994166910648346, 0.00012096986756660044, 0.0003629096026998013, 0.0003629096026998013, 0.00012096986756660044, 0.00012096986756660044, 0.00048387947026640177, 0.9848156571388245, 0.007379161659628153, 0.00012096986756660044, 0.00024193973513320088, 0.0006048493087291718, 0.0006048493087291718, 0.004838794469833374, 0.0002734761801548302, 0.0001367380900774151, 0.0001367380900774151, 0.0001367380900774151, 0.990804135799408, 0.0041021425276994705, 0.0001367380900774151, 0.00041021424112841487, 0.00041021424112841487, 0.00041021424112841487, 0.003144975984469056, 0.00014947606541682035, 0.00014947606541682035, 0.00014947606541682035, 0.040657490491867065, 0.012182299047708511, 0.00014947606541682035, 7.473803270841017e-05, 7.473803270841017e-05, 0.001494760625064373, 0.2961120903491974, 0.0006726423162035644, 0.6280236840248108, 0.014947607181966305, 0.004932710435241461, 7.473803270841017e-05, 0.00024093578394968063, 0.00024093578394968063, 0.00024093578394968063, 0.0007228073664009571, 0.00024093578394968063, 0.00024093578394968063, 0.9967513680458069, 0.00024093578394968063, 0.00024093578394968063, 0.00048187156789936125, 0.00024093578394968063, 0.9938920736312866, 0.00040266808355227113, 0.00040266808355227113, 0.00040266808355227113, 0.006845357362180948, 0.00020133404177613556, 0.00020133404177613556, 0.00020133404177613556, 0.00020133404177613556, 0.13751114904880524, 0.00020133404177613556, 0.8534549474716187, 0.00020133404177613556, 0.0006548749515786767, 0.003929249942302704, 0.8775324821472168, 0.0006548749515786767, 0.0006548749515786767, 0.0006548749515786767, 0.0006548749515786767, 0.0013097499031573534, 0.0006548749515786767, 0.0032743748743087053, 0.11001899838447571, 0.9933422207832336, 0.002217281609773636, 0.9939315319061279, 0.9952807426452637, 0.0003359914699103683, 0.7613566517829895, 0.005039872135967016, 0.0026879317592829466, 0.0003359914699103683, 0.06820626556873322, 0.0003359914699103683, 0.00873577781021595, 0.0003359914699103683, 0.14313235878944397, 0.009071769192814827, 0.0032008220441639423, 0.9922548532485962, 0.9921143651008606, 0.0004281741857994348, 0.0004281741857994348, 0.0004281741857994348, 0.0004281741857994348, 0.0004281741857994348, 0.0021408710163086653, 0.0012845225865021348, 0.0008563483715988696, 0.0004281741857994348, 0.03853567689657211, 0.9544003009796143, 0.0007621569093316793, 0.0007621569093316793, 0.9969012141227722, 0.0005187105853110552, 0.0005187105853110552, 0.0005187105853110552, 0.0005187105853110552, 0.0005187105853110552, 0.0010374211706221104, 0.0005187105853110552, 0.0005187105853110552, 0.9954055547714233, 0.996870219707489, 0.9948192834854126, 0.0012515848502516747, 0.0014601823640987277, 0.06153625622391701, 0.00031289621256291866, 0.0006257924251258373, 0.7511594891548157, 0.00031289621256291866, 0.0008343898807652295, 0.001877377275377512, 0.00031289621256291866, 0.0015644810628145933, 0.09032270312309265, 0.08781953901052475, 0.00020859747019130737, 0.00041719494038261473, 0.0025539426133036613, 0.9924620985984802, 0.0005107885226607323, 0.0030647311359643936, 0.0005107885226607323, 0.0021380872931331396, 0.004276174586266279, 0.9899343848228455, 0.0001373916311422363, 0.9901815056800842, 0.0001373916311422363, 0.006594798527657986, 0.0004121749079786241, 0.0001373916311422363, 0.0004121749079786241, 0.0001373916311422363, 0.0006869581993669271, 0.0002747832622844726, 0.0002747832622844726, 0.0005495665245689452, 0.0001373916311422363, 0.0359983891248703, 0.0536959208548069, 0.0046254913322627544, 0.0004022166249342263, 0.0008044332498684525, 0.0026144080329686403, 0.0008044332498684525, 0.8579280972480774, 0.0020110830664634705, 0.00020110831246711314, 0.007239899132400751, 0.002212191466242075, 0.022725239396095276, 0.0004022166249342263, 0.008647657930850983, 0.00024123470939230174, 0.061273619532585144, 0.00024123470939230174, 0.00024123470939230174, 0.00024123470939230174, 0.9347845315933228, 0.002171112457290292, 0.00024123470939230174, 0.0022822069004178047, 0.0015799894463270903, 0.037217527627944946, 0.7336417436599731, 0.0024577612057328224, 0.005617740098387003, 0.0036866420414298773, 0.0005266631487756968, 0.047224126756191254, 0.001755543751642108, 0.011059925891458988, 0.0010533262975513935, 0.15045009553432465, 0.0010533262975513935, 0.00017555437807459384, 0.003071144688874483, 0.9919797778129578, 0.00123556237667799, 0.9970988035202026, 0.0010740620782598853, 0.0005370310391299427, 0.9097306132316589, 0.08377684652805328, 0.002685155253857374, 0.0005370310391299427, 0.04092578962445259, 0.00041761010652408004, 0.00041761010652408004, 0.00041761010652408004, 0.008352202363312244, 0.0008352202130481601, 0.9412931799888611, 0.00041761010652408004, 0.006681761704385281, 0.00041761010652408004, 0.9854589104652405, 0.009152868762612343, 0.9932615756988525, 0.0003230339498259127, 0.0006460678996518254, 0.00016151697491295636, 0.00016151697491295636, 0.0003230339498259127, 0.0008075848454609513, 0.04037924483418465, 0.004199441056698561, 0.026811817660927773, 0.0012921357993036509, 0.0003230339498259127, 0.00048455092473886907, 0.0295576062053442, 0.8946425318717957, 0.002233329229056835, 0.9915981888771057, 0.00446665845811367, 0.0012701002415269613, 0.0012701002415269613, 0.003810300724580884, 0.9894080758094788, 0.0025402004830539227, 0.0019311928190290928, 0.0019311928190290928, 0.9926331043243408, 0.00019949254055973142, 0.000598477607127279, 0.00039898508111946285, 0.000598477607127279, 0.00019949254055973142, 0.00019949254055973142, 0.9823012948036194, 0.004787820857018232, 0.0031918806489557028, 0.002593402983620763, 0.0009974626591429114, 0.00039898508111946285, 0.0037903583142906427, 0.01566132716834545, 0.003915331792086363, 0.0019576658960431814, 0.004894164856523275, 0.9641504883766174, 0.003915331792086363, 0.004894164856523275, 0.0016348441131412983, 0.0005030289757996798, 0.0006287861615419388, 0.01006057858467102, 0.0032696882262825966, 0.0007545434054918587, 0.0031439310405403376, 0.00012575724394991994, 0.9761276841163635, 0.0011318151373416185, 0.0005030289757996798, 0.0002515144878998399, 0.0002515144878998399, 0.0018863586010411382, 0.00042936523095704615, 0.0024936210829764605, 0.0034514358267188072, 0.0008752444991841912, 0.00325326737947762, 0.14728878438472748, 0.028239021077752113, 0.6719400882720947, 0.0011724974028766155, 0.00028073880821466446, 0.09171901643276215, 3.3028092730091885e-05, 0.048055876046419144, 8.257023728219792e-05, 0.0006770759355276823, 0.0010599229717627168, 0.0010599229717627168, 0.03709730505943298, 0.0010599229717627168, 0.9571104645729065, 0.9965400099754333, 0.0009998900350183249, 0.9888913035392761, 0.0019997800700366497, 0.002999670337885618, 0.0009998900350183249, 0.002999670337885618, 0.034325432032346725, 0.005973165854811668, 0.0007838091114535928, 0.2160610407590866, 0.00018919529975391924, 0.0010000322945415974, 0.10386822372674942, 0.0041893245652318, 0.0012973392149433494, 0.4267435073852539, 0.0007027254323475063, 0.03373081982135773, 0.12092282623052597, 0.049704309552907944, 0.00043244639527983963, 0.9936332702636719, 0.1870393007993698, 0.44251495599746704, 0.00018088906654156744, 0.0012662234948948026, 0.0015074089169502258, 0.20434434711933136, 0.005426672287285328, 0.0933990553021431, 6.0296355513855815e-05, 0.000783852650783956, 0.00114563072565943, 0.00018088906654156744, 0.003075114218518138, 0.00844149012118578, 0.05064893886446953, 0.0003786618181038648, 0.012117178179323673, 0.0003786618181038648, 0.010791861452162266, 0.0001893309090519324, 0.0003786618181038648, 0.041842129081487656, 0.0001893309090519324, 0.30482277274131775, 0.0007573236362077296, 0.0017039781669154763, 0.02196238562464714, 0.5829498767852783, 0.006437250878661871, 0.015335803851485252, 0.0018915865803137422, 0.9874082207679749, 0.009457932785153389, 0.001382546965032816, 0.9940512776374817, 0.002765093930065632, 0.0003113060083705932, 0.0003113060083705932, 0.000186783610843122, 0.002241403330117464, 6.22612060396932e-05, 0.000373567221686244, 0.9544642567634583, 0.002179142087697983, 0.022289510816335678, 0.013510681688785553, 0.000373567221686244, 0.0001245224120793864, 0.0002490448241587728, 0.000186783610843122, 0.003299843752756715, 0.9937378168106079, 0.987094521522522, 0.003133633406832814, 0.006267266813665628, 0.9953094720840454, 0.9869850873947144, 0.0017437899950891733, 0.0017437899950891733, 0.005231369752436876, 0.0348457507789135, 0.39373835921287537, 0.018075069412589073, 0.01742287538945675, 0.35721555352211, 0.055715933442115784, 0.002701943274587393, 0.07164808362722397, 0.00027951138326898217, 0.007453636731952429, 0.012764353305101395, 0.0007453636499121785, 0.004751693457365036, 0.004751693457365036, 0.01798189803957939, 0.00013737886911258101, 6.868943455629051e-05, 6.868943455629051e-05, 0.0017172357765957713, 6.868943455629051e-05, 6.868943455629051e-05, 0.81685471534729, 0.0006868943455629051, 0.17783693969249725, 0.0010990309529006481, 0.00013737886911258101, 0.00013737886911258101, 6.868943455629051e-05, 0.0005495154764503241, 0.0004808260127902031, 0.8954198360443115, 0.04681282863020897, 0.023572416976094246, 0.02124837599694729, 0.0006640117499046028, 0.0003320058749523014, 0.0003320058749523014, 0.002988052787259221, 0.002988052787259221, 0.004316076636314392, 0.0016600294038653374, 0.9870737791061401, 0.0053718299604952335, 0.0013429574901238084, 0.0026859149802476168, 0.0026859149802476168, 0.002607843605801463, 0.993588387966156, 0.7225947380065918, 0.0029484406113624573, 0.02993801236152649, 0.001814424991607666, 0.0004536062479019165, 0.00022680312395095825, 0.0013608187437057495, 0.0004536062479019165, 0.000907212495803833, 0.0024948343634605408, 0.0024948343634605408, 0.014969006180763245, 0.19006101787090302, 0.029257602989673615, 0.00046096276491880417, 0.00437914626672864, 0.00046096276491880417, 0.00046096276491880417, 0.00046096276491880417, 0.0018438510596752167, 0.00023048138245940208, 0.00023048138245940208, 0.9913004636764526, 0.0022243845742195845, 0.9898511171340942, 0.004448769148439169, 0.9932965636253357, 0.004474308807402849, 0.0028437678702175617, 0.0028437678702175617, 0.9924749732017517, 0.14689606428146362, 0.020676804706454277, 0.014336408115923405, 0.008572411723434925, 0.032535918056964874, 0.11617518961429596, 0.03777257353067398, 0.2583496868610382, 0.05641357973217964, 0.010362929664552212, 0.02195224165916443, 0.03130953758955002, 0.0745517760515213, 0.027434170246124268, 0.1426650434732437, 0.0005189001676626503, 0.0002594500838313252, 0.0002594500838313252, 0.0002594500838313252, 0.0002594500838313252, 0.9957694411277771, 0.001556700561195612, 0.0002594500838313252, 0.000778350280597806, 0.0005693259881809354, 0.0005693259881809354, 0.0005693259881809354, 0.9974591732025146, 0.0016253786161541939, 0.00020317232701927423, 0.0006095169810578227, 0.9868080019950867, 0.004672963637858629, 0.0028444125782698393, 0.0026412401348352432, 0.00020317232701927423, 0.9833325743675232, 0.01242104358971119, 0.002070173854008317, 0.9942095279693604, 0.002028999151661992, 0.003229302354156971, 0.9930105209350586, 0.08879514783620834, 0.07439622282981873, 0.0007826086366549134, 0.000554011610802263, 0.00025818016729317605, 0.13918869197368622, 0.18896366655826569, 0.13829313218593597, 0.00010488568659638986, 0.02498161979019642, 0.11195068061351776, 0.0004545046540442854, 0.0005432540783658624, 0.00042492151260375977, 0.2303047776222229, 0.0001388270320603624, 0.00564919225871563, 0.00017086404841393232, 0.0002669750538188964, 6.407401815522462e-05, 0.9186078310012817, 0.00011746902600862086, 0.0008649991941638291, 0.0004805551143363118, 1.067900302587077e-05, 0.0007902461802586913, 0.0003096910659223795, 0.00014950604236219078, 0.00038444410893134773, 0.07198715955018997, 8.546444587409496e-05, 0.0002563933376222849, 0.0001424407382728532, 8.546444587409496e-05, 5.697629603673704e-05, 0.00011395259207347408, 0.993125319480896, 0.0034185778349637985, 2.848814801836852e-05, 0.00011395259207347408, 0.00034185778349637985, 0.00031336964457295835, 0.00031336964457295835, 2.848814801836852e-05, 0.0015383600257337093, 0.6016663312911987, 0.18948349356651306, 0.00015296787023544312, 0.009374745190143585, 6.555765867233276e-05, 8.741021156311035e-05, 0.19767819344997406, 0.0002185255434596911, 0.0003496408462524414, 0.00015296787023544312, 0.00013111531734466553, 8.741021156311035e-05, 0.0002403780963504687, 0.0001748204231262207, 0.00015296787023544312, 0.0057917493395507336, 8.307837561005726e-05, 4.7473356971750036e-05, 0.00017802508955355734, 0.00018989342788700014, 0.0004391285474412143, 0.00022549844288732857, 0.9898906946182251, 5.934169530519284e-05, 7.121003727661446e-05, 0.00022549844288732857, 0.00013055172166787088, 8.307837561005726e-05, 0.002433009445667267, 0.00017802508955355734, 0.0035882850643247366, 0.0006043427274562418, 0.0003021713637281209, 0.000377714226488024, 0.00033994278055615723, 0.000188857113244012, 7.554284093203023e-05, 0.7221895456314087, 0.0002643999469000846, 0.00011331426503602415, 7.554284093203023e-05, 3.7771420466015115e-05, 0.271727591753006, 3.7771420466015115e-05, 7.554284093203023e-05, 0.00029164727311581373, 0.001166589092463255, 0.0007291181827895343, 0.005978769157081842, 0.0014582363655790687, 0.8494226932525635, 0.0007291181827895343, 0.012540833093225956, 0.1073262020945549, 0.0007291181827895343, 0.0016040600603446364, 0.00029164727311581373, 0.00612459285184741, 0.011374243535101414, 0.08130699396133423, 0.04601258039474487, 0.0006846407777629793, 0.0011568068293854594, 0.01699797809123993, 0.25345873832702637, 0.01990179903805256, 0.2617688477039337, 0.000566599250305444, 0.0024316550698131323, 0.018178392201662064, 0.0010387653019279242, 0.001133198500610888, 0.030336668714880943, 0.2650268077850342, 0.0002244694478577003, 0.0011223471956327558, 0.0004489388957154006, 0.0002244694478577003, 0.0002244694478577003, 0.9845229983329773, 0.002469163853675127, 0.0006734083290211856, 0.0008978777914308012, 0.0004489388957154006, 0.0008978777914308012, 0.008080900646746159, 0.9947580099105835, 0.005408361554145813, 0.031817611306905746, 0.09051889181137085, 0.11515698581933975, 0.005313477944582701, 0.0008223239565268159, 0.40445688366889954, 0.000980463228188455, 0.07287055253982544, 0.002150693442672491, 0.006230685394257307, 0.04383619502186775, 0.1912219524383545, 0.025239020586013794, 0.003985108342021704, 0.00020098398090340197, 0.00020098398090340197, 0.0034167275298386812, 0.00040196796180680394, 0.00020098398090340197, 0.003818695666268468, 0.00020098398090340197, 0.00040196796180680394, 0.00020098398090340197, 0.991051971912384, 0.00020098398090340197, 0.0032774570863693953, 0.0013109828578308225, 0.0006554914289154112, 0.0003277457144577056, 0.0003277457144577056, 0.9937250018119812, 0.04978501424193382, 0.002039107959717512, 0.0009233696619048715, 0.0011157382978126407, 0.0011157382978126407, 0.33133581280708313, 0.002039107959717512, 0.34845662117004395, 0.0009618433541618288, 0.0006925272173248231, 0.0052324277348816395, 0.0010387907968834043, 0.009733854793012142, 0.24138420820236206, 0.004232110921293497, 0.8838995099067688, 0.07384476810693741, 0.0005594300455413759, 0.040838394314050674, 0.0005594300455413759, 0.0009614948066882789, 0.010439086705446243, 0.00041206920286640525, 0.00027471280191093683, 0.00013735640095546842, 0.9803126454353333, 0.00013735640095546842, 0.0010988512076437473, 0.0005494256038218737, 0.00013735640095546842, 0.00041206920286640525, 0.00041206920286640525, 0.00041206920286640525, 0.0009614948066882789, 0.003296553622931242, 0.0033849379979074, 0.9917868375778198, 0.9907800555229187, 0.9967127442359924, 0.001088114338926971, 0.001088114338926971, 0.994652509689331, 0.0015147089725360274, 0.00046606428804807365, 0.0006408384069800377, 0.0021555472631007433, 0.001689483062364161, 0.6845319271087646, 0.27328845858573914, 0.006699674297124147, 0.00011651607201201841, 0.0001747741043800488, 0.00029129019821994007, 0.0008738705655559897, 0.0016312249936163425, 0.0005825803964398801, 0.02540050446987152, 0.09188958257436752, 0.020202673971652985, 0.0006516991998068988, 0.0014482203405350447, 0.0014482203405350447, 0.2287464141845703, 0.48805028200149536, 0.04453277587890625, 0.0001448220427846536, 0.0002896440855693072, 0.00101375428494066, 0.00050687714247033, 0.0006516991998068988, 0.0034033178817480803, 0.11687138676643372, 0.9938338398933411, 0.0046691205352544785, 0.9945226907730103, 0.11517301946878433, 0.01823854260146618, 0.21413399279117584, 0.0003377507964614779, 0.0003377507964614779, 0.0003377507964614779, 0.0023642554879188538, 0.0013510031858459115, 0.0003377507964614779, 0.6451039910316467, 0.0003377507964614779, 0.0010132523020729423, 0.0010132523020729423, 0.0003377507964614779, 0.9933509230613708, 0.9964953064918518, 0.00027647832757793367, 0.003455979283899069, 0.0416099913418293, 0.950117826461792, 0.00013823916378896683, 0.0005529566551558673, 0.0008294350118376315, 0.00013823916378896683, 0.00027647832757793367, 0.00027647832757793367, 0.00013823916378896683, 0.00027647832757793367, 0.00013823916378896683, 0.001797109143808484, 0.0003708978765644133, 0.0007417957531288266, 0.01446501724421978, 0.9776868224143982, 0.0007417957531288266, 0.002596285194158554, 0.0003708978765644133, 0.002596285194158554, 0.9952820539474487, 0.0022216117940843105, 0.00336713925935328, 0.0004489518760237843, 0.764116108417511, 0.19349826872348785, 0.0005611898377537727, 0.0004489518760237843, 0.004489518702030182, 0.012570653110742569, 0.00022447593801189214, 0.0003367139142937958, 0.00022447593801189214, 0.018182551488280296, 0.0007856658194214106, 0.00011223796900594607, 0.0005611898377537727, 0.9961390495300293, 0.9902538657188416, 0.9943317174911499, 0.9950021505355835, 0.9949995875358582, 0.9932604432106018, 0.9938952922821045, 0.9937882423400879, 0.9938935041427612, 0.003596613882109523, 0.9926654100418091, 0.004857872147113085, 0.00013879634207114577, 0.06870418787002563, 0.003053519641980529, 0.00013879634207114577, 0.00013879634207114577, 0.002914723241701722, 0.0004163890262134373, 0.00805018749088049, 0.00013879634207114577, 0.8767765164375305, 0.00027759268414229155, 0.0335887148976326, 0.0004163890262134373, 0.00027759268414229155, 0.9904906749725342, 0.9936375021934509, 0.9936372637748718, 0.9936373829841614, 0.03407458961009979, 0.03068353794515133, 0.2008923888206482, 0.010993567295372486, 0.0007657210808247328, 0.005524130538105965, 0.012032759375870228, 0.046654291450977325, 0.0037192166782915592, 0.06732875853776932, 0.1835542768239975, 0.005852296948432922, 0.3964247405529022, 0.0004922492662444711, 0.0009844985324889421, 0.00047961747623048723, 0.00023980873811524361, 0.04772194102406502, 0.003836939809843898, 0.00023980873811524361, 0.002877705032005906, 0.008153497241437435, 0.00023980873811524361, 0.3247010409832001, 0.5673874616622925, 0.00023980873811524361, 0.04220633953809738, 0.001678661210462451, 0.00023980873811524361, 0.9896541833877563, 0.9926968216896057, 0.9963108897209167, 0.9886121153831482, 0.9856900572776794, 0.0035782535560429096, 0.0013572686584666371, 0.13054455816745758, 0.5828851461410522, 0.0029613133519887924, 0.003948417957872152, 0.0013572686584666371, 0.055401235818862915, 0.003948417957872152, 0.009007328189909458, 0.1050032377243042, 0.004195193760097027, 0.0946386381983757, 0.0002467761223670095, 0.0007403283379971981, 0.0036245391238480806, 0.0006994724390096962, 0.11744778603315353, 0.024481534957885742, 0.0008902376866899431, 0.0031794202513992786, 0.002162005752325058, 0.008902376517653465, 0.00044511884334497154, 0.0008266492513939738, 0.7301220297813416, 0.01042849849909544, 0.09608207643032074, 0.00012717681238427758, 0.0006994724390096962, 0.003260646481066942, 0.9912365078926086, 0.9925945997238159, 0.0005324946832843125, 0.00026624734164215624, 0.017838571220636368, 0.0015974841080605984, 0.010916140861809254, 0.0007987420540302992, 0.0013312366791069508, 0.0013312366791069508, 0.0026624733582139015, 0.008253667503595352, 0.9015135169029236, 0.032748423516750336, 0.019702304154634476, 0.0027287097182124853, 0.9959790706634521, 0.9974960088729858, 0.0008748900145292282, 0.01691454090178013, 0.015164759941399097, 0.012977534905076027, 0.0005832600290887058, 0.0002916300145443529, 0.0004374450072646141, 0.0027704851236194372, 0.005686785094439983, 0.00014581500727217644, 0.6162142157554626, 0.0002916300145443529, 0.32691723108291626, 0.00014581500727217644, 0.0004374450072646141, 0.9906304478645325], \"Term\": [\"05_order\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"0_24\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10\", \"10c\", \"10day_homepage\", \"10k\", \"10k\", \"10k\", \"10x13\", \"10x13\", \"10x13\", \"10x13\", \"10x13\", \"10x13_poly\", \"10x13_poly\", \"10x13_polymailer\", \"10x13_polymailer\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12_month\", \"12x15.5\", \"12x15.5\", \"12x15.5_14x17\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"18_month\", \"1b\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\\\"x3_tablespoon\", \"2\\\"x3_tablespoon\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"20\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"21\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"26\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"27\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"28\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"29\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_bottom\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"2t-5t_shoe\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3\", \"3.4_oz\", \"3.4_oz\", \"3.4_oz\", \"3.4_oz\", \"3/6_month\", \"3ds\", \"3ds\", \"3ds\", \"3ds\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/coats_jacket\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/dresses\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/shoe\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4+)/top_t\", \"4th_generation\", \"4x8\", \"4x8\", \"4y\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"5\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"50\", \"5th_generation\", \"5th_generation\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5\", \"6.5x10\", \"6/9\", \"6/9_month\", \"6_6s\", \"6_6s\", \"6_6s\", \"6_6s\", \"6_6s\", \"6_6s\", \"6c\", \"6c\", \"6s\", \"6s\", \"6s\", \"6s\", \"6s\", \"6s\", \"6s\", \"6s\", \"6s\", \"6x9\", \"6x9\", \"6x9_7.5x10.5\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7.5\", \"7c\", \"7c\", \"7y\", \"7y\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8.5\", \"8c\", \"8c\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"9.5\", \"925\", \"925\", \"925\", \"925\", \"925\", \"925\", \"925_sterling\", \"925_sterling\", \"9c\", \"9x12\", \"9x12\", \"9x12_10x13\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"@mrscoolbreeze\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accent\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_hat\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_sunglass\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"accessory_wallet\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"action_figure\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adapter\", \"adult_coloring\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_jordan\", \"air_max\", \"air_max\", \"air_max\", \"alex_ani\", \"alex_ani\", \"alex_ani\", \"alex_ani\", \"alo_yoga\", \"amelia\", \"amelia\", \"amelia\", \"amelia\", \"amelia\", \"amelia\", \"amelia\", \"amelia\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"american_eagle\", \"anastasia_moonchild\", \"android\", \"android\", \"android\", \"android\", \"android\", \"android\", \"android\", \"android\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"animal_plush\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_pant\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_short\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"apparel_tracksuit\", \"appearance_fine\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple_watch\", \"apple_watch\", \"apple_watch\", \"apple_watch\", \"apple_watch\", \"apple_watch\", \"apple_watch\", \"apple_watch\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"art_craft\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"athletic_apparel\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"authentic\", \"avent\", \"aviator\", \"aviator\", \"aviator\", \"aviator\", \"aviator\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby\", \"baby_gap\", \"baby_gap\", \"baby_gap\", \"baby_gap\", \"baby_gap\", \"baby_gap\", \"baby_gap\", \"baby_gap\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"backpack_style\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"bag\", \"banana_powder\", \"banana_powder\", \"bandeau\", \"bandeau\", \"bandeau\", \"bandeau\", \"bangle\", \"bangle\", \"bangle\", \"bangle\", \"bangle\", \"bangle\", \"bangle\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"barbie\", \"baseball_cap\", \"baseball_cap\", \"baseball_cap\", \"baseball_softball\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_body\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bath_bomb\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"bathing_suit\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"batman\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battery\", \"battlefield\", \"beach_towel\", \"beach_towel\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie\", \"beanie_slouchy\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beast\", \"beat_dr\", \"beat_dre\", \"beat_dre\", \"beat_solo\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_fragrance\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_makeup\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beauty_skin\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer_stein\", \"bellami\", \"bellami\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"belt\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"betsey_johnson\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini\", \"bikini_bottom\", \"bikini_bottom\", \"bio_entire\", \"birkenstock\", \"birkenstock\", \"birkenstock\", \"birkenstock\", \"birkenstock\", \"birkenstock\", \"bitty_baby\", \"bitty_baby\", \"bitty_baby\", \"bitty_baby\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blackhead\", \"blackhead\", \"blackhead_remover\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blanket\", \"blazer\", \"blazer\", \"blazer\", \"blazer\", \"blazer\", \"blazer\", \"blazer\", \"blazer\", \"blazer\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_t\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tank\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blouse_tunic\", \"blu_ray\", \"blu_ray\", \"blu_ray\", \"blu_ray\", \"blu_ray\", \"blu_ray\", \"blu_ray\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"blue\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"bluetooth\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"blush\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"bnwt\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"bodycon\", \"bodycon\", \"bodycon\", \"bodycon\", \"bodycon\", \"bodycon\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bodysuit\", \"bomber_jacket\", \"bomber_jacket\", \"bomber_jacket\", \"bomber_jacket\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book\", \"book_education\", \"book_education\", \"book_education\", \"book_education\", \"book_literature\", \"book_literature\", \"book_literature\", \"book_literature\", \"book_literature\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"boot_cut\", \"bootcut\", \"bootcut\", \"bootcut\", \"bootcut\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"bootie\", \"borax\", \"borax\", \"borax_borax\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle_feeding\", \"bottle_feeding\", \"bottle_feeding\", \"bottle_feeding\", \"bottle_feeding\", \"bottle_feeding\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"bowl\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"box\", \"boxer_brief\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bra\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bracelet\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"bralette\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brand_new\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brandy_melville\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brow\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"brush\", \"bubble_mailer\", \"bubble_mailer\", \"bubble_mailer\", \"bubble_mailer\", \"bubble_mailer\", \"bubble_mailer\", \"bubblegum_slime\", \"buckle_trace\", \"building_toy\", \"building_toy\", \"building_toy\", \"building_toy\", \"building_toy\", \"bulb\", \"bulb\", \"bulb\", \"bulb\", \"bulb\", \"bulb\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"bundle\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"business_day\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"bust\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"button\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"buyer\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable\", \"cable_adapter\", \"cable_adapter\", \"cable_adapter\", \"cable_adapter\", \"cable_adapter\", \"cable_adapter\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"camera\", \"campus_tee\", \"campus_tee\", \"campus_tee\", \"campus_tee\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle\", \"candle_holder\", \"candle_holder\", \"candle_holder\", \"candle_holder\", \"candle_holder\", \"candle_holder\", \"candle_holder\", \"candle_holder\", \"candy_land\", \"canister\", \"canister\", \"canister\", \"canister\", \"canister\", \"canister\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"cap\", \"carat\", \"carat\", \"carat_weight\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"card\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"cardigan\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_body\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"care_face\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"cargo\", \"carly\", \"carly\", \"carly\", \"carly\", \"carly\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"carter\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"case_skin\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"cell_phone\", \"chaco\", \"chaco\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger\", \"charger_cradle\", \"charger_cradle\", \"charger_cradle\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"charm\", \"chase\", \"chase\", \"chase\", \"chase\", \"chase\", \"cheer_bow\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"child_place\", \"chip_crack\", \"chip_crack\", \"chip_crack\", \"chip_crack\", \"chip_crack\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker\", \"choker_concerned\", \"choker_liking\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"christma\", \"chunky_heel\", \"cib\", \"claim_usp\", \"clarin\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"cleaning_supply\", \"clear_lens\", \"closet_rule\", \"clubmaster\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"clutch\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coach\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coat_jacket\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee_tea\", \"coffee_tea\", \"coffee_tea\", \"coffee_tea\", \"coffee_tea\", \"coin_purse\", \"coin_purse\", \"coin_purse\", \"coin_purse\", \"coin_purse\", \"collectible_figurine\", \"collectible_figurine\", \"collectible_figurine\", \"collectible_figurine\", \"collectible_figurine\", \"collectible_figurine\", \"cologne\", \"cologne\", \"cologne\", \"cologne\", \"cologne\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"comforter\", \"comforter\", \"comforter\", \"comforter\", \"comforter\", \"comforter\", \"comforter_set\", \"comforter_set\", \"comforter_set\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic\", \"comic_con\", \"comment_\\u2795\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compartment\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"complexion\", \"complexion\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"concealer\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"conditioner\", \"console\", \"console\", \"console\", \"console\", \"console\", \"console\", \"console\", \"console\", \"console\", \"console\", \"contact_bad\", \"contact_bad\", \"contact_bad\", \"container_dryness\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"converse\", \"converse\", \"converse\", \"converse\", \"converse\", \"converse\", \"converse\", \"converse\", \"converse\", \"converse\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"cosmetic_bag\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"costume\", \"cover_up\", \"cover_up\", \"cover_up\", \"cover_up\", \"creamer\", \"crest_3d\", \"crew_neck\", \"crew_neck\", \"crew_neck\", \"crew_neck\", \"crew_neck\", \"crew_neck\", \"crew_neck\", \"crewneck\", \"crewneck\", \"crewneck\", \"crewneck\", \"crewneck\", \"crewneck\", \"crewneck\", \"crocs\", \"crocs\", \"crocs\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"crop\", \"cross_body\", \"cross_body\", \"cross_body\", \"cross_body\", \"cross_body\", \"cross_body\", \"cross_body\", \"cross_body\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crossbody\", \"crunchy\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"crystal\", \"cubic_zirconia\", \"cubic_zirconia\", \"cubic_zirconia\", \"cubic_zirconia\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"curtain_panel\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cute\", \"cz\", \"cz\", \"cz\", \"cz\", \"cz\", \"dad_hat\", \"dad_hat\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"daily_travel\", \"dansko\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark_circle\", \"david_bridal\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal\", \"decal_sticker\", \"decal_sticker\", \"decal_sticker\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"decor\", \"deduction\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"denim\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"description\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"device\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diamond\", \"diecast\", \"diffuser\", \"diffuser\", \"diffuser\", \"diffuser\", \"diffuser\", \"diffuser\", \"diffuser\", \"dining_bakeware\", \"dining_bakeware\", \"dining_bakeware\", \"dining_bakeware\", \"dining_bakeware\", \"dining_bakeware\", \"dining_bakeware\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_coffee\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_dining\", \"dining_entertaining\", \"dining_kitchen\", \"dining_kitchen\", \"dining_kitchen\", \"dining_kitchen\", \"dining_kitchen\", \"dining_kitchen\", \"dining_kitchen\", \"dining_kitchen\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"disc\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"discount\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"disney\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"distressed\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"doll_accessory\", \"dooney\", \"dooney\", \"dooney\", \"dooney\", \"dooney_bourke\", \"dooney_bourke\", \"dooney_bourke\", \"dooney_bourke\", \"dooney_bourke\", \"dooney_bourke\", \"dorbz\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress\", \"dress_asymmetrical\", \"dress_asymmetrical\", \"dress_asymmetrical\", \"dress_asymmetrical\", \"dress_asymmetrical\", \"dress_asymmetrical\", \"dress_asymmetrical\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"dress_knee\", \"ds\", \"ds\", \"ds\", \"ds\", \"ds\", \"ds\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"duffle_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"dust_bag\", \"duvet\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"dvd\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"d\\u00e9cor\", \"earphone\", \"earphone\", \"earphone\", \"earphone\", \"earphone\", \"earphone\", \"earphone\", \"earphone\", \"earphone\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"earring\", \"eau\", \"eau\", \"eau\", \"eau_de\", \"eau_de\", \"eau_de\", \"eau_de\", \"edp\", \"edt\", \"electric_wax\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_cell\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"electronic_video\", \"elmer_glue\", \"enfamil\", \"engagement\", \"engagement\", \"engagement_ring\", \"engagement_wedding\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"entertaining\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope_u\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"equipment\", \"era\", \"era\", \"era\", \"era\", \"era\", \"era\", \"era\", \"espadrille\", \"etc_limit\", \"exc\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"exclusive\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eye_shadow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeliner\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow\", \"eyeshadow_palette\", \"eyeshadow_palette\", \"eyeshadow_palette\", \"eyeshadow_palette\", \"eyeshadow_palette\", \"eyeshadow_palette\", \"eyeshadow_palette\", \"fabletic\", \"fabletic\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"faced\", \"family_planning\", \"faux_nose\", \"faux_nose\", \"faux_nose\", \"fedex_dhl\", \"fedex_dhl\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fee\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"fiction\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"final_fantasy\", \"fine_line\", \"fishbowl_slime\", \"fitbit\", \"fitbit\", \"fitbit_charge\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flat\", \"flats\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"fleece_jacket\", \"floam\", \"floam\", \"floam\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"fluffy_slime\", \"flyknit\", \"foam_bead\", \"foam_bead\", \"following_shop\", \"footie\", \"footie\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"fragrance\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"free_shipping\", \"fridge_30\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"funko\", \"funko\", \"funko\", \"funko\", \"funko\", \"funko\", \"funko\", \"funko\", \"funko_dorbz\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"funko_pop\", \"g_string\", \"g_string\", \"g_string\", \"g_string\", \"galaxy_note\", \"galaxy_note\", \"galaxy_note\", \"galaxy_note\", \"galaxy_s6\", \"galaxy_s6\", \"galaxy_s7\", \"galaxy_s7\", \"galaxy_s7\", \"galaxy_s7\", \"galaxy_s7\", \"galaxy_s7\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"game_console\", \"gameboy\", \"gameboy\", \"gameboy\", \"gameboy_advance\", \"gamecube\", \"gamecube\", \"gamecube\", \"gamestop_exclusive\", \"garanimal\", \"garanimal\", \"garanimal\", \"garanimal\", \"garcinia_cambogia\", \"garland\", \"gb_ram\", \"gba\", \"gear_backpack\", \"gear_backpack\", \"gear_backpack\", \"gear_backpack\", \"gear_backpack\", \"gear_backpack\", \"gear_backpack\", \"gemstone\", \"gemstone\", \"gemstone\", \"gemstone\", \"gemstone\", \"gemstone\", \"gemstone\", \"geneva\", \"gerber\", \"gerber\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"girl\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass_pipe\", \"glass_pipe\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"gold_hardware\", \"golf\", \"golf\", \"golf\", \"golf\", \"golf\", \"golf\", \"golf\", \"golf\", \"golf\", \"golf\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"good_condition\", \"grand_theft\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"great_condition\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"gta\", \"guy_harvey\", \"gymboree\", \"gymboree\", \"gymboree\", \"gymboree\", \"gymboree\", \"gymboree\", \"gymshark\", \"gymshark\", \"h_x\", \"h_x\", \"h_x\", \"h_x\", \"h_x\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_care\", \"hair_scalp\", \"hair_scalp\", \"half_zip\", \"half_zip\", \"half_zip\", \"half_zip\", \"half_zip\", \"half_zip\", \"half_zip\", \"half_zip\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halloween_costume\", \"halter_bralette\", \"halter_bralette\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"handbag\", \"handbag\", \"handbag\", \"handbag\", \"handbag\", \"handbag\", \"handbag\", \"handbag\", \"handbag\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"handmade_paper\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"harley_quinn\", \"harley_quinn\", \"harley_quinn\", \"harley_quinn\", \"harley_quinn\", \"harley_quinn\", \"harley_quinn\", \"harley_quinn\", \"harlow\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"hat\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headband\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headphone\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"headset\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"heel\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"hello_kitty\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"highlighter\", \"hobo\", \"hobo\", \"hobo\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"hollister\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_d\\u00e9cor\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_kitchen\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"home_seasonal\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hoodie_sweatshirt\", \"hookah\", \"hoop\", \"hoop\", \"hoop\", \"hoop\", \"hoop\", \"hoop\", \"hoop\", \"hoop\", \"hoop_earring\", \"hoop_earring\", \"hoop_earring\", \"hot_wheel\", \"hot_wheel\", \"hot_wheel\", \"hot_wheel\", \"house_lash\", \"house_lash\", \"house_lash\", \"house_lash\", \"household_cleaner\", \"household_cleaner\", \"household_cleaner\", \"household_cleaner\", \"household_cleaner\", \"houseware_bowl\", \"houseware_ceramic\", \"htf_unicorn\", \"htf_unicorn\", \"huarache\", \"hygienic_liner\", \"idea_range\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"inch\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"infant\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam\", \"inseam_31\", \"inseam_31\", \"inseam_31\", \"inseam_31\", \"insole\", \"insole\", \"insole\", \"instax_mini\", \"intel_core\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"interior\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad\", \"ipad_air\", \"ipad_air\", \"ipad_mini\", \"ipad_mini\", \"ipad_mini\", \"ipad_mini\", \"ipad_mini\", \"ipad_mini\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone\", \"iphone_5/5s\", \"iphone_5/5s\", \"iphone_5/5s\", \"iphone_5c\", \"iphone_5c\", \"iphone_5c\", \"iphone_5c\", \"iphone_5c\", \"iphone_5c\", \"iphone_5s\", \"iphone_5s\", \"iphone_5s\", \"iphone_5s\", \"iphone_5s\", \"iphone_5s\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6/6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_6s\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"iphone_7\", \"ipod\", \"ipod\", \"ipod\", \"ipod\", \"ipod\", \"ipod\", \"ipod\", \"ipod_nano\", \"ipod_nano\", \"ipod_nano\", \"ipod_touch\", \"ipod_touch\", \"ipod_touch\", \"ipod_touch\", \"ipod_touch\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item_health\", \"item_health\", \"item_health\", \"item_health\", \"item_health\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"item_personal\", \"itsy\", \"ivory_ella\", \"ivory_ella\", \"ivory_ella\", \"ivory_ella\", \"ivory_ella\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jacket\", \"jamberry\", \"jamberry\", \"jamberry_nail\", \"janie_jack\", \"janie_jack\", \"janie_jack\", \"janie_jack\", \"janie_jack\", \"janie_jack\", \"janie_jack\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jean_slim\", \"jeans\", \"jeans\", \"jeffree_star\", \"jeffree_star\", \"jeffree_star\", \"jeffree_star\", \"jeffree_star\", \"jeffree_star\", \"jegging\", \"jegging\", \"jegging\", \"jegging\", \"jegging\", \"jegging\", \"jegging\", \"jegging\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jersey\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry\", \"jewelry_[rm\", \"jewelry_earring\", \"jewelry_earring\", \"jewelry_ring\", \"jewelry_ring\", \"john_galt\", \"joker\", \"joker\", \"joker\", \"jolyn\", \"jolyn\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"jordan\", \"julia\", \"julia\", \"julia\", \"julia\", \"julia\", \"julia\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kate_spade\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"kendra_scott\", \"keratin\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_boy\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_girl\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kid_toy\", \"kimono\", \"kimono\", \"kimono\", \"kimono\", \"kimono\", \"kimono\", \"kimono\", \"kimono\", \"kimono\", \"kitchen_dining\", \"kitchen_dining\", \"kitchen_dining\", \"kitchen_dining\", \"kitchen_dining\", \"kitchen_dining\", \"koko_kollection\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kylie\", \"kyshadow\", \"kyshadow\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lace\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"lanyard\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lash\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"lauren\", \"learning_education\", \"learning_education\", \"learning_education\", \"learning_education\", \"learning_education\", \"learning_education\", \"learning_education\", \"learning_education\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leather\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"legging\", \"lego\", \"lego\", \"lego\", \"lego\", \"lego\", \"lego\", \"lego\", \"lego\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"lens\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"levi\", \"lid_02\", \"lifeproof\", \"lifeproof\", \"lifeproof\", \"lifeproof\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"light\", \"line_wrinkle\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lingerie\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip\", \"lip_kit\", \"lip_kit\", \"lip_kit\", \"lip_kit\", \"lip_kit\", \"lip_kit\", \"lip_kit\", \"lip_smacker\", \"lip_smacker\", \"lipliner\", \"lipsense\", \"lipsense\", \"lipsense\", \"lipsense\", \"lipsense\", \"lipsense\", \"lipsense\", \"lipsense\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"lipstick\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid\", \"liquid_lipstick\", \"liquid_lipstick\", \"liquid_lipstick\", \"liquid_lipstick\", \"liquid_lipstick\", \"liquid_lipstick\", \"liquid_lipstick\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"listing\", \"llr\", \"llr\", \"llr\", \"llr\", \"llr\", \"llr\", \"llr\", \"llr\", \"llr\", \"llr\", \"lokai_bracelet\", \"lokai_bracelet\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"lotion\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"louis_vuitton\", \"love_lemon\", \"love_lemon\", \"love_lemon\", \"low_advance.\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lp\", \"lps\", \"lps\", \"lps\", \"lps\", \"lps\", \"lps\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe\", \"lularoe_cassie\", \"lularoe_cassie\", \"lularoe_julia\", \"lularoe_julia\", \"lularoe_julia\", \"lularoe_randy\", \"lularoe_randy\", \"lularoe_randy\", \"lularoe_randy\", \"lularoe_randy\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lululemon\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"mac\", \"magnetic_snap\", \"magnetic_snap\", \"mailer\", \"mailer\", \"mailer\", \"mailer\", \"mailer\", \"mailer\", \"mailer_office\", \"mailer_office\", \"main_compartment\", \"main_compartment\", \"major_unicorn\", \"major_unicorn\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_brush\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"makeup_palette\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"man_sweat\", \"manicure\", \"mario_kart\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mascara\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"mask\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"material\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"maternity\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte\", \"matte_lipstick\", \"matte_lipstick\", \"matte_lipstick\", \"matte_lipstick\", \"matte_lipstick\", \"matte_lipstick\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"maxi\", \"medical_supply\", \"medical_supply\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"medium_dvd\", \"melter_30\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"mercarii_policy\", \"message_choice\", \"message_choice\", \"message_choice\", \"messenger\", \"messenger\", \"messenger\", \"messenger\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"messenger_crossbody\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"michael_kor\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"mickey_mouse\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"mini\", \"minifigure\", \"mink\", \"mink\", \"mink\", \"mink\", \"mink\", \"mink\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mist\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mm\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"mo_piece\", \"modeling_\\u26d4\", \"monokini\", \"monster_high\", \"monster_high\", \"monster_high\", \"monster_high\", \"monster_high\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"month\", \"moonlight_path\", \"moroccan_oil\", \"morphe\", \"morphe\", \"morphe\", \"morphe\", \"morphe\", \"morphe\", \"morphe\", \"morphe\", \"mosaic_bear\", \"mud_mask\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mug\", \"mule_clog\", \"mule_clog\", \"multifunction_pocket\", \"mumu\", \"murano\", \"music_vinyl\", \"music_vinyl\", \"music_vinyl\", \"music_vinyl\", \"n64\", \"n64_-sega\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"nail_polish\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"naked\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"necklace\", \"nes\", \"nes\", \"nes\", \"netbook\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"new_tag\", \"newborn\", \"newborn\", \"newborn\", \"newborn\", \"newborn\", \"newborn\", \"newborn\", \"newborn\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nfl\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike\", \"nike_roshe\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo\", \"nintendo_3ds\", \"nintendo_3ds\", \"nintendo_64\", \"nintendo_64\", \"nintendo_64\", \"nintendo_64\", \"nintendo_ds\", \"nintendo_ds\", \"nintendo_gamecube\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"north_face\", \"nose_ring\", \"nose_ring\", \"nose_ring\", \"nose_ring\", \"nursery_bedding\", \"nursery_bedding\", \"nursery_bedding\", \"nursery_bedding\", \"nursery_bedding\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nwt\", \"nycc\", \"nyx\", \"nyx\", \"nyx\", \"nyx\", \"nyx\", \"nyx\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"office_supply\", \"oil_rig\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onesie\", \"onsie\", \"onsie\", \"onsie\", \"oral_b\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order_status\", \"order_status\", \"order_status\", \"order_status\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"organizer\", \"oribe\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"ornament\", \"otter_box\", \"otter_box\", \"otterbox\", \"otterbox\", \"otterbox_defender\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"outfit\", \"oval_plate\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"oz\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"package\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pair\", \"pairs-[rm\", \"palazzo\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"palette\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora\", \"pandora_charm\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pant\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"pantie\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"panty\", \"paperback\", \"paperback\", \"paperback\", \"parfum\", \"parfum\", \"parfum\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pcs\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"pearl\", \"peel_mask\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"perfume\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"pet_supply\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"phone_accessory\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"pigment\", \"pigment\", \"pigment\", \"pigment\", \"pigment\", \"pigmentation\", \"pigmented\", \"pigmented\", \"pigmented\", \"pigmented\", \"pigmented\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pink\", \"pipe\", \"pipe\", \"pipe\", \"planner\", \"planner\", \"planner\", \"planner\", \"planner\", \"planner\", \"planner\", \"planner\", \"planner\", \"planner\", \"planter\", \"planter\", \"planter\", \"planter\", \"platter\", \"platter\", \"platter\", \"playstation\", \"playstation\", \"playstation\", \"playstation\", \"playstation\", \"playstation\", \"pleasant_company\", \"plus/6s\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"plush\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pokemon_card\", \"pokemon_card\", \"pokemon_card\", \"pokemon_card\", \"pokemon_card\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polish\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_ralph\", \"polo_rugby\", \"polo_rugby\", \"polo_rugby\", \"polo_rugby\", \"polo_rugby\", \"poly_mailer\", \"poly_mailer\", \"poly_mailer\", \"poly_mailer\", \"poly_mailer\", \"polymailer\", \"polymailer\", \"polymailer\", \"polymailer\", \"polymailer_office\", \"polymailer_office\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop\", \"pop_expandable\", \"popsocket\", \"popsocket\", \"popsocket\", \"popsocket\", \"pore\", \"pore\", \"pore\", \"pore\", \"pore_blackhead\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"post_office\", \"postage_\\u2666\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"powder\", \"pregnancy_test\", \"pressure_sensitive\", \"pressure_sensitive\", \"pressure_sensitive\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"price_firm\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"primer\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"print\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product_menu\", \"product_menu\", \"product_menu\", \"prom\", \"prom\", \"prom\", \"prom\", \"prom\", \"prom\", \"prom\", \"proof_measure\", \"proof_measure\", \"ps1\", \"ps2\", \"ps2\", \"ps3\", \"ps3\", \"ps3\", \"ps3\", \"ps4\", \"ps4\", \"ps4\", \"ps4\", \"ps4\", \"ps4\", \"ps4\", \"ps4\", \"puffer\", \"puffer\", \"puffer\", \"puffer\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"pullover\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase_progress\", \"purchase_progress\", \"purchase_progress\", \"pure_romance\", \"purple_grass\", \"purple_grass\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"purse\", \"puzzle\", \"puzzle\", \"puzzle\", \"puzzle\", \"puzzle\", \"puzzle\", \"puzzle\", \"puzzle\", \"pyrex\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae\", \"rae_bake\", \"rae_dunn\", \"rae_dunn\", \"rae_dunn\", \"rae_dunn\", \"rae_dunn\", \"rae_dunn\", \"rae_flour\", \"rain_boot\", \"rain_boot\", \"rain_boot\", \"rain_boot\", \"raincoat\", \"raincoat\", \"raincoat\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"ralph_lauren\", \"random_polish\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"ray_ban\", \"ray_ban\", \"ray_ban\", \"ray_ban\", \"ray_ban\", \"ray_ban\", \"rayne\", \"rd\", \"rd\", \"rd_minor\", \"ready_disclaimer\", \"redken\", \"redness\", \"refuge\", \"respectful_policy\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible\", \"responsible_breakage\", \"responsible_damage\", \"responsible_damage\", \"responsible_damage\", \"responsible_damage\", \"responsible_damage\", \"returnss_incorrect\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring_navel\", \"ring_navel\", \"ringer_tee\", \"ringer_tee\", \"ringer_tee\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rm\", \"rock_revival\", \"rock_revival\", \"rock_revival\", \"rock_revival\", \"rock_revival\", \"rock_revival\", \"rock_revival\", \"rock_revival\", \"rollerball\", \"rollerball\", \"rollerball\", \"roshe\", \"saffiano_leather\", \"saffiano_leather\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"sale_final\", \"san_lorenzo\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"sandal\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"satchel\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scarf\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scent\", \"scented_goodness\", \"scentsy\", \"scentsy\", \"scentsy\", \"scentsy\", \"scoop_equal\", \"scoop_equal\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"screen_protector\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"scrub\", \"sdcc\", \"sdcc\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"sega_genesis\", \"self_adhesive\", \"self_adhesive\", \"self_adhesive\", \"self_adhesive\", \"self_adhesive\", \"self_adhesive\", \"self_adhesive\", \"self_seal\", \"self_seal\", \"self_seal\", \"selma\", \"selma\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"sexy\", \"shabby_chic\", \"shabby_chic\", \"shabby_chic\", \"shabby_chic\", \"shabby_chic\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shade\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shadow\", \"shampoo\", \"shampoo\", \"shampoo\", \"shampoo\", \"shampoo\", \"shampoo\", \"shampoo\", \"shampoo\", \"shampoo_conditioner\", \"shampoo_conditioner\", \"shampoo_conditioner\", \"shampoo_conditioner\", \"shampoo_conditioner\", \"shampoo_conditioner\", \"shampoo_conditioner\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet\", \"sheet_jamberry\", \"sherpa_blanket\", \"sherri_hill\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship\", \"ship_uship\", \"shipment_weather\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\", \"shipping\\u2022_\\u2764\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shirt\", \"shockproof\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_boot\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_fashion\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_loafer\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_pump\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shoe_sandal\", \"shop_mlb\", \"shop_mlb\", \"shop_mlb\", \"shop_mlb\", \"shop_mlb\", \"shop_nba\", \"shop_nba\", \"shop_nba\", \"shop_nba\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shop_nfl\", \"shopkin\", \"shopkin\", \"shopkin\", \"shopkin\", \"shopkin\", \"shopkin\", \"shopkin\", \"shopkin\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"silver\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"sizing-_10/12\", \"skecher\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skinny_jean\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt\", \"skirt_skort\", \"skirt_skort\", \"skirt_skort\", \"skirt_straight\", \"skirt_straight\", \"skirt_straight\", \"skirt_straight\", \"skirt_straight\", \"sleep_sack\", \"sleeper\", \"sleeper\", \"sleeper\", \"sleeper\", \"sleeper\", \"sleeper\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime\", \"slime_borax\", \"slime_sticky\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slip_on\", \"slipper\", \"slipper\", \"slipper\", \"slipper\", \"slipper\", \"slipper\", \"slipper\", \"slipper\", \"slipper\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smartphone\", \"smashbox\", \"smashbox\", \"smashbox\", \"smashbox\", \"smashbox\", \"smashbox\", \"snapback\", \"snapback\", \"snapback\", \"sne\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"sneaker\", \"snowsuit\", \"snowsuit\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sock\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sol\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"sole\", \"solitaire\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"soy_wax\", \"soy_wax\", \"spacious\", \"spacious\", \"spacious\", \"speaker_subwoofer\", \"sperry\", \"sperry\", \"sperry\", \"sperry\", \"sperry\", \"sperry\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spinner\", \"spoon_pipe\", \"spoon_rest\", \"spoon_rest\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_bra\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_fan\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"sport_team\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"sprinkle\", \"sprinkle\", \"sprinkle\", \"sprinkle\", \"sprinkle_concentrated\", \"squishie\", \"squishie\", \"squishie\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"statue\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sterling_silver\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"sticker\", \"stila\", \"stila\", \"stila\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"storage_organization\", \"straight_leg\", \"straight_leg\", \"straight_leg\", \"straight_leg\", \"straight_leg\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strap\", \"strapback\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretch\", \"stretchy_noise\", \"stud_earring\", \"stud_earring\", \"stud_earring\", \"stud_earring\", \"styling_product\", \"styling_product\", \"styling_product\", \"styling_product\", \"styling_product\", \"sugar_canister\", \"suit_blazer\", \"suit_blazer\", \"suit_blazer\", \"suit_blazer\", \"suit_blazer\", \"sundress\", \"sundress\", \"sundress\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunglass\", \"sunnie\", \"super_mario\", \"super_mario\", \"super_mario\", \"super_mario\", \"super_mario\", \"super_mario\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweat\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_cardigan\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweater_crewneck\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweatpant\", \"sweet_peach\", \"sweet_peach\", \"sweet_peach\", \"swim\", \"swim\", \"swim\", \"swim\", \"swim\", \"swim\", \"swim\", \"swim\", \"swim\", \"swim\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimsuit\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"swimwear_piece\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"t_shirt\", \"tablet_drive\", \"tablet_drive\", \"tablet_laptop\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tag\", \"tall_curvy\", \"tall_curvy\", \"tall_curvy\", \"tall_curvy\", \"tall_curvy\", \"tall_curvy\", \"tall_curvy\", \"tall_curvy\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tank\", \"tapestry\", \"tapestry\", \"tapestry\", \"tapestry\", \"tapestry\", \"tapestry\", \"tc\", \"tc\", \"tc\", \"tc\", \"tc\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"tea_accessory\", \"teaching\", \"teaching\", \"teaching\", \"teaching\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"tee\", \"temperature_resistant\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thirty\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"thong\", \"throw_blanket\", \"thumbhole\", \"tide_pod\", \"tide_pod\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"tight_legging\", \"timberland\", \"timberland\", \"timberland\", \"timberland\", \"timberland\", \"timberland\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toe\", \"toilette\", \"toilette\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tom\", \"tone_hardware\", \"tone_hardware\", \"tone_hardware\", \"tooth_whitener\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"top_t\", \"topp\", \"topp\", \"topp\", \"topp\", \"topp\", \"total_carat\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"tote_shopper\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"toy_action\", \"tradess_\\u26d4\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"trading_card\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"transaction\", \"trench\", \"trench\", \"triangl\", \"triangl_bikini\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"true_religion\", \"tula\", \"tula\", \"tumbler_rambler\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg\", \"ugg_boot\", \"ugg_boot\", \"ugg_boot\", \"uggs\", \"uggs\", \"uggs\", \"uggs\", \"uggs\", \"uggs\", \"uggs\", \"uggs\", \"uggs\", \"ultra_matte\", \"ultra_shea\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unicorn\", \"unlocked\", \"unlocked\", \"unlocked\", \"unlocked\", \"unlocked\", \"up_fedex\", \"up_fedex\", \"up_fedex\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_decay\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"urban_outfitter\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usb\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp\", \"usp_tr.number\", \"usp_tr.number\", \"usp_up\", \"usp_up\", \"utensil_gadget\", \"utensil_gadget\", \"utensil_gadget\", \"utensil_gadget\", \"utensil_gadget\", \"utensil_gadget\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"v_neck\", \"vacuum_cup\", \"vacuum_cup\", \"vacuum_floor\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"van\", \"vape\", \"vape\", \"vape\", \"varsity_crew\", \"varsity_crew\", \"varsity_crew\", \"varsity_crew\", \"varsity_crew\", \"velvet_choker\", \"velvet_choker\", \"velvet_choker\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"vera_bradley\", \"versace\", \"versace\", \"versace\", \"versace\", \"versace\", \"versace\", \"versace\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"vest\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"victoria_secret\", \"video_gaming\", \"video_gaming\", \"video_gaming\", \"video_gaming\", \"video_gaming\", \"video_gps\", \"vietnam\", \"vietnam\", \"vietnam\", \"vietnam\", \"vietnam\", \"vietnam\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"vintage_collectible\", \"viva_la\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"waist\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall\", \"wall_art\", \"wall_art\", \"wall_art\", \"wall_decor\", \"wall_decor\", \"wall_decor\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallet\", \"wallflow\", \"wallflower\", \"wallflower\", \"wallflower\", \"warfare\", \"warmer\", \"warmer\", \"warmer\", \"warmer\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"wash\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax\", \"wax_melt\", \"wax_melt\", \"wax_melt\", \"wax_melt\", \"wax_melt\", \"wayfarer\", \"wayfarer\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedding\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"wedge\", \"weekender\", \"weekender\", \"weekender\", \"weft\", \"weft\", \"wen\", \"wen\", \"wen\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"wig\", \"wig\", \"wig\", \"wig\", \"wig\", \"wig\", \"wig\", \"wig\", \"wig\", \"wii\", \"wii\", \"wii\", \"wii\", \"windbreaker\", \"windbreaker\", \"windbreaker\", \"windbreaker\", \"windbreaker\", \"windbreaker\", \"windbreaker\", \"windbreaker\", \"window_treatment\", \"window_treatment\", \"window_treatment\", \"wispie\", \"wispie\", \"wispy\", \"wispy\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_athletic\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_handbag\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_jewelry\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_top\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"woman_underwear\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"workout\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"worn\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wristlet\", \"wunderbrow\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"x\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox\", \"xbox_360\", \"xbox_360\", \"xbox_360\", \"xbox_360\", \"xbox_360\", \"xbox_360\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"xl\", \"yeti_cup\", \"yeti_cup\", \"yeti_cup\", \"yeti_cup\", \"yeti_cup\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yoga\", \"yogas\", \"yogas\", \"zaful\", \"zebra_sprinkle\", \"zebra_sprinkle\", \"zebra_sprinkle\", \"zella\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zip\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zipper\", \"zippo\", \"\\u2550_\\u2550\", \"\\u2550_\\u2550\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa\", \"\\u25aa_\\ufe0fget\", \"\\u25c6_\\u25c7\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u2606_\\u2606\", \"\\u261e\", \"\\u261e\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2661\", \"\\u2665_cheap\", \"\\u2666_\\ufe0fi\", \"\\u2666_\\ufe0fperfect\", \"\\u2666_\\ufe0fup\", \"\\u2666_\\ufe0fwater\", \"\\u26d4_\\u26d4\", \"\\u26d4_\\ufe0flowball\", \"\\u26d4_\\ufe0fplease\", \"\\u26d4_\\ufe0frude\", \"\\u26d4_\\ufe0fwhen\", \"\\u26d4_\\ufe0fwhen\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2705\", \"\\u2714_\\ufe0f100\", \"\\u2714_\\ufe0feverything\", \"\\u2714_\\ufe0four\", \"\\u2714_\\ufe0fwe\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u2728_\\u2728\", \"\\u272f\", \"\\u2733_\\ufe0fdescription\", \"\\u2734_\\u2734\", \"\\u273d\", \"\\u2740\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u274c\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764\", \"\\u2764_patkat\", \"\\u2764_patkat\", \"\\u2764_tmfpolish\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2764_\\u2764\", \"\\u2800_\\u2800\", \"\\u2800_\\u2800\", \"\\ua565_\\u2728\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0f\", \"\\ufe0flight_weight\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el65711406768537248888532895063\", ldavis_el65711406768537248888532895063_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el65711406768537248888532895063\", ldavis_el65711406768537248888532895063_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el65711406768537248888532895063\", ldavis_el65711406768537248888532895063_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.105992 -0.139042       1        1   6.541109\n",
       "1     -0.080590 -0.052679       2        1   4.451007\n",
       "2      0.150754 -0.028283       3        1  10.564756\n",
       "3      0.174307 -0.038508       4        1   6.533017\n",
       "4      0.058099  0.153225       5        1   5.707425\n",
       "5     -0.218605  0.020521       6        1   9.543053\n",
       "6     -0.064397 -0.147733       7        1   6.462804\n",
       "7     -0.218374  0.003631       8        1  10.656355\n",
       "8      0.149326 -0.026998       9        1   7.956305\n",
       "9      0.111801  0.087286      10        1   4.041940\n",
       "10     0.025020 -0.035209      11        1   4.071248\n",
       "11     0.132513  0.024881      12        1   4.923011\n",
       "12     0.106232 -0.082569      13        1   5.931906\n",
       "13    -0.067733  0.228336      14        1   4.515010\n",
       "14    -0.152363  0.033141      15        1   8.101048, topic_info=     Category           Freq              Term          Total  loglift  \\\n",
       "term                                                                     \n",
       "70    Default  371833.000000             woman  371833.000000  30.0000   \n",
       "106   Default  126240.000000               man  126240.000000  29.0000   \n",
       "441   Default  137716.000000             shirt  137716.000000  28.0000   \n",
       "466   Default   79502.000000             dress   79502.000000  27.0000   \n",
       "158   Default   93641.000000    woman_athletic   93641.000000  26.0000   \n",
       "80    Default   96818.000000     beauty_makeup   96818.000000  25.0000   \n",
       "313   Default   74787.000000              shoe   74787.000000  24.0000   \n",
       "248   Default   84257.000000         woman_top   84257.000000  23.0000   \n",
       "40    Default  387106.000000              size  387106.000000  22.0000   \n",
       "653   Default   72650.000000               bag   72650.000000  21.0000   \n",
       "320   Default   50174.000000              game   50174.000000  20.0000   \n",
       "272   Default   72642.000000              case   72642.000000  19.0000   \n",
       "5     Default   63318.000000           legging   63318.000000  18.0000   \n",
       "485   Default   51754.000000              jean   51754.000000  17.0000   \n",
       "55    Default  109416.000000              item  109416.000000  16.0000   \n",
       "6     Default   62746.000000           lularoe   62746.000000  15.0000   \n",
       "221   Default   46148.000000           kid_toy   46148.000000  14.0000   \n",
       "157   Default   45821.000000     tight_legging   45821.000000  13.0000   \n",
       "284   Default   42140.000000   phone_accessory   42140.000000  12.0000   \n",
       "153   Default   45669.000000      apparel_pant   45669.000000  11.0000   \n",
       "276   Default   42028.000000   electronic_cell   42028.000000  10.0000   \n",
       "186   Default  146487.000000                rm  146487.000000   9.0000   \n",
       "1522  Default   49439.000000            blouse   49439.000000   8.0000   \n",
       "356   Default   35102.000000     woman_handbag   35102.000000   7.0000   \n",
       "319   Default   29490.000000  electronic_video   29490.000000   6.0000   \n",
       "253   Default   29457.000000          bracelet   29457.000000   5.0000   \n",
       "321   Default   27865.000000      game_console   27865.000000   4.0000   \n",
       "86    Default   41907.000000               lip   41907.000000   3.0000   \n",
       "648   Default   48044.000000               bra   48044.000000   2.0000   \n",
       "395   Default   37460.000000            jacket   37460.000000   1.0000   \n",
       "...       ...            ...               ...            ...      ...   \n",
       "1774  Topic15    4009.228027           slipper    4198.949219   2.4669   \n",
       "2729  Topic15    9082.270508            jordan    9856.148438   2.4314   \n",
       "4557  Topic15    4561.125000               toe    4893.267578   2.4429   \n",
       "304   Topic15    5869.714844               8.5    6585.931152   2.3980   \n",
       "2267  Topic15    5538.535645               van    6191.299805   2.4018   \n",
       "2617  Topic15    2228.597412               ugg    2335.498047   2.4663   \n",
       "365   Topic15   35746.500000             short   53617.250000   2.1078   \n",
       "94    Topic15    5953.847656               7.5    7075.423828   2.3406   \n",
       "776   Topic15    3284.663086               9.5    3655.120117   2.4063   \n",
       "310   Topic15   20381.867188              nike   33993.164062   2.0017   \n",
       "615   Topic15    9527.129883              flat   14309.636719   2.1064   \n",
       "2981  Topic15    4203.908203               6.5    5015.729492   2.3366   \n",
       "40    Topic15   94700.867188              size  387106.156250   1.1052   \n",
       "70    Topic15   85635.257812             woman  371833.375000   1.0448   \n",
       "95    Topic15   19920.244141                 8   46613.984375   1.6630   \n",
       "609   Topic15   13197.373047                 9   26705.230469   1.8083   \n",
       "303   Topic15   16563.513672                 7   41244.117188   1.6009   \n",
       "3     Topic15   13899.423828           kid_boy   34642.156250   1.6000   \n",
       "106   Topic15   27772.062500               man  126240.750000   0.9990   \n",
       "51    Topic15   26473.933594             black  161154.312500   0.7070   \n",
       "939   Topic15   11925.276367          kid_girl   40554.660156   1.2892   \n",
       "348   Topic15   13443.303711    good_condition   56639.066406   1.0750   \n",
       "60    Topic15   21591.630859               new  221221.171875   0.1863   \n",
       "81    Topic15   14627.575195               box   81301.390625   0.7979   \n",
       "196   Topic15   12938.110352                 6   61824.484375   0.9491   \n",
       "42    Topic15   11719.156250                10   46750.167969   1.1296   \n",
       "233   Topic15   11225.846680              worn   42357.980469   1.1852   \n",
       "9     Topic15   13849.880859         brand_new  191853.937500  -0.1153   \n",
       "69    Topic15   11633.077148             white   81540.648438   0.5659   \n",
       "1     Topic15   10872.209961   great_condition   52103.281250   0.9462   \n",
       "\n",
       "      logprob  \n",
       "term           \n",
       "70    30.0000  \n",
       "106   29.0000  \n",
       "441   28.0000  \n",
       "466   27.0000  \n",
       "158   26.0000  \n",
       "80    25.0000  \n",
       "313   24.0000  \n",
       "248   23.0000  \n",
       "40    22.0000  \n",
       "653   21.0000  \n",
       "320   20.0000  \n",
       "272   19.0000  \n",
       "5     18.0000  \n",
       "485   17.0000  \n",
       "55    16.0000  \n",
       "6     15.0000  \n",
       "221   14.0000  \n",
       "157   13.0000  \n",
       "284   12.0000  \n",
       "153   11.0000  \n",
       "276   10.0000  \n",
       "186    9.0000  \n",
       "1522   8.0000  \n",
       "356    7.0000  \n",
       "319    6.0000  \n",
       "253    5.0000  \n",
       "321    4.0000  \n",
       "86     3.0000  \n",
       "648    2.0000  \n",
       "395    1.0000  \n",
       "...       ...  \n",
       "1774  -6.0517  \n",
       "2729  -5.2340  \n",
       "4557  -5.9227  \n",
       "304   -5.6705  \n",
       "2267  -5.7286  \n",
       "2617  -6.6389  \n",
       "365   -3.8638  \n",
       "94    -5.6563  \n",
       "776   -6.2510  \n",
       "310   -4.4257  \n",
       "615   -5.1862  \n",
       "2981  -6.0043  \n",
       "40    -2.8896  \n",
       "70    -2.9902  \n",
       "95    -4.4486  \n",
       "609   -4.8603  \n",
       "303   -4.6331  \n",
       "3     -4.8085  \n",
       "106   -4.1163  \n",
       "51    -4.1641  \n",
       "939   -4.9616  \n",
       "348   -4.8418  \n",
       "60    -4.3680  \n",
       "81    -4.7574  \n",
       "196   -4.8801  \n",
       "42    -4.9791  \n",
       "233   -5.0221  \n",
       "9     -4.8120  \n",
       "69    -4.9864  \n",
       "1     -5.0541  \n",
       "\n",
       "[1257 rows x 6 columns], token_table=       Topic      Freq           Term\n",
       "term                                 \n",
       "26870      1  0.988381       05_order\n",
       "481        1  0.000187           0_24\n",
       "481        2  0.000075           0_24\n",
       "481        3  0.000299           0_24\n",
       "481        4  0.000112           0_24\n",
       "481        5  0.979000           0_24\n",
       "481        6  0.000672           0_24\n",
       "481        7  0.000299           0_24\n",
       "481        8  0.000522           0_24\n",
       "481        9  0.000037           0_24\n",
       "481       10  0.000485           0_24\n",
       "481       11  0.000187           0_24\n",
       "481       12  0.000075           0_24\n",
       "481       13  0.000037           0_24\n",
       "481       14  0.000709           0_24\n",
       "481       15  0.017278           0_24\n",
       "92         1  0.025250              1\n",
       "92         2  0.039489              1\n",
       "92         3  0.188934              1\n",
       "92         4  0.141548              1\n",
       "92         5  0.149996              1\n",
       "92         6  0.007587              1\n",
       "92         7  0.029217              1\n",
       "92         8  0.016241              1\n",
       "92         9  0.114455              1\n",
       "92        10  0.019618              1\n",
       "92        11  0.035747              1\n",
       "92        12  0.050426              1\n",
       "92        13  0.120274              1\n",
       "92        14  0.024904              1\n",
       "...      ...       ...            ...\n",
       "4319       3  0.017839            _\n",
       "4319       4  0.001597            _\n",
       "4319       5  0.010916            _\n",
       "4319       6  0.000799            _\n",
       "4319       7  0.001331            _\n",
       "4319       8  0.001331            _\n",
       "4319       9  0.002662            _\n",
       "4319      10  0.008254            _\n",
       "4319      11  0.901514            _\n",
       "4319      12  0.032748            _\n",
       "4319      13  0.019702            _\n",
       "40333     11  0.002729            _\n",
       "40333     12  0.995979            _\n",
       "30847     11  0.997496            _\n",
       "2560       1  0.000875              \n",
       "2560       2  0.016915              \n",
       "2560       3  0.015165              \n",
       "2560       4  0.012978              \n",
       "2560       5  0.000583              \n",
       "2560       6  0.000292              \n",
       "2560       7  0.000437              \n",
       "2560       8  0.002770              \n",
       "2560       9  0.005687              \n",
       "2560      10  0.000146              \n",
       "2560      11  0.616214              \n",
       "2560      12  0.000292              \n",
       "2560      13  0.326917              \n",
       "2560      14  0.000146              \n",
       "2560      15  0.000437              \n",
       "2018      13  0.990630  light_weight\n",
       "\n",
       "[8556 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG : Setting topics to those of the model: LdaModel(num_terms=329852, num_topics=15, decay=0.5, chunksize=8000)\n",
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 10000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 11000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 12000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 13000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 14000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 15000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 16000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 17000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 18000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 19000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 20000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 21000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 22000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 23000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 24000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 25000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 26000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 27000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 28000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 29000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 30000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 31000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 32000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 33000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 34000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 35000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 36000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 37000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 38000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 39000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 40000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 41000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 42000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 43000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 44000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 45000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 46000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 47000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 48000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 49000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 50000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 51000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 52000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 53000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 54000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 55000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 56000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 57000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 58000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 59000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 60000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 61000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 62000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 63000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 64000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 65000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 66000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 67000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 68000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 69000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 70000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 71000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 72000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 73000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 74000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 75000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 76000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 77000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 78000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 79000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 80000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 81000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 82000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 83000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 84000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 85000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 86000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 87000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 88000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 89000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 90000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 91000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 92000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 93000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 94000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 95000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 96000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 97000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 98000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 99000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 126000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 139000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 186000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 187000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 188000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 189000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 190000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 191000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 192000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 193000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 194000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 195000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 196000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 197000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 198000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 199000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 200000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 201000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 202000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 203000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 204000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 205000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 206000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 207000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 208000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 209000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 210000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 211000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 212000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 213000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 214000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 215000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 216000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 217000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 218000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 219000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 220000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 221000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 222000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 223000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 224000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 225000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 226000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 227000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 228000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 229000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 230000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 231000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 232000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 233000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 234000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 235000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 236000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 237000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 238000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 239000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 240000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 241000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 242000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 243000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 244000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 245000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 246000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 247000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 248000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 249000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 250000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 251000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 252000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 253000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 254000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 255000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 256000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 257000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 258000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 259000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 260000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 261000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 262000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 263000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 264000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 265000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 266000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 267000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 268000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 269000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 270000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 271000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 272000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 273000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 274000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 275000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 276000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 277000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 278000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 279000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 280000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 281000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 282000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 283000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 284000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 285000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 286000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 287000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 288000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 289000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 290000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 291000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 292000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 293000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 294000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 295000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 296000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 297000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 298000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 299000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 300000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 301000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 302000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 303000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 304000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 305000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 306000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 307000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 308000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 309000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 310000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 311000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 312000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 313000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 314000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 315000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 316000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 317000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 318000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 319000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 320000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 321000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 322000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 323000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 324000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 325000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 326000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 327000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 328000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 329000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 330000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 331000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 332000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 333000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 334000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 335000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 336000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 337000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 338000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 339000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 340000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 341000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 342000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 343000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 344000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 345000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 346000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 347000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 348000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 349000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 350000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 351000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 352000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 353000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 354000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 355000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 356000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 357000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 358000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 359000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 360000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 361000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 362000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 363000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 364000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 365000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 366000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 367000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 368000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 369000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 370000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 371000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 372000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 373000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 374000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 375000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 376000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 377000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 378000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 379000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 380000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 381000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 382000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 383000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 384000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 385000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 386000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 387000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 388000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 389000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 390000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 391000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 392000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 393000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 394000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 395000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 396000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 397000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 398000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 399000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 400000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 401000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 402000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 403000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 404000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 405000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 406000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 407000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 408000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 409000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 410000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 411000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 412000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 413000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 414000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 415000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 416000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 417000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 418000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 419000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 420000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 421000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 422000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 423000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 424000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 425000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 426000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 427000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 428000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 429000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 430000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 431000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 432000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 433000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 434000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 435000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 436000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 437000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 438000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 439000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 440000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 441000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 442000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 443000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 444000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 445000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 446000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 447000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 448000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 449000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 450000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 451000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 452000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 453000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 454000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 455000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 456000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 457000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 458000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 459000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 460000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 461000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 462000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 463000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 464000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 465000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 466000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 467000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 468000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 469000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 470000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 471000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 472000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 473000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 474000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 475000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 476000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 477000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 478000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 479000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 480000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 481000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 482000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 483000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 484000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 485000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 486000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 487000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 488000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 489000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 490000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 491000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 492000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 493000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 494000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 495000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 496000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 497000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 498000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 499000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 500000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 501000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 502000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 503000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 504000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 505000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 506000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 507000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 508000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 509000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 510000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 511000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 512000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 513000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 514000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 515000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 516000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 517000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 518000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 519000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 520000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 521000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 522000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 523000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 524000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 525000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 526000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 527000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 528000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 529000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 530000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 531000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 532000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 533000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 534000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 535000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 536000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 537000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 538000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 539000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 540000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 541000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 542000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 543000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 544000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 545000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 546000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 547000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 548000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 549000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 550000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 551000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 552000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 553000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 554000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 555000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 556000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 557000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 558000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 559000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 560000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 561000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 562000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 563000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 564000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 565000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 566000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 567000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 568000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 569000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 570000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 571000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 572000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 573000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 574000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 575000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 576000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 577000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 578000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 579000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 580000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 581000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 582000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 583000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 584000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 585000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 586000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 587000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 588000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 589000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 590000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 591000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 592000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 593000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 594000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 595000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 596000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 597000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 598000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 599000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 600000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 601000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 602000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 603000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 604000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 605000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 606000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 607000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 608000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 609000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 610000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 611000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 612000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 613000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 614000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 615000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 616000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 617000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 618000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 619000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 620000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 621000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 622000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 623000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 624000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 625000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 626000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 627000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 628000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 629000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 630000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 631000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 632000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 633000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 634000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 635000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 636000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 637000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 638000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 639000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 640000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 641000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 642000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 643000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 644000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 645000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 646000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 647000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 648000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 649000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 650000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 651000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 652000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 653000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 654000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 655000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 656000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 657000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 658000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 659000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 660000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 661000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 662000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 663000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 664000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 665000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 666000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 667000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 668000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 669000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 670000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 671000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 672000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 673000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 674000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 675000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 676000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 677000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 678000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 679000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 680000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 681000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 682000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 683000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 684000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 685000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 686000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 687000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 688000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 689000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 690000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 691000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 692000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 693000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 694000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 695000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 696000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 697000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 698000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 699000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 700000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 701000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 702000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 703000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 704000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 705000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 706000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 707000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 708000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 709000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 710000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 711000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 712000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 713000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 714000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 715000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 716000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 717000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 718000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 719000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 720000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 721000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 722000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 723000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 724000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 725000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 726000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 727000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 728000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 729000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 730000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 731000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 732000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 733000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 734000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 735000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 736000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 737000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 738000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 739000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 740000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 741000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 742000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 743000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 744000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 745000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 746000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 747000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 748000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 749000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 750000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 751000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 752000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 753000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 754000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 755000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 756000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 757000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 758000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 759000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 760000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 761000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 762000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 763000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 764000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 765000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 766000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 767000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 768000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 769000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 770000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 771000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 772000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 773000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 774000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 775000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 776000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 777000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 778000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 779000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 780000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 781000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 782000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 783000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 784000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 785000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 786000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 787000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 788000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 789000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 790000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 791000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 792000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 793000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 794000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 795000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 796000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 797000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 798000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 799000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 800000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 801000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 802000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 803000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 804000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 805000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 806000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 807000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 808000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 809000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 810000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 811000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 812000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 813000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 814000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 815000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 816000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 817000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 818000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 819000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 820000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 821000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 822000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 823000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 824000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 825000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 826000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 827000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 828000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 829000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 830000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 831000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 832000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 833000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 834000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 835000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 836000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 837000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 838000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 839000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 840000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 841000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 842000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 843000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 844000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 845000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 846000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 847000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 848000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 849000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 850000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 851000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 852000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 853000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 854000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 855000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 856000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 857000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 858000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 859000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 860000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 861000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 862000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 863000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 864000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 865000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 866000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 867000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 868000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 869000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 870000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 871000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 872000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 873000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 874000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 875000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 876000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 877000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 878000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 879000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 880000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 881000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 882000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 883000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 884000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 885000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 886000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 887000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 888000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 889000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 890000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 891000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 892000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 893000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 894000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 895000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 896000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 897000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 898000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 899000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 900000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 901000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 902000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 903000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 904000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 905000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 906000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 907000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 908000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 909000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 910000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 911000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 912000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 913000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 914000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 915000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 916000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 917000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 918000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 919000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 920000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 921000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 922000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 923000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 924000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 925000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 926000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 927000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 928000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 929000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 930000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 931000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 932000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 933000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 934000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 935000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 936000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 937000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 938000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 939000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 940000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 941000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 942000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 943000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 944000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 945000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 946000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 947000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 948000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 949000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 950000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 951000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 952000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 953000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 954000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 955000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 956000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 957000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 958000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 959000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 960000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 961000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 962000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 963000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 964000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 965000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 966000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 967000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 968000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 969000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 970000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 971000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 972000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 973000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 974000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 975000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 976000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 977000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 978000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 979000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 980000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 981000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 982000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 983000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 984000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 985000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 986000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 987000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 988000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 989000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 990000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 991000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 992000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 993000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 994000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 995000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 996000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 997000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 998000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 999000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1000000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1001000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1002000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1003000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1004000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1005000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1006000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1007000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1008000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1009000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1010000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1011000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1012000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1013000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1014000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1015000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1016000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1017000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1018000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1019000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1020000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1021000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1022000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1023000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1024000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1025000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1026000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1027000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1028000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1029000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1030000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1031000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1032000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1033000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1034000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1035000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1036000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1037000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1038000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1039000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1040000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1041000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1042000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1043000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1044000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1045000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1046000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1047000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1048000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1049000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1050000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1051000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1052000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1053000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1054000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1055000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1056000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1057000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1058000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1059000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1060000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1061000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1062000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1063000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1064000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1065000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1066000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1067000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1068000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1069000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1070000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1071000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1072000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1073000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1074000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1075000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1076000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1077000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1078000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1079000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1080000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1081000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1082000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1083000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1084000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1085000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1086000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1087000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1088000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1089000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1090000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1091000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1092000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1093000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1094000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1095000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1096000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1097000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1098000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1099000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1100000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1101000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1102000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1103000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1104000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1105000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1106000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1107000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1108000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1109000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1110000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1111000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1112000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1113000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1114000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1115000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1116000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1117000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1118000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1119000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1120000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1121000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1122000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1123000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1124000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1125000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1126000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1127000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1128000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1129000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1130000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1131000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1132000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1133000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1134000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1135000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1136000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1137000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1138000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1139000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1140000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1141000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1142000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1143000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1144000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1145000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1146000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1147000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1148000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1149000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1150000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1151000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1152000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1153000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1154000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1155000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1156000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1157000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1158000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1159000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1160000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1161000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1162000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1163000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1164000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1165000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1166000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1167000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1168000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1169000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1170000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1171000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1172000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1173000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1174000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1175000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1176000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1177000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1178000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1179000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1180000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1181000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1182000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1183000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1184000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1185000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 1186000 documents\n"
     ]
    }
   ],
   "source": [
    "a = lda.show_topics(num_topics=15,formatted=False,num_words=10)\n",
    "b = lda.top_topics(doc_term_matrix,dictionary=dictionary,topn=10) # This orders the topics in the decreasing order of coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2skillb = {}\n",
    "topic2csb = {}\n",
    "topic2skilla = {}\n",
    "topic2csa = {}\n",
    "num_topics =lda.num_topics\n",
    "cnt =1\n",
    "\n",
    "for ws in b:\n",
    "    wset = set(w[1] for w in ws[0])\n",
    "    topic2skillb[cnt] = wset\n",
    "    topic2csb[cnt] = ws[1]\n",
    "    cnt +=1\n",
    "\n",
    "for ws in a:\n",
    "    wset = set(w[0]for w in ws[1])\n",
    "    topic2skilla[ws[0]+1] = wset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,num_topics+1):\n",
    "    for j in range(1,num_topics+1):  \n",
    "        if topic2skilla[i].intersection(topic2skillb[j])==topic2skilla[i]:\n",
    "            topic2csa[i] = topic2csb[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>words</th>\n",
       "      <th>cs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Topic9</td>\n",
       "      <td>{brand_new, case, iphone, electronic_cell, pho...</td>\n",
       "      <td>-1.719978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic5</td>\n",
       "      <td>{0_24, baby, t, kid_girl, size, 2, kid_boy, mo...</td>\n",
       "      <td>-2.223498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Topic8</td>\n",
       "      <td>{woman, blouse_t, pink, small, blouse, victori...</td>\n",
       "      <td>-2.326963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>{brand_new, eye, skin, new, color, lip, face, ...</td>\n",
       "      <td>-2.439212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic4</td>\n",
       "      <td>{1, day, shipping, item, price, rm, 2, home_ki...</td>\n",
       "      <td>-2.445025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic6</td>\n",
       "      <td>{woman_athletic, tight_legging, woman, apparel...</td>\n",
       "      <td>-2.608073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Topic15</td>\n",
       "      <td>{woman, boot, athletic, shoe, short, black, ne...</td>\n",
       "      <td>-2.652402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Topic7</td>\n",
       "      <td>{wallet, bag, woman, necklace, purse, gold, po...</td>\n",
       "      <td>-2.802160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Topic13</td>\n",
       "      <td>{1, accent, shipping, pink, home, card, rm, ho...</td>\n",
       "      <td>-3.222567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Topic12</td>\n",
       "      <td>{brand_new, kid, 2, slime, disney, book, new, ...</td>\n",
       "      <td>-3.457248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>{dress, woman, earring, ring, mini, dress_knee...</td>\n",
       "      <td>-3.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Topic10</td>\n",
       "      <td>{vintage_collectible, doll, beauty, doll_acces...</td>\n",
       "      <td>-4.134246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic2</td>\n",
       "      <td>{skinny, bracelet, woman, jean, sock, home, wo...</td>\n",
       "      <td>-4.323009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Topic14</td>\n",
       "      <td>{game, top_t, game_console, electronic_video, ...</td>\n",
       "      <td>-4.505509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Topic11</td>\n",
       "      <td>{brand_new, woman, hat, , accessory_hat, jers...</td>\n",
       "      <td>-4.974756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic                                              words        cs\n",
       "8    Topic9  {brand_new, case, iphone, electronic_cell, pho... -1.719978\n",
       "4    Topic5  {0_24, baby, t, kid_girl, size, 2, kid_boy, mo... -2.223498\n",
       "7    Topic8  {woman, blouse_t, pink, small, blouse, victori... -2.326963\n",
       "2    Topic3  {brand_new, eye, skin, new, color, lip, face, ... -2.439212\n",
       "3    Topic4  {1, day, shipping, item, price, rm, 2, home_ki... -2.445025\n",
       "5    Topic6  {woman_athletic, tight_legging, woman, apparel... -2.608073\n",
       "14  Topic15  {woman, boot, athletic, shoe, short, black, ne... -2.652402\n",
       "6    Topic7  {wallet, bag, woman, necklace, purse, gold, po... -2.802160\n",
       "12  Topic13  {1, accent, shipping, pink, home, card, rm, ho... -3.222567\n",
       "11  Topic12  {brand_new, kid, 2, slime, disney, book, new, ... -3.457248\n",
       "0    Topic1  {dress, woman, earring, ring, mini, dress_knee... -3.667300\n",
       "9   Topic10  {vintage_collectible, doll, beauty, doll_acces... -4.134246\n",
       "1    Topic2  {skinny, bracelet, woman, jean, sock, home, wo... -4.323009\n",
       "13  Topic14  {game, top_t, game_console, electronic_video, ... -4.505509\n",
       "10  Topic11  {brand_new, woman, hat, , accessory_hat, jers... -4.974756"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalData = pd.DataFrame([],columns=['Topic','words'])\n",
    "finalData['Topic']=topic2skilla.keys()\n",
    "finalData['Topic'] = finalData['Topic'].apply(lambda x: 'Topic'+str(x))\n",
    "finalData['words']=topic2skilla.values()\n",
    "finalData['cs'] = topic2csa.values()\n",
    "finalData.sort_values(by='cs',ascending=False,inplace=True)\n",
    "# finalData.to_csv('CoherenceScore.csv')\n",
    "finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>words</th>\n",
       "      <th>cs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Topic9</td>\n",
       "      <td>{brand_new, case, iphone, electronic_cell, phone, case_skin, phone_accessory, accessory, new, charger}</td>\n",
       "      <td>-1.719978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic5</td>\n",
       "      <td>{0_24, baby, t, kid_girl, size, 2, kid_boy, mo, 3, girl}</td>\n",
       "      <td>-2.223498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Topic8</td>\n",
       "      <td>{woman, blouse_t, pink, small, blouse, victoria_secret, woman_top, medium, shirt, size}</td>\n",
       "      <td>-2.326963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>{brand_new, eye, skin, new, color, lip, face, brush, makeup, beauty_makeup}</td>\n",
       "      <td>-2.439212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic4</td>\n",
       "      <td>{1, day, shipping, item, price, rm, 2, home_kitchen, rae, new}</td>\n",
       "      <td>-2.445025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic6</td>\n",
       "      <td>{woman_athletic, tight_legging, woman, apparel_pant, jacket, black, lularoe, legging, size, apparel}</td>\n",
       "      <td>-2.608073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Topic15</td>\n",
       "      <td>{woman, boot, athletic, shoe, short, black, new, size, nike, man}</td>\n",
       "      <td>-2.652402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Topic7</td>\n",
       "      <td>{wallet, bag, woman, necklace, purse, gold, pocket, new, woman_handbag, leather}</td>\n",
       "      <td>-2.802160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Topic13</td>\n",
       "      <td>{1, accent, shipping, pink, home, card, rm, home_dcor, 2, bra}</td>\n",
       "      <td>-3.222567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Topic12</td>\n",
       "      <td>{brand_new, kid, 2, slime, disney, book, new, toy, kid_toy, nail}</td>\n",
       "      <td>-3.457248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>{dress, woman, earring, ring, mini, dress_knee, woman_jewelry, black, length, size}</td>\n",
       "      <td>-3.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Topic10</td>\n",
       "      <td>{vintage_collectible, doll, beauty, doll_accessory, statue, accessory, action_figure, box, kid_toy, hair}</td>\n",
       "      <td>-4.134246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic2</td>\n",
       "      <td>{skinny, bracelet, woman, jean, sock, home, woman_jewelry, jean_slim, size, pair}</td>\n",
       "      <td>-4.323009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Topic14</td>\n",
       "      <td>{game, top_t, game_console, electronic_video, 2, shirt, polo, new, size, man}</td>\n",
       "      <td>-4.505509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Topic11</td>\n",
       "      <td>{brand_new, woman, hat, , accessory_hat, jersey, beauty_fragrance, perfume, new, man}</td>\n",
       "      <td>-4.974756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic  \\\n",
       "8    Topic9   \n",
       "4    Topic5   \n",
       "7    Topic8   \n",
       "2    Topic3   \n",
       "3    Topic4   \n",
       "5    Topic6   \n",
       "14  Topic15   \n",
       "6    Topic7   \n",
       "12  Topic13   \n",
       "11  Topic12   \n",
       "0    Topic1   \n",
       "9   Topic10   \n",
       "1    Topic2   \n",
       "13  Topic14   \n",
       "10  Topic11   \n",
       "\n",
       "                                                                                                        words  \\\n",
       "8      {brand_new, case, iphone, electronic_cell, phone, case_skin, phone_accessory, accessory, new, charger}   \n",
       "4                                                    {0_24, baby, t, kid_girl, size, 2, kid_boy, mo, 3, girl}   \n",
       "7                     {woman, blouse_t, pink, small, blouse, victoria_secret, woman_top, medium, shirt, size}   \n",
       "2                                 {brand_new, eye, skin, new, color, lip, face, brush, makeup, beauty_makeup}   \n",
       "3                                              {1, day, shipping, item, price, rm, 2, home_kitchen, rae, new}   \n",
       "5        {woman_athletic, tight_legging, woman, apparel_pant, jacket, black, lularoe, legging, size, apparel}   \n",
       "14                                          {woman, boot, athletic, shoe, short, black, new, size, nike, man}   \n",
       "6                            {wallet, bag, woman, necklace, purse, gold, pocket, new, woman_handbag, leather}   \n",
       "12                                            {1, accent, shipping, pink, home, card, rm, home_dcor, 2, bra}   \n",
       "11                                          {brand_new, kid, 2, slime, disney, book, new, toy, kid_toy, nail}   \n",
       "0                         {dress, woman, earring, ring, mini, dress_knee, woman_jewelry, black, length, size}   \n",
       "9   {vintage_collectible, doll, beauty, doll_accessory, statue, accessory, action_figure, box, kid_toy, hair}   \n",
       "1                           {skinny, bracelet, woman, jean, sock, home, woman_jewelry, jean_slim, size, pair}   \n",
       "13                              {game, top_t, game_console, electronic_video, 2, shirt, polo, new, size, man}   \n",
       "10                     {brand_new, woman, hat, , accessory_hat, jersey, beauty_fragrance, perfume, new, man}   \n",
       "\n",
       "          cs  \n",
       "8  -1.719978  \n",
       "4  -2.223498  \n",
       "7  -2.326963  \n",
       "2  -2.439212  \n",
       "3  -2.445025  \n",
       "5  -2.608073  \n",
       "14 -2.652402  \n",
       "6  -2.802160  \n",
       "12 -3.222567  \n",
       "11 -3.457248  \n",
       "0  -3.667300  \n",
       "9  -4.134246  \n",
       "1  -4.323009  \n",
       "13 -4.505509  \n",
       "10 -4.974756  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',200)\n",
    "finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "      <th>Topic5</th>\n",
       "      <th>Topic6</th>\n",
       "      <th>Topic7</th>\n",
       "      <th>Topic8</th>\n",
       "      <th>Topic9</th>\n",
       "      <th>Topic10</th>\n",
       "      <th>Topic11</th>\n",
       "      <th>Topic12</th>\n",
       "      <th>Topic13</th>\n",
       "      <th>Topic14</th>\n",
       "      <th>Topic15</th>\n",
       "      <th>Automated_topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.529548</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.374155</td>\n",
       "      <td>Topic6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>Topic4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.036847</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.279305</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.225152</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>Topic3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.132233</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.767052</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.080715</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>Topic6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.692227</td>\n",
       "      <td>0.107801</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.119972</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>Topic8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic1    Topic2    Topic3    Topic4    Topic5    Topic6    Topic7  \\\n",
       "0  0.007407  0.007407  0.007407  0.007407  0.007407  0.529548  0.007407   \n",
       "1  0.011111  0.011111  0.011111  0.844444  0.011111  0.011111  0.011111   \n",
       "2  0.001481  0.036847  0.442400  0.001481  0.279305  0.001481  0.001481   \n",
       "3  0.001667  0.001667  0.001667  0.132233  0.001667  0.767052  0.001667   \n",
       "4  0.006667  0.006667  0.006667  0.006667  0.006667  0.006667  0.006667   \n",
       "\n",
       "     Topic8    Topic9   Topic10   Topic11   Topic12   Topic13   Topic14  \\\n",
       "0  0.007407  0.007407  0.007407  0.007407  0.007407  0.007407  0.007407   \n",
       "1  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "2  0.001481  0.001481  0.225152  0.001481  0.001481  0.001481  0.001481   \n",
       "3  0.001667  0.001667  0.001667  0.001667  0.001667  0.001667  0.080715   \n",
       "4  0.692227  0.107801  0.006667  0.006667  0.006667  0.119972  0.006667   \n",
       "\n",
       "    Topic15 Automated_topic_id  \n",
       "0  0.374155             Topic6  \n",
       "1  0.011111             Topic4  \n",
       "2  0.001481             Topic3  \n",
       "3  0.001667             Topic6  \n",
       "4  0.006667             Topic8  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2topic =lda.get_document_topics(doc_term_matrix,minimum_probability=0)\n",
    "doc2topic = pd.DataFrame(list(doc2topic))\n",
    "num_topics = lda.num_topics\n",
    "doc2topic.columns = ['Topic'+str(i+1) for i in range(num_topics)]\n",
    "for i in range(len(doc2topic.columns)):\n",
    "    doc2topic.iloc[:,i]=doc2topic.iloc[:,i].apply(lambda x: x[1])\n",
    "doc2topic['Automated_topic_id'] =doc2topic.apply(lambda x: np.argmax(x),axis=1)\n",
    "doc2topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_words(vis,lam=0.3,topn=5):\n",
    "    a = vis.topic_info\n",
    "    a['finalscore'] = a['logprob']*lam+(1-lam)*a['loglift']\n",
    "    a = a.loc[:,['Category','Term','finalscore']].groupby(['Category'])\\\n",
    "    .apply(lambda x: x.sort_values(by='finalscore',ascending=False).head(topn))\n",
    "    a = a.loc[:,'Term'].reset_index().loc[:,['Category','Term']]\n",
    "    a = a[a['Category']!='Default']\n",
    "    a = a.to_dict('split')['data']\n",
    "    d ={}\n",
    "    for k,v in a: \n",
    "        if k not in d.keys():\n",
    "            d[k] =set()\n",
    "            d[k].add(v)\n",
    "        else:\n",
    "            d[k].add(v)\n",
    "    finalData = pd.DataFrame([],columns=['Topic','Relevance_words'])\n",
    "    finalData['Topic']=d.keys()\n",
    "    finalData['Relevance_words']=d.values()\n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "rel_words_per_topic = get_relevant_words(vis,lam=0.55)\n",
    "rel_word_list = list(chain.from_iterable(rel_words_per_topic.Relevance_words.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_word_list = [w for w in rel_word_list if len(w)>2]\n",
    "rel_word_list2id = [dictionary.token2id[w] for w in rel_word_list]\n",
    "dictionary.filter_tokens(good_ids=rel_word_list2id)\n",
    "doc_term_matrix =p.map(dictionary.doc2bow,tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matric = pd.DataFrame(0, index=np.arange(len(doc_term_matrix)),columns = dictionary.keys())\n",
    "for index,record in tqdm(enumerate(doc_term_matrix)):\n",
    "    for col in record:\n",
    "        doc_matric[index,col[0]] =col[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('relevant_words.pkl','rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Relevance_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>{blouse, woman_top, shirt, size, blouse_t}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic10</td>\n",
       "      <td>{bra, victoria_secret, brush, woman_underwear, pantie}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic11</td>\n",
       "      <td>{case_skin, phone_accessory, case, iphone, electronic_cell}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic12</td>\n",
       "      <td>{woman_handbag, purse, coach, bag, woman}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic13</td>\n",
       "      <td>{game, electronic_video, console, game_console, slime}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Topic14</td>\n",
       "      <td>{rae, home_kitchen, kid_toy, item, mug}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Topic15</td>\n",
       "      <td>{phone_accessory, phone, case, iphone, electronic_cell}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Topic2</td>\n",
       "      <td>{accessory, audio_surveillance, electronic_tv, hair, headphone}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Topic3</td>\n",
       "      <td>{jean, woman, size, shoe, man}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Topic4</td>\n",
       "      <td>{rm, ring, earring, necklace, woman_jewelry}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Topic5</td>\n",
       "      <td>{beauty_skin, care_body, beauty_fragrance, care_face, perfume}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Topic6</td>\n",
       "      <td>{camera, coat_jacket, jacket, skin, nail}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Topic7</td>\n",
       "      <td>{0_24, home_dcor, baby, home, kid_girl}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Topic8</td>\n",
       "      <td>{makeup, beauty_makeup, beauty, face, lip}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Topic9</td>\n",
       "      <td>{lularoe, apparel_pant, woman_athletic, legging, tight_legging}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic                                                  Relevance_words\n",
       "0    Topic1                       {blouse, woman_top, shirt, size, blouse_t}\n",
       "1   Topic10           {bra, victoria_secret, brush, woman_underwear, pantie}\n",
       "2   Topic11      {case_skin, phone_accessory, case, iphone, electronic_cell}\n",
       "3   Topic12                        {woman_handbag, purse, coach, bag, woman}\n",
       "4   Topic13           {game, electronic_video, console, game_console, slime}\n",
       "5   Topic14                          {rae, home_kitchen, kid_toy, item, mug}\n",
       "6   Topic15          {phone_accessory, phone, case, iphone, electronic_cell}\n",
       "7    Topic2  {accessory, audio_surveillance, electronic_tv, hair, headphone}\n",
       "8    Topic3                                   {jean, woman, size, shoe, man}\n",
       "9    Topic4                     {rm, ring, earring, necklace, woman_jewelry}\n",
       "10   Topic5   {beauty_skin, care_body, beauty_fragrance, care_face, perfume}\n",
       "11   Topic6                        {camera, coat_jacket, jacket, skin, nail}\n",
       "12   Topic7                         {0_24, home_dcor, baby, home, kid_girl}\n",
       "13   Topic8                       {makeup, beauty_makeup, beauty, face, lip}\n",
       "14   Topic9  {lularoe, apparel_pant, woman_athletic, legging, tight_legging}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',300)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
